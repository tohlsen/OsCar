# Contextual and Numerically Augmented QANet (CNAQANet)

**C**ontextual and **N**umerically **A**ugmented **QANet** (CNAQANet) is designed to improve the the performance of the current state-of-the-art model, [NAQANet](https://arxiv.org/pdf/1903.00161.pdf) on the [Discrete Reasoning Over Paragraphs (DROP) dataset](https://allennlp.org/drop).

CNAQANet uses contextualized word embeddings and a modified number extraction method. CNAQANet achieved a performance of **48.86% EM** and **52.09% F1** on DROP, a significant increase compared to NAQANetâ€™s EM and F1 scores of 44.24% and 47.24%, respectively.

The table below shows a side by side comparison of the performance of NAQANet and CNAQANet. As you can see, one of the most important results is that CNAQANet's ability for answering arithmetic questions has a significant boost over NAQANet.

<table>
<caption>Comparison of CNAQANet and NAQANet. Both models were trained for 50-60 epochs.<caption>
  <tr>
    <td>Model</td>
    <td colspan="2">Dev (arithmetic)</td>
    <td colspan="2">Dev (span)</td>
    <td colspan="2">Dev (full)</td>
    <td colspan="2">Hidden Test Set</td>
  </tr>
  <tr>
    <td></td>
    <td>EM</td>
    <td>F1</td>
    <td>EM</td>
    <td>F1</td>
    <td>EM</td>
    <td>F1</td>
    <td>EM</td>
    <td>F1</td>
  </tr>
  <tr>
    <td>NAQANet</td>
    <td>44.55%</td>
    <td>44.79%</td>
    <td>50.69%</td>
    <td>60.17%</td>
    <td>46.75%</td>
    <td>49.87%</td>
    <td>44.24%</td>
    <td>47.24%</td>
  </tr>
  <tr>
    <td>CNAQANet</td>
    <td><b>49.43%</b></td>
    <td><b>49.64%</b></td>
    <td><b>53.07%</b></td>
    <td><b>60.73%</b></td>
    <td><b>50.46%</b></td>
    <td><b>53.87%</b></td>
    <td><b>48.86%</b></td>
    <td><b>52.09%</b></td>
  </tr>
</table>

You can also take a look at our [paper]().

## Who are we

We are a [CSE 481n NLP Capstone](https://courses.cs.washington.edu/courses/cse481n/19sp/) project group at the University of Washington. We spent a quarter doing research, experiments, and developments on CNAQANet. You can follow our [blog](https://medium.com/oscar-capstone) to see our updates.

### Team Members
<ul>
    <li><a href="https://www.linkedin.com/in/patelr3/">Ravi Patel</a></li>
    <li><a href="https://www.linkedin.com/in/tyler-ohlsen/">Tyler Ohlsen</a></li>
    <li><a href="https://www.linkedin.com/in/blarry/">Blarry Wang</a></li>
</ul>

## Ackownledgements
Thanks to Noah Smith, Elizabeth Clark, Lucy Lin, and Yizhong Wang for helpful discussions and supporting us throughout this experience. Also, thanks to Divye Jain, Dan Tran, Yuchong Xiang, Kevin Zhao, Jack Khuu, Vardhman Mehta, Garrett Zhang, and Chia-ko Wu for insighful review and feedback on this paper.

## Developer Notes
- Be sure to update your allennlp so that we can get the DROP dataset/models: <code>pip install allennlp --upgrade</code></li>
- We are following <a href="https://github.com/allenai/allennlp-as-a-library-example/tree/master">this library structure</a>. Please be sure to follow this structure so we can maintain organization. Properly name your files and output folders.
- Do not add models (*.th) to the repo.
- The original baseline configuration was referenced from <a href="https://github.com/allenai/allennlp/blob/master/training_config/naqanet.jsonnet">here</a>
- Yes. The model takes forever to train.
- To run the training script, do it from the top-level of the repo: <code>./scripts/train.sh experiments/baseline.jsonnet out/test</code>. First argument is the config file. Second is the output file. Will automatically run in background so you can log off if needed. Will also automatically REMOVE THE SPECIFIED OUTPUT DIRECTORY. BE CAREFUL.
