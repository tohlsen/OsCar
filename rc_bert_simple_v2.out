2019-05-09 19:47:57,719 - INFO - allennlp.common.params - random_seed = 13370
2019-05-09 19:47:57,719 - INFO - allennlp.common.params - numpy_seed = 1337
2019-05-09 19:47:57,719 - INFO - allennlp.common.params - pytorch_seed = 133
2019-05-09 19:47:57,771 - INFO - allennlp.common.checks - Pytorch version: 1.0.0
2019-05-09 19:47:57,785 - INFO - allennlp.common.params - evaluate_on_test = False
2019-05-09 19:47:57,785 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'instance_format': 'drop', 'passage_length_limit': 200, 'question_length_limit': 50, 'skip_when_all_empty': ['passage_span', 'question_span', 'addition_subtraction', 'counting'], 'token_indexers': {'bert': {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'type': 'bert-pretrained', 'use_starting_offsets': False}, 'token_characters': {'min_padding_length': 5, 'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}, 'type': 'drop_rc_v2'} and extras set()
2019-05-09 19:47:57,786 - INFO - allennlp.common.params - dataset_reader.type = drop_rc_v2
2019-05-09 19:47:57,786 - INFO - allennlp.common.from_params - instantiating class <class 'drop_library.dataset_readers.rc_drop_reader_v2.RCDropReaderV2'> from params {'instance_format': 'drop', 'passage_length_limit': 200, 'question_length_limit': 50, 'skip_when_all_empty': ['passage_span', 'question_span', 'addition_subtraction', 'counting'], 'token_indexers': {'bert': {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'type': 'bert-pretrained', 'use_starting_offsets': False}, 'token_characters': {'min_padding_length': 5, 'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}} and extras set()
2019-05-09 19:47:57,787 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'type': 'bert-pretrained', 'use_starting_offsets': False} and extras set()
2019-05-09 19:47:57,787 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.type = bert-pretrained
2019-05-09 19:47:57,787 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer from params {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'use_starting_offsets': False} and extras set()
2019-05-09 19:47:57,787 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.pretrained_model = bert-base-uncased
2019-05-09 19:47:57,788 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.use_starting_offsets = False
2019-05-09 19:47:57,788 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.do_lowercase = True
2019-05-09 19:47:57,788 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.never_lowercase = None
2019-05-09 19:47:57,788 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.max_pieces = 512
2019-05-09 19:47:58,122 - INFO - pytorch_pretrained_bert.tokenization - loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /homes/iws/patelr3/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
2019-05-09 19:47:58,158 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'min_padding_length': 5, 'type': 'characters'} and extras set()
2019-05-09 19:47:58,158 - INFO - allennlp.common.params - dataset_reader.token_indexers.token_characters.type = characters
2019-05-09 19:47:58,158 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer from params {'min_padding_length': 5} and extras set()
2019-05-09 19:47:58,159 - INFO - allennlp.common.params - dataset_reader.token_indexers.token_characters.namespace = token_characters
2019-05-09 19:47:58,159 - INFO - allennlp.common.params - dataset_reader.token_indexers.token_characters.start_tokens = None
2019-05-09 19:47:58,159 - INFO - allennlp.common.params - dataset_reader.token_indexers.token_characters.end_tokens = None
2019-05-09 19:47:58,159 - INFO - allennlp.common.params - dataset_reader.token_indexers.token_characters.min_padding_length = 5
2019-05-09 19:47:58,159 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'lowercase_tokens': True, 'type': 'single_id'} and extras set()
2019-05-09 19:47:58,159 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id
2019-05-09 19:47:58,159 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer from params {'lowercase_tokens': True} and extras set()
2019-05-09 19:47:58,159 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens
2019-05-09 19:47:58,159 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = True
2019-05-09 19:47:58,159 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None
2019-05-09 19:47:58,160 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None
2019-05-09 19:47:58,160 - INFO - allennlp.common.params - dataset_reader.lazy = False
2019-05-09 19:47:58,160 - INFO - allennlp.common.params - dataset_reader.passage_length_limit = 200
2019-05-09 19:47:58,160 - INFO - allennlp.common.params - dataset_reader.question_length_limit = 50
2019-05-09 19:47:58,160 - INFO - allennlp.common.params - dataset_reader.skip_when_all_empty = ['passage_span', 'question_span', 'addition_subtraction', 'counting']
2019-05-09 19:47:58,160 - INFO - allennlp.common.params - dataset_reader.instance_format = drop
2019-05-09 19:47:58,160 - INFO - allennlp.common.params - dataset_reader.relaxed_span_match_for_finding_labels = True
2019-05-09 19:47:59,090 - INFO - allennlp.training.util - Using a separate dataset reader to load validation and test data.
2019-05-09 19:47:59,090 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'instance_format': 'drop', 'passage_length_limit': 400, 'question_length_limit': 50, 'skip_when_all_empty': [], 'token_indexers': {'bert': {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'type': 'bert-pretrained', 'use_starting_offsets': False}, 'token_characters': {'min_padding_length': 5, 'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}, 'type': 'drop'} and extras set()
2019-05-09 19:47:59,090 - INFO - allennlp.common.params - validation_dataset_reader.type = drop
2019-05-09 19:47:59,090 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.dataset_readers.reading_comprehension.drop.DropReader'> from params {'instance_format': 'drop', 'passage_length_limit': 400, 'question_length_limit': 50, 'skip_when_all_empty': [], 'token_indexers': {'bert': {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'type': 'bert-pretrained', 'use_starting_offsets': False}, 'token_characters': {'min_padding_length': 5, 'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}} and extras set()
2019-05-09 19:47:59,090 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'type': 'bert-pretrained', 'use_starting_offsets': False} and extras set()
2019-05-09 19:47:59,090 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.bert.type = bert-pretrained
2019-05-09 19:47:59,091 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer from params {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'use_starting_offsets': False} and extras set()
2019-05-09 19:47:59,091 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.bert.pretrained_model = bert-base-uncased
2019-05-09 19:47:59,091 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.bert.use_starting_offsets = False
2019-05-09 19:47:59,091 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.bert.do_lowercase = True
2019-05-09 19:47:59,091 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.bert.never_lowercase = None
2019-05-09 19:47:59,091 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.bert.max_pieces = 512
2019-05-09 19:47:59,392 - INFO - pytorch_pretrained_bert.tokenization - loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /homes/iws/patelr3/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
2019-05-09 19:47:59,428 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'min_padding_length': 5, 'type': 'characters'} and extras set()
2019-05-09 19:47:59,429 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.token_characters.type = characters
2019-05-09 19:47:59,429 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer from params {'min_padding_length': 5} and extras set()
2019-05-09 19:47:59,429 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.token_characters.namespace = token_characters
2019-05-09 19:47:59,429 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.token_characters.start_tokens = None
2019-05-09 19:47:59,429 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.token_characters.end_tokens = None
2019-05-09 19:47:59,429 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.token_characters.min_padding_length = 5
2019-05-09 19:47:59,429 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'lowercase_tokens': True, 'type': 'single_id'} and extras set()
2019-05-09 19:47:59,429 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = single_id
2019-05-09 19:47:59,429 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer from params {'lowercase_tokens': True} and extras set()
2019-05-09 19:47:59,430 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.namespace = tokens
2019-05-09 19:47:59,430 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.lowercase_tokens = True
2019-05-09 19:47:59,430 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.start_tokens = None
2019-05-09 19:47:59,430 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.end_tokens = None
2019-05-09 19:47:59,430 - INFO - allennlp.common.params - validation_dataset_reader.lazy = False
2019-05-09 19:47:59,430 - INFO - allennlp.common.params - validation_dataset_reader.passage_length_limit = 400
2019-05-09 19:47:59,430 - INFO - allennlp.common.params - validation_dataset_reader.question_length_limit = 50
2019-05-09 19:47:59,430 - INFO - allennlp.common.params - validation_dataset_reader.skip_when_all_empty = []
2019-05-09 19:47:59,430 - INFO - allennlp.common.params - validation_dataset_reader.instance_format = drop
2019-05-09 19:47:59,430 - INFO - allennlp.common.params - validation_dataset_reader.relaxed_span_match_for_finding_labels = True
2019-05-09 19:47:59,430 - INFO - allennlp.common.params - train_data_path = drop_dataset/drop_dataset_train.json
2019-05-09 19:47:59,430 - INFO - allennlp.training.util - Reading training data from drop_dataset/drop_dataset_train.json
0it [00:00, ?it/s]
2019-05-09 19:47:59,435 - INFO - drop_library.dataset_readers.rc_drop_reader_v2 - Reading file at drop_dataset/drop_dataset_train.json
2019-05-09 19:48:05,094 - INFO - drop_library.dataset_readers.rc_drop_reader_v2 - Reading the dataset
867it [00:10, 86.66it/s]
2938it [00:20, 104.95it/s]
5046it [00:30, 123.55it/s]
7301it [00:40, 142.91it/s]
9555it [00:50, 159.62it/s]
11890it [01:00, 176.35it/s]
14225it [01:10, 187.23it/s]
16412it [01:21, 193.76it/s]
18635it [01:31, 201.52it/s]
20920it [01:41, 208.92it/s]
23205it [01:51, 211.24it/s]
25648it [02:01, 220.16it/s]
28091it [02:13, 217.28it/s]
30466it [02:23, 222.97it/s]
32841it [02:33, 224.06it/s]
35108it [02:44, 220.84it/s]
37245it [02:56, 207.87it/s]
39074it [03:07, 191.08it/s]
2019-05-09 19:51:07,326 - INFO - drop_library.dataset_readers.rc_drop_reader_v2 - Skipped 38290 questions, kept 39119 questions.
39119it [03:07, 208.17it/s]

2019-05-09 19:51:07,351 - INFO - allennlp.common.params - validation_data_path = drop_dataset/drop_dataset_dev.json
2019-05-09 19:51:07,351 - INFO - allennlp.training.util - Reading validation data from drop_dataset/drop_dataset_dev.json
2019-05-09 19:51:07,353 - INFO - allennlp.data.dataset_readers.reading_comprehension.drop - Reading file at drop_dataset/drop_dataset_dev.json
2019-05-09 19:51:08,854 - INFO - allennlp.data.dataset_readers.reading_comprehension.drop - Reading the dataset
2019-05-09 19:51:25,106 - INFO - allennlp.data.dataset_readers.reading_comprehension.drop - Skipped 0 questions, kept 9536 questions.
2019-05-09 19:51:25,116 - INFO - allennlp.common.params - test_data_path = None
2019-05-09 19:51:25,116 - INFO - allennlp.training.trainer - From dataset instances, validation, train will be considered for vocabulary creation.
2019-05-09 19:51:25,116 - INFO - allennlp.common.params - vocabulary.type = None
2019-05-09 19:51:25,116 - INFO - allennlp.common.params - vocabulary.extend = False
2019-05-09 19:51:25,116 - INFO - allennlp.common.params - vocabulary.directory_path = None
2019-05-09 19:51:25,116 - INFO - allennlp.common.params - vocabulary.max_vocab_size = None
2019-05-09 19:51:25,116 - INFO - allennlp.common.params - vocabulary.non_padded_namespaces = ('*tags', '*labels')
2019-05-09 19:51:25,116 - INFO - allennlp.common.params - vocabulary.min_pretrained_embeddings = None
2019-05-09 19:51:25,116 - INFO - allennlp.common.params - vocabulary.only_include_pretrained_words = True
2019-05-09 19:51:25,117 - INFO - allennlp.common.params - vocabulary.tokens_to_add = None
2019-05-09 19:51:25,117 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
0it [00:00, ?it/s]
8677it [00:10, 867.62it/s]
17354it [00:20, 864.84it/s]
26163it [00:30, 869.57it/s]
34972it [00:40, 871.50it/s]
43733it [00:50, 858.57it/s]
48655it [00:56, 855.61it/s]

2019-05-09 19:52:21,985 - INFO - allennlp.data.vocabulary - Reading pretrained tokens from: https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.lower.converted.zip
0it [00:00, ?it/s]
468716it [00:10, 46871.41it/s]
947592it [00:20, 47171.68it/s]
1427993it [00:30, 47428.82it/s]
1702926it [00:35, 47627.24it/s]

2019-05-09 19:52:58,691 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.models.model.Model'> from params {'answering_abilities': ['passage_span_extraction', 'question_span_extraction', 'addition_subtraction', 'counting'], 'dropout_prob': 0.1, 'matrix_attention_layer': {'combination': 'x,y,x*y', 'tensor_1_dim': 128, 'tensor_2_dim': 128, 'type': 'linear'}, 'modeling_layer': {'attention_dropout_prob': 0, 'attention_projection_dim': 128, 'conv_kernel_size': 5, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 128, 'hidden_dim': 128, 'input_dim': 128, 'layer_dropout_undecayed_prob': 0.1, 'num_attention_heads': 8, 'num_blocks': 2, 'num_convs_per_block': 2, 'type': 'qanet_encoder'}, 'num_highway_layers': 2, 'phrase_layer': {'attention_dropout_prob': 0, 'attention_projection_dim': 128, 'conv_kernel_size': 7, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 128, 'hidden_dim': 128, 'input_dim': 128, 'layer_dropout_undecayed_prob': 0.1, 'num_attention_heads': 8, 'num_blocks': 1, 'num_convs_per_block': 4, 'type': 'qanet_encoder'}, 'regularizer': [['.*', {'alpha': 1e-07, 'type': 'l2'}]], 'text_field_embedder': {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'bert': ['bert', 'bert-offsets'], 'token_characters': ['token_characters'], 'tokens': ['tokens']}, 'token_embedders': {'bert': {'pretrained_model': 'bert-base-uncased', 'requires_grad': False, 'top_layer_only': True, 'type': 'bert-pretrained'}, 'token_characters': {'embedding': {'embedding_dim': 64}, 'encoder': {'embedding_dim': 64, 'ngram_filter_sizes': [5], 'num_filters': 200, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 300, 'pretrained_file': 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.lower.converted.zip', 'trainable': False, 'type': 'embedding'}}}, 'type': 'naqanet'} and extras {'vocab'}
2019-05-09 19:52:58,692 - INFO - allennlp.common.params - model.type = naqanet
2019-05-09 19:52:58,692 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.models.reading_comprehension.naqanet.NumericallyAugmentedQaNet'> from params {'answering_abilities': ['passage_span_extraction', 'question_span_extraction', 'addition_subtraction', 'counting'], 'dropout_prob': 0.1, 'matrix_attention_layer': {'combination': 'x,y,x*y', 'tensor_1_dim': 128, 'tensor_2_dim': 128, 'type': 'linear'}, 'modeling_layer': {'attention_dropout_prob': 0, 'attention_projection_dim': 128, 'conv_kernel_size': 5, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 128, 'hidden_dim': 128, 'input_dim': 128, 'layer_dropout_undecayed_prob': 0.1, 'num_attention_heads': 8, 'num_blocks': 2, 'num_convs_per_block': 2, 'type': 'qanet_encoder'}, 'num_highway_layers': 2, 'phrase_layer': {'attention_dropout_prob': 0, 'attention_projection_dim': 128, 'conv_kernel_size': 7, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 128, 'hidden_dim': 128, 'input_dim': 128, 'layer_dropout_undecayed_prob': 0.1, 'num_attention_heads': 8, 'num_blocks': 1, 'num_convs_per_block': 4, 'type': 'qanet_encoder'}, 'regularizer': [['.*', {'alpha': 1e-07, 'type': 'l2'}]], 'text_field_embedder': {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'bert': ['bert', 'bert-offsets'], 'token_characters': ['token_characters'], 'tokens': ['tokens']}, 'token_embedders': {'bert': {'pretrained_model': 'bert-base-uncased', 'requires_grad': False, 'top_layer_only': True, 'type': 'bert-pretrained'}, 'token_characters': {'embedding': {'embedding_dim': 64}, 'encoder': {'embedding_dim': 64, 'ngram_filter_sizes': [5], 'num_filters': 200, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 300, 'pretrained_file': 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.lower.converted.zip', 'trainable': False, 'type': 'embedding'}}}} and extras {'vocab'}
2019-05-09 19:52:58,693 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'bert': ['bert', 'bert-offsets'], 'token_characters': ['token_characters'], 'tokens': ['tokens']}, 'token_embedders': {'bert': {'pretrained_model': 'bert-base-uncased', 'requires_grad': False, 'top_layer_only': True, 'type': 'bert-pretrained'}, 'token_characters': {'embedding': {'embedding_dim': 64}, 'encoder': {'embedding_dim': 64, 'ngram_filter_sizes': [5], 'num_filters': 200, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 300, 'pretrained_file': 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.lower.converted.zip', 'trainable': False, 'type': 'embedding'}}} and extras {'vocab'}
2019-05-09 19:52:58,693 - INFO - allennlp.common.params - model.text_field_embedder.type = basic
2019-05-09 19:52:58,693 - INFO - allennlp.common.params - model.text_field_embedder.allow_unmatched_keys = True
2019-05-09 19:52:58,693 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'pretrained_model': 'bert-base-uncased', 'requires_grad': False, 'top_layer_only': True, 'type': 'bert-pretrained'} and extras {'vocab'}
2019-05-09 19:52:58,693 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.type = bert-pretrained
2019-05-09 19:52:58,693 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.bert_token_embedder.PretrainedBertEmbedder'> from params {'pretrained_model': 'bert-base-uncased', 'requires_grad': False, 'top_layer_only': True} and extras {'vocab'}
2019-05-09 19:52:58,693 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.pretrained_model = bert-base-uncased
2019-05-09 19:52:58,693 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.requires_grad = False
2019-05-09 19:52:58,693 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.top_layer_only = True
2019-05-09 19:52:59,002 - INFO - pytorch_pretrained_bert.modeling - loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /homes/iws/patelr3/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba
2019-05-09 19:52:59,003 - INFO - pytorch_pretrained_bert.modeling - extracting archive file /homes/iws/patelr3/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpszglposl
2019-05-09 19:53:33,731 - INFO - pytorch_pretrained_bert.modeling - Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-05-09 19:53:42,162 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding': {'embedding_dim': 64}, 'encoder': {'embedding_dim': 64, 'ngram_filter_sizes': [5], 'num_filters': 200, 'type': 'cnn'}, 'type': 'character_encoding'} and extras {'vocab'}
2019-05-09 19:53:42,163 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.type = character_encoding
2019-05-09 19:53:42,163 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.embedding.num_embeddings = None
2019-05-09 19:53:42,163 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.embedding.vocab_namespace = token_characters
2019-05-09 19:53:42,163 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.embedding.embedding_dim = 64
2019-05-09 19:53:42,163 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.embedding.pretrained_file = None
2019-05-09 19:53:42,163 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.embedding.projection_dim = None
2019-05-09 19:53:42,163 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.embedding.trainable = True
2019-05-09 19:53:42,163 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.embedding.padding_index = None
2019-05-09 19:53:42,164 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.embedding.max_norm = None
2019-05-09 19:53:42,164 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.embedding.norm_type = 2.0
2019-05-09 19:53:42,164 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.embedding.scale_grad_by_freq = False
2019-05-09 19:53:42,164 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.embedding.sparse = False
2019-05-09 19:53:42,164 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'embedding_dim': 64, 'ngram_filter_sizes': [5], 'num_filters': 200, 'type': 'cnn'} and extras set()
2019-05-09 19:53:42,164 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.encoder.type = cnn
2019-05-09 19:53:42,164 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder'> from params {'embedding_dim': 64, 'ngram_filter_sizes': [5], 'num_filters': 200} and extras set()
2019-05-09 19:53:42,164 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.encoder.embedding_dim = 64
2019-05-09 19:53:42,164 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.encoder.num_filters = 200
2019-05-09 19:53:42,164 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.encoder.ngram_filter_sizes = [5]
2019-05-09 19:53:42,165 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.encoder.output_dim = None
2019-05-09 19:53:42,167 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.dropout = 0.0
2019-05-09 19:53:42,167 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 300, 'pretrained_file': 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.lower.converted.zip', 'trainable': False, 'type': 'embedding'} and extras {'vocab'}
2019-05-09 19:53:42,167 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = embedding
2019-05-09 19:53:42,167 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.num_embeddings = None
2019-05-09 19:53:42,167 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens
2019-05-09 19:53:42,167 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.embedding_dim = 300
2019-05-09 19:53:42,168 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.pretrained_file = https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.lower.converted.zip
2019-05-09 19:53:42,168 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.projection_dim = None
2019-05-09 19:53:42,168 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.trainable = False
2019-05-09 19:53:42,168 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.padding_index = None
2019-05-09 19:53:42,168 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_norm = None
2019-05-09 19:53:42,168 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.norm_type = 2.0
2019-05-09 19:53:42,168 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False
2019-05-09 19:53:42,168 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sparse = False
2019-05-09 19:53:42,171 - INFO - allennlp.modules.token_embedders.embedding - Reading pretrained embeddings from file
0it [00:00, ?it/s]
385801it [00:10, 38579.88it/s]
829453it [00:20, 40150.58it/s]
1296784it [00:30, 41921.99it/s]
1702926it [00:38, 44137.09it/s]

2019-05-09 19:54:21,054 - INFO - allennlp.modules.token_embedders.embedding - Initializing pre-trained embedding layer
2019-05-09 19:54:22,138 - INFO - allennlp.modules.token_embedders.embedding - Pretrained embeddings were found for 35000 out of 35002 tokens
2019-05-09 19:54:22,146 - INFO - allennlp.common.params - model.num_highway_layers = 2
2019-05-09 19:54:22,147 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'attention_dropout_prob': 0, 'attention_projection_dim': 128, 'conv_kernel_size': 7, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 128, 'hidden_dim': 128, 'input_dim': 128, 'layer_dropout_undecayed_prob': 0.1, 'num_attention_heads': 8, 'num_blocks': 1, 'num_convs_per_block': 4, 'type': 'qanet_encoder'} and extras {'vocab'}
2019-05-09 19:54:22,147 - INFO - allennlp.common.params - model.phrase_layer.type = qanet_encoder
2019-05-09 19:54:22,147 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2seq_encoders.qanet_encoder.QaNetEncoder'> from params {'attention_dropout_prob': 0, 'attention_projection_dim': 128, 'conv_kernel_size': 7, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 128, 'hidden_dim': 128, 'input_dim': 128, 'layer_dropout_undecayed_prob': 0.1, 'num_attention_heads': 8, 'num_blocks': 1, 'num_convs_per_block': 4} and extras {'vocab'}
2019-05-09 19:54:22,147 - INFO - allennlp.common.params - model.phrase_layer.input_dim = 128
2019-05-09 19:54:22,147 - INFO - allennlp.common.params - model.phrase_layer.hidden_dim = 128
2019-05-09 19:54:22,147 - INFO - allennlp.common.params - model.phrase_layer.attention_projection_dim = 128
2019-05-09 19:54:22,147 - INFO - allennlp.common.params - model.phrase_layer.feedforward_hidden_dim = 128
2019-05-09 19:54:22,147 - INFO - allennlp.common.params - model.phrase_layer.num_blocks = 1
2019-05-09 19:54:22,148 - INFO - allennlp.common.params - model.phrase_layer.num_convs_per_block = 4
2019-05-09 19:54:22,148 - INFO - allennlp.common.params - model.phrase_layer.conv_kernel_size = 7
2019-05-09 19:54:22,148 - INFO - allennlp.common.params - model.phrase_layer.num_attention_heads = 8
2019-05-09 19:54:22,148 - INFO - allennlp.common.params - model.phrase_layer.use_positional_encoding = True
2019-05-09 19:54:22,148 - INFO - allennlp.common.params - model.phrase_layer.dropout_prob = 0.1
2019-05-09 19:54:22,148 - INFO - allennlp.common.params - model.phrase_layer.layer_dropout_undecayed_prob = 0.1
2019-05-09 19:54:22,148 - INFO - allennlp.common.params - model.phrase_layer.attention_dropout_prob = 0
2019-05-09 19:54:22,152 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.matrix_attention.matrix_attention.MatrixAttention'> from params {'combination': 'x,y,x*y', 'tensor_1_dim': 128, 'tensor_2_dim': 128, 'type': 'linear'} and extras {'vocab'}
2019-05-09 19:54:22,152 - INFO - allennlp.common.params - model.matrix_attention_layer.type = linear
2019-05-09 19:54:22,153 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.matrix_attention.linear_matrix_attention.LinearMatrixAttention'> from params {'combination': 'x,y,x*y', 'tensor_1_dim': 128, 'tensor_2_dim': 128} and extras {'vocab'}
2019-05-09 19:54:22,153 - INFO - allennlp.common.params - model.matrix_attention_layer.tensor_1_dim = 128
2019-05-09 19:54:22,153 - INFO - allennlp.common.params - model.matrix_attention_layer.tensor_2_dim = 128
2019-05-09 19:54:22,153 - INFO - allennlp.common.params - model.matrix_attention_layer.combination = x,y,x*y
2019-05-09 19:54:22,153 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'attention_dropout_prob': 0, 'attention_projection_dim': 128, 'conv_kernel_size': 5, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 128, 'hidden_dim': 128, 'input_dim': 128, 'layer_dropout_undecayed_prob': 0.1, 'num_attention_heads': 8, 'num_blocks': 2, 'num_convs_per_block': 2, 'type': 'qanet_encoder'} and extras {'vocab'}
2019-05-09 19:54:22,153 - INFO - allennlp.common.params - model.modeling_layer.type = qanet_encoder
2019-05-09 19:54:22,153 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2seq_encoders.qanet_encoder.QaNetEncoder'> from params {'attention_dropout_prob': 0, 'attention_projection_dim': 128, 'conv_kernel_size': 5, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 128, 'hidden_dim': 128, 'input_dim': 128, 'layer_dropout_undecayed_prob': 0.1, 'num_attention_heads': 8, 'num_blocks': 2, 'num_convs_per_block': 2} and extras {'vocab'}
2019-05-09 19:54:22,153 - INFO - allennlp.common.params - model.modeling_layer.input_dim = 128
2019-05-09 19:54:22,154 - INFO - allennlp.common.params - model.modeling_layer.hidden_dim = 128
2019-05-09 19:54:22,154 - INFO - allennlp.common.params - model.modeling_layer.attention_projection_dim = 128
2019-05-09 19:54:22,154 - INFO - allennlp.common.params - model.modeling_layer.feedforward_hidden_dim = 128
2019-05-09 19:54:22,154 - INFO - allennlp.common.params - model.modeling_layer.num_blocks = 2
2019-05-09 19:54:22,154 - INFO - allennlp.common.params - model.modeling_layer.num_convs_per_block = 2
2019-05-09 19:54:22,154 - INFO - allennlp.common.params - model.modeling_layer.conv_kernel_size = 5
2019-05-09 19:54:22,154 - INFO - allennlp.common.params - model.modeling_layer.num_attention_heads = 8
2019-05-09 19:54:22,154 - INFO - allennlp.common.params - model.modeling_layer.use_positional_encoding = True
2019-05-09 19:54:22,154 - INFO - allennlp.common.params - model.modeling_layer.dropout_prob = 0.1
2019-05-09 19:54:22,154 - INFO - allennlp.common.params - model.modeling_layer.layer_dropout_undecayed_prob = 0.1
2019-05-09 19:54:22,154 - INFO - allennlp.common.params - model.modeling_layer.attention_dropout_prob = 0
2019-05-09 19:54:22,160 - INFO - allennlp.common.params - model.dropout_prob = 0.1
2019-05-09 19:54:22,160 - INFO - allennlp.common.params - model.regularizer = [['.*', {'alpha': 1e-07, 'type': 'l2'}]]
2019-05-09 19:54:22,160 - INFO - allennlp.common.params - model.regularizer.0.1.type = l2
2019-05-09 19:54:22,160 - INFO - allennlp.common.params - model.answering_abilities = ['passage_span_extraction', 'question_span_extraction', 'addition_subtraction', 'counting']
2019-05-09 19:54:22,167 - INFO - allennlp.nn.initializers - Initializing parameters
2019-05-09 19:54:22,169 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2019-05-09 19:54:22,169 - INFO - allennlp.nn.initializers -    _answer_ability_predictor._linear_layers.0.bias
2019-05-09 19:54:22,169 - INFO - allennlp.nn.initializers -    _answer_ability_predictor._linear_layers.0.weight
2019-05-09 19:54:22,169 - INFO - allennlp.nn.initializers -    _answer_ability_predictor._linear_layers.1.bias
2019-05-09 19:54:22,169 - INFO - allennlp.nn.initializers -    _answer_ability_predictor._linear_layers.1.weight
2019-05-09 19:54:22,169 - INFO - allennlp.nn.initializers -    _count_number_predictor._linear_layers.0.bias
2019-05-09 19:54:22,169 - INFO - allennlp.nn.initializers -    _count_number_predictor._linear_layers.0.weight
2019-05-09 19:54:22,169 - INFO - allennlp.nn.initializers -    _count_number_predictor._linear_layers.1.bias
2019-05-09 19:54:22,169 - INFO - allennlp.nn.initializers -    _count_number_predictor._linear_layers.1.weight
2019-05-09 19:54:22,169 - INFO - allennlp.nn.initializers -    _embedding_proj_layer.bias
2019-05-09 19:54:22,170 - INFO - allennlp.nn.initializers -    _embedding_proj_layer.weight
2019-05-09 19:54:22,170 - INFO - allennlp.nn.initializers -    _encoding_proj_layer.bias
2019-05-09 19:54:22,170 - INFO - allennlp.nn.initializers -    _encoding_proj_layer.weight
2019-05-09 19:54:22,170 - INFO - allennlp.nn.initializers -    _highway_layer._layers.0.bias
2019-05-09 19:54:22,170 - INFO - allennlp.nn.initializers -    _highway_layer._layers.0.weight
2019-05-09 19:54:22,170 - INFO - allennlp.nn.initializers -    _highway_layer._layers.1.bias
2019-05-09 19:54:22,170 - INFO - allennlp.nn.initializers -    _highway_layer._layers.1.weight
2019-05-09 19:54:22,170 - INFO - allennlp.nn.initializers -    _matrix_attention._bias
2019-05-09 19:54:22,170 - INFO - allennlp.nn.initializers -    _matrix_attention._weight_vector
2019-05-09 19:54:22,170 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_layers.0.1.bias
2019-05-09 19:54:22,170 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_layers.0.1.weight
2019-05-09 19:54:22,170 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_layers.0.2.bias
2019-05-09 19:54:22,170 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_layers.0.2.weight
2019-05-09 19:54:22,170 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_layers.1.1.bias
2019-05-09 19:54:22,170 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_layers.1.1.weight
2019-05-09 19:54:22,170 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_layers.1.2.bias
2019-05-09 19:54:22,170 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_layers.1.2.weight
2019-05-09 19:54:22,171 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_norm_layers.0.bias
2019-05-09 19:54:22,171 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_norm_layers.0.weight
2019-05-09 19:54:22,171 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_norm_layers.1.bias
2019-05-09 19:54:22,171 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_norm_layers.1.weight
2019-05-09 19:54:22,171 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.attention_layer._combined_projection.bias
2019-05-09 19:54:22,171 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.attention_layer._combined_projection.weight
2019-05-09 19:54:22,171 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.attention_layer._output_projection.bias
2019-05-09 19:54:22,171 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.attention_layer._output_projection.weight
2019-05-09 19:54:22,171 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.attention_norm_layer.bias
2019-05-09 19:54:22,171 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.attention_norm_layer.weight
2019-05-09 19:54:22,171 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.feedforward._linear_layers.0.bias
2019-05-09 19:54:22,171 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.feedforward._linear_layers.0.weight
2019-05-09 19:54:22,171 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.feedforward._linear_layers.1.bias
2019-05-09 19:54:22,171 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.feedforward._linear_layers.1.weight
2019-05-09 19:54:22,171 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.feedforward_norm_layer.bias
2019-05-09 19:54:22,171 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.feedforward_norm_layer.weight
2019-05-09 19:54:22,171 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_layers.0.1.bias
2019-05-09 19:54:22,172 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_layers.0.1.weight
2019-05-09 19:54:22,172 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_layers.0.2.bias
2019-05-09 19:54:22,172 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_layers.0.2.weight
2019-05-09 19:54:22,172 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_layers.1.1.bias
2019-05-09 19:54:22,172 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_layers.1.1.weight
2019-05-09 19:54:22,172 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_layers.1.2.bias
2019-05-09 19:54:22,172 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_layers.1.2.weight
2019-05-09 19:54:22,172 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_norm_layers.0.bias
2019-05-09 19:54:22,172 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_norm_layers.0.weight
2019-05-09 19:54:22,172 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_norm_layers.1.bias
2019-05-09 19:54:22,172 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_norm_layers.1.weight
2019-05-09 19:54:22,172 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.attention_layer._combined_projection.bias
2019-05-09 19:54:22,172 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.attention_layer._combined_projection.weight
2019-05-09 19:54:22,172 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.attention_layer._output_projection.bias
2019-05-09 19:54:22,172 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.attention_layer._output_projection.weight
2019-05-09 19:54:22,172 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.attention_norm_layer.bias
2019-05-09 19:54:22,172 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.attention_norm_layer.weight
2019-05-09 19:54:22,173 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.feedforward._linear_layers.0.bias
2019-05-09 19:54:22,173 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.feedforward._linear_layers.0.weight
2019-05-09 19:54:22,173 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.feedforward._linear_layers.1.bias
2019-05-09 19:54:22,173 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.feedforward._linear_layers.1.weight
2019-05-09 19:54:22,173 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.feedforward_norm_layer.bias
2019-05-09 19:54:22,173 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.feedforward_norm_layer.weight
2019-05-09 19:54:22,173 - INFO - allennlp.nn.initializers -    _modeling_proj_layer.bias
2019-05-09 19:54:22,173 - INFO - allennlp.nn.initializers -    _modeling_proj_layer.weight
2019-05-09 19:54:22,173 - INFO - allennlp.nn.initializers -    _number_sign_predictor._linear_layers.0.bias
2019-05-09 19:54:22,173 - INFO - allennlp.nn.initializers -    _number_sign_predictor._linear_layers.0.weight
2019-05-09 19:54:22,173 - INFO - allennlp.nn.initializers -    _number_sign_predictor._linear_layers.1.bias
2019-05-09 19:54:22,173 - INFO - allennlp.nn.initializers -    _number_sign_predictor._linear_layers.1.weight
2019-05-09 19:54:22,173 - INFO - allennlp.nn.initializers -    _passage_span_end_predictor._linear_layers.0.bias
2019-05-09 19:54:22,173 - INFO - allennlp.nn.initializers -    _passage_span_end_predictor._linear_layers.0.weight
2019-05-09 19:54:22,173 - INFO - allennlp.nn.initializers -    _passage_span_end_predictor._linear_layers.1.bias
2019-05-09 19:54:22,173 - INFO - allennlp.nn.initializers -    _passage_span_end_predictor._linear_layers.1.weight
2019-05-09 19:54:22,173 - INFO - allennlp.nn.initializers -    _passage_span_start_predictor._linear_layers.0.bias
2019-05-09 19:54:22,174 - INFO - allennlp.nn.initializers -    _passage_span_start_predictor._linear_layers.0.weight
2019-05-09 19:54:22,174 - INFO - allennlp.nn.initializers -    _passage_span_start_predictor._linear_layers.1.bias
2019-05-09 19:54:22,174 - INFO - allennlp.nn.initializers -    _passage_span_start_predictor._linear_layers.1.weight
2019-05-09 19:54:22,174 - INFO - allennlp.nn.initializers -    _passage_weights_predictor.bias
2019-05-09 19:54:22,174 - INFO - allennlp.nn.initializers -    _passage_weights_predictor.weight
2019-05-09 19:54:22,174 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.0.1.bias
2019-05-09 19:54:22,174 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.0.1.weight
2019-05-09 19:54:22,174 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.0.2.bias
2019-05-09 19:54:22,174 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.0.2.weight
2019-05-09 19:54:22,174 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.1.1.bias
2019-05-09 19:54:22,174 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.1.1.weight
2019-05-09 19:54:22,174 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.1.2.bias
2019-05-09 19:54:22,174 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.1.2.weight
2019-05-09 19:54:22,174 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.2.1.bias
2019-05-09 19:54:22,174 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.2.1.weight
2019-05-09 19:54:22,174 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.2.2.bias
2019-05-09 19:54:22,174 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.2.2.weight
2019-05-09 19:54:22,175 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.3.1.bias
2019-05-09 19:54:22,175 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.3.1.weight
2019-05-09 19:54:22,175 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.3.2.bias
2019-05-09 19:54:22,175 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.3.2.weight
2019-05-09 19:54:22,175 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_norm_layers.0.bias
2019-05-09 19:54:22,175 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_norm_layers.0.weight
2019-05-09 19:54:22,175 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_norm_layers.1.bias
2019-05-09 19:54:22,175 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_norm_layers.1.weight
2019-05-09 19:54:22,175 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_norm_layers.2.bias
2019-05-09 19:54:22,175 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_norm_layers.2.weight
2019-05-09 19:54:22,175 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_norm_layers.3.bias
2019-05-09 19:54:22,175 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_norm_layers.3.weight
2019-05-09 19:54:22,175 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.attention_layer._combined_projection.bias
2019-05-09 19:54:22,175 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.attention_layer._combined_projection.weight
2019-05-09 19:54:22,175 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.attention_layer._output_projection.bias
2019-05-09 19:54:22,175 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.attention_layer._output_projection.weight
2019-05-09 19:54:22,175 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.attention_norm_layer.bias
2019-05-09 19:54:22,176 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.attention_norm_layer.weight
2019-05-09 19:54:22,176 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.feedforward._linear_layers.0.bias
2019-05-09 19:54:22,176 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.feedforward._linear_layers.0.weight
2019-05-09 19:54:22,176 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.feedforward._linear_layers.1.bias
2019-05-09 19:54:22,176 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.feedforward._linear_layers.1.weight
2019-05-09 19:54:22,176 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.feedforward_norm_layer.bias
2019-05-09 19:54:22,176 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.feedforward_norm_layer.weight
2019-05-09 19:54:22,176 - INFO - allennlp.nn.initializers -    _question_span_end_predictor._linear_layers.0.bias
2019-05-09 19:54:22,176 - INFO - allennlp.nn.initializers -    _question_span_end_predictor._linear_layers.0.weight
2019-05-09 19:54:22,176 - INFO - allennlp.nn.initializers -    _question_span_end_predictor._linear_layers.1.bias
2019-05-09 19:54:22,176 - INFO - allennlp.nn.initializers -    _question_span_end_predictor._linear_layers.1.weight
2019-05-09 19:54:22,176 - INFO - allennlp.nn.initializers -    _question_span_start_predictor._linear_layers.0.bias
2019-05-09 19:54:22,176 - INFO - allennlp.nn.initializers -    _question_span_start_predictor._linear_layers.0.weight
2019-05-09 19:54:22,176 - INFO - allennlp.nn.initializers -    _question_span_start_predictor._linear_layers.1.bias
2019-05-09 19:54:22,176 - INFO - allennlp.nn.initializers -    _question_span_start_predictor._linear_layers.1.weight
2019-05-09 19:54:22,176 - INFO - allennlp.nn.initializers -    _question_weights_predictor.bias
2019-05-09 19:54:22,176 - INFO - allennlp.nn.initializers -    _question_weights_predictor.weight
2019-05-09 19:54:22,177 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.bias
2019-05-09 19:54:22,177 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.weight
2019-05-09 19:54:22,177 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.embeddings.position_embeddings.weight
2019-05-09 19:54:22,177 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.embeddings.token_type_embeddings.weight
2019-05-09 19:54:22,177 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.embeddings.word_embeddings.weight
2019-05-09 19:54:22,177 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.bias
2019-05-09 19:54:22,177 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.weight
2019-05-09 19:54:22,177 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.bias
2019-05-09 19:54:22,177 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.weight
2019-05-09 19:54:22,177 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.bias
2019-05-09 19:54:22,177 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.weight
2019-05-09 19:54:22,177 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.bias
2019-05-09 19:54:22,177 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.weight
2019-05-09 19:54:22,177 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.bias
2019-05-09 19:54:22,177 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.weight
2019-05-09 19:54:22,177 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.bias
2019-05-09 19:54:22,177 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.weight
2019-05-09 19:54:22,178 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.bias
2019-05-09 19:54:22,178 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.weight
2019-05-09 19:54:22,178 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.bias
2019-05-09 19:54:22,178 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.weight
2019-05-09 19:54:22,178 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.bias
2019-05-09 19:54:22,178 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.weight
2019-05-09 19:54:22,178 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.bias
2019-05-09 19:54:22,178 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.weight
2019-05-09 19:54:22,178 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.bias
2019-05-09 19:54:22,178 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.weight
2019-05-09 19:54:22,178 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.bias
2019-05-09 19:54:22,178 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.weight
2019-05-09 19:54:22,178 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.bias
2019-05-09 19:54:22,178 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.weight
2019-05-09 19:54:22,178 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.bias
2019-05-09 19:54:22,178 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.weight
2019-05-09 19:54:22,178 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.bias
2019-05-09 19:54:22,179 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.weight
2019-05-09 19:54:22,179 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.bias
2019-05-09 19:54:22,179 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.weight
2019-05-09 19:54:22,179 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.bias
2019-05-09 19:54:22,179 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.weight
2019-05-09 19:54:22,179 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.bias
2019-05-09 19:54:22,179 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.weight
2019-05-09 19:54:22,179 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.bias
2019-05-09 19:54:22,179 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.weight
2019-05-09 19:54:22,179 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.bias
2019-05-09 19:54:22,179 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.weight
2019-05-09 19:54:22,179 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.bias
2019-05-09 19:54:22,179 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.weight
2019-05-09 19:54:22,179 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.bias
2019-05-09 19:54:22,179 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.weight
2019-05-09 19:54:22,179 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.bias
2019-05-09 19:54:22,179 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.weight
2019-05-09 19:54:22,180 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.bias
2019-05-09 19:54:22,180 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.weight
2019-05-09 19:54:22,180 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.bias
2019-05-09 19:54:22,180 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.weight
2019-05-09 19:54:22,180 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.bias
2019-05-09 19:54:22,180 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.weight
2019-05-09 19:54:22,180 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.bias
2019-05-09 19:54:22,180 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.weight
2019-05-09 19:54:22,180 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.bias
2019-05-09 19:54:22,180 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.weight
2019-05-09 19:54:22,180 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.bias
2019-05-09 19:54:22,180 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.weight
2019-05-09 19:54:22,180 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.bias
2019-05-09 19:54:22,180 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.weight
2019-05-09 19:54:22,180 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.bias
2019-05-09 19:54:22,180 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.weight
2019-05-09 19:54:22,180 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.bias
2019-05-09 19:54:22,181 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.weight
2019-05-09 19:54:22,181 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.bias
2019-05-09 19:54:22,181 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.weight
2019-05-09 19:54:22,181 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.bias
2019-05-09 19:54:22,181 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.weight
2019-05-09 19:54:22,181 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.bias
2019-05-09 19:54:22,181 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.weight
2019-05-09 19:54:22,181 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.bias
2019-05-09 19:54:22,181 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.weight
2019-05-09 19:54:22,181 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.bias
2019-05-09 19:54:22,181 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.weight
2019-05-09 19:54:22,181 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.bias
2019-05-09 19:54:22,181 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.weight
2019-05-09 19:54:22,181 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.bias
2019-05-09 19:54:22,181 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.weight
2019-05-09 19:54:22,181 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.bias
2019-05-09 19:54:22,181 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.weight
2019-05-09 19:54:22,182 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.bias
2019-05-09 19:54:22,182 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.weight
2019-05-09 19:54:22,182 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.bias
2019-05-09 19:54:22,182 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.weight
2019-05-09 19:54:22,182 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.bias
2019-05-09 19:54:22,182 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.weight
2019-05-09 19:54:22,182 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.bias
2019-05-09 19:54:22,182 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.weight
2019-05-09 19:54:22,182 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.bias
2019-05-09 19:54:22,182 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.weight
2019-05-09 19:54:22,182 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.bias
2019-05-09 19:54:22,182 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.weight
2019-05-09 19:54:22,182 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.bias
2019-05-09 19:54:22,182 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.weight
2019-05-09 19:54:22,182 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.bias
2019-05-09 19:54:22,182 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.weight
2019-05-09 19:54:22,182 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.bias
2019-05-09 19:54:22,183 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.weight
2019-05-09 19:54:22,183 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.bias
2019-05-09 19:54:22,183 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.weight
2019-05-09 19:54:22,183 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.bias
2019-05-09 19:54:22,183 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.weight
2019-05-09 19:54:22,183 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.bias
2019-05-09 19:54:22,183 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.weight
2019-05-09 19:54:22,183 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.bias
2019-05-09 19:54:22,183 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.weight
2019-05-09 19:54:22,183 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.bias
2019-05-09 19:54:22,183 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.weight
2019-05-09 19:54:22,183 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.bias
2019-05-09 19:54:22,183 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.weight
2019-05-09 19:54:22,183 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.bias
2019-05-09 19:54:22,183 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.weight
2019-05-09 19:54:22,183 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.bias
2019-05-09 19:54:22,183 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.weight
2019-05-09 19:54:22,184 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.bias
2019-05-09 19:54:22,184 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.weight
2019-05-09 19:54:22,184 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.bias
2019-05-09 19:54:22,184 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.weight
2019-05-09 19:54:22,184 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.bias
2019-05-09 19:54:22,184 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.weight
2019-05-09 19:54:22,184 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.bias
2019-05-09 19:54:22,184 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.weight
2019-05-09 19:54:22,184 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.bias
2019-05-09 19:54:22,184 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.weight
2019-05-09 19:54:22,184 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.bias
2019-05-09 19:54:22,184 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.weight
2019-05-09 19:54:22,184 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.bias
2019-05-09 19:54:22,184 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.weight
2019-05-09 19:54:22,184 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.bias
2019-05-09 19:54:22,184 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.weight
2019-05-09 19:54:22,184 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.bias
2019-05-09 19:54:22,185 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.weight
2019-05-09 19:54:22,185 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.bias
2019-05-09 19:54:22,185 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.weight
2019-05-09 19:54:22,185 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.bias
2019-05-09 19:54:22,185 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.weight
2019-05-09 19:54:22,185 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.bias
2019-05-09 19:54:22,185 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.weight
2019-05-09 19:54:22,185 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.bias
2019-05-09 19:54:22,185 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.weight
2019-05-09 19:54:22,185 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.bias
2019-05-09 19:54:22,185 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.weight
2019-05-09 19:54:22,185 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.bias
2019-05-09 19:54:22,185 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.weight
2019-05-09 19:54:22,185 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.bias
2019-05-09 19:54:22,185 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.weight
2019-05-09 19:54:22,185 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.bias
2019-05-09 19:54:22,185 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.weight
2019-05-09 19:54:22,186 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.bias
2019-05-09 19:54:22,186 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.weight
2019-05-09 19:54:22,186 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.bias
2019-05-09 19:54:22,186 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.weight
2019-05-09 19:54:22,186 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.bias
2019-05-09 19:54:22,186 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.weight
2019-05-09 19:54:22,186 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.bias
2019-05-09 19:54:22,186 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.weight
2019-05-09 19:54:22,186 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.bias
2019-05-09 19:54:22,186 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.weight
2019-05-09 19:54:22,186 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.bias
2019-05-09 19:54:22,186 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.weight
2019-05-09 19:54:22,186 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.bias
2019-05-09 19:54:22,186 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.weight
2019-05-09 19:54:22,186 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.bias
2019-05-09 19:54:22,186 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.weight
2019-05-09 19:54:22,186 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.bias
2019-05-09 19:54:22,187 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.weight
2019-05-09 19:54:22,187 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.bias
2019-05-09 19:54:22,187 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.weight
2019-05-09 19:54:22,187 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.bias
2019-05-09 19:54:22,187 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.weight
2019-05-09 19:54:22,187 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.bias
2019-05-09 19:54:22,187 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.weight
2019-05-09 19:54:22,187 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.bias
2019-05-09 19:54:22,187 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.weight
2019-05-09 19:54:22,187 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.bias
2019-05-09 19:54:22,187 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.weight
2019-05-09 19:54:22,187 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.bias
2019-05-09 19:54:22,187 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.weight
2019-05-09 19:54:22,187 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.bias
2019-05-09 19:54:22,187 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.weight
2019-05-09 19:54:22,187 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.bias
2019-05-09 19:54:22,187 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.weight
2019-05-09 19:54:22,188 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.bias
2019-05-09 19:54:22,188 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.weight
2019-05-09 19:54:22,188 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.bias
2019-05-09 19:54:22,188 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.weight
2019-05-09 19:54:22,188 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.bias
2019-05-09 19:54:22,188 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.weight
2019-05-09 19:54:22,188 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.bias
2019-05-09 19:54:22,188 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.weight
2019-05-09 19:54:22,188 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.bias
2019-05-09 19:54:22,188 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.weight
2019-05-09 19:54:22,188 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.pooler.dense.bias
2019-05-09 19:54:22,188 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.pooler.dense.weight
2019-05-09 19:54:22,188 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_token_characters._embedding._module.weight
2019-05-09 19:54:22,188 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias
2019-05-09 19:54:22,188 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight
2019-05-09 19:54:22,188 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.weight
2019-05-09 19:54:22,346 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.data_iterator.DataIterator'> from params {'batch_size': 16, 'max_instances_in_memory': 600, 'sorting_keys': [['passage', 'num_tokens'], ['question', 'num_tokens']], 'type': 'bucket'} and extras set()
2019-05-09 19:54:22,346 - INFO - allennlp.common.params - iterator.type = bucket
2019-05-09 19:54:22,346 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.bucket_iterator.BucketIterator'> from params {'batch_size': 16, 'max_instances_in_memory': 600, 'sorting_keys': [['passage', 'num_tokens'], ['question', 'num_tokens']]} and extras set()
2019-05-09 19:54:22,346 - INFO - allennlp.common.params - iterator.sorting_keys = [['passage', 'num_tokens'], ['question', 'num_tokens']]
2019-05-09 19:54:22,347 - INFO - allennlp.common.params - iterator.padding_noise = 0.1
2019-05-09 19:54:22,347 - INFO - allennlp.common.params - iterator.biggest_batch_first = False
2019-05-09 19:54:22,347 - INFO - allennlp.common.params - iterator.batch_size = 16
2019-05-09 19:54:22,347 - INFO - allennlp.common.params - iterator.instances_per_epoch = None
2019-05-09 19:54:22,347 - INFO - allennlp.common.params - iterator.max_instances_in_memory = 600
2019-05-09 19:54:22,347 - INFO - allennlp.common.params - iterator.cache_instances = False
2019-05-09 19:54:22,347 - INFO - allennlp.common.params - iterator.track_epoch = False
2019-05-09 19:54:22,347 - INFO - allennlp.common.params - iterator.maximum_samples_per_batch = None
2019-05-09 19:54:22,347 - INFO - allennlp.common.params - validation_iterator = None
2019-05-09 19:54:22,348 - INFO - allennlp.common.params - trainer.no_grad = ()
2019-05-09 19:54:22,351 - INFO - allennlp.training.trainer - Following parameters are Frozen  (without gradient):
2019-05-09 19:54:22,352 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.embeddings.word_embeddings.weight
2019-05-09 19:54:22,352 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.embeddings.position_embeddings.weight
2019-05-09 19:54:22,352 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.embeddings.token_type_embeddings.weight
2019-05-09 19:54:22,352 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.weight
2019-05-09 19:54:22,352 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.bias
2019-05-09 19:54:22,352 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.weight
2019-05-09 19:54:22,352 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.bias
2019-05-09 19:54:22,352 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.weight
2019-05-09 19:54:22,352 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.bias
2019-05-09 19:54:22,352 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.weight
2019-05-09 19:54:22,352 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.bias
2019-05-09 19:54:22,352 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.weight
2019-05-09 19:54:22,353 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.bias
2019-05-09 19:54:22,353 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.weight
2019-05-09 19:54:22,353 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.bias
2019-05-09 19:54:22,353 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.weight
2019-05-09 19:54:22,353 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.bias
2019-05-09 19:54:22,353 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.weight
2019-05-09 19:54:22,353 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.bias
2019-05-09 19:54:22,353 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.weight
2019-05-09 19:54:22,353 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.bias
2019-05-09 19:54:22,353 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.weight
2019-05-09 19:54:22,353 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.bias
2019-05-09 19:54:22,354 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.weight
2019-05-09 19:54:22,354 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.bias
2019-05-09 19:54:22,354 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.weight
2019-05-09 19:54:22,354 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.bias
2019-05-09 19:54:22,354 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.weight
2019-05-09 19:54:22,354 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.bias
2019-05-09 19:54:22,354 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.weight
2019-05-09 19:54:22,354 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.bias
2019-05-09 19:54:22,354 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.weight
2019-05-09 19:54:22,354 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.bias
2019-05-09 19:54:22,354 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.weight
2019-05-09 19:54:22,354 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.bias
2019-05-09 19:54:22,355 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.weight
2019-05-09 19:54:22,355 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.bias
2019-05-09 19:54:22,355 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.weight
2019-05-09 19:54:22,355 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.bias
2019-05-09 19:54:22,355 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.weight
2019-05-09 19:54:22,355 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.bias
2019-05-09 19:54:22,355 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.weight
2019-05-09 19:54:22,355 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.bias
2019-05-09 19:54:22,355 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.weight
2019-05-09 19:54:22,355 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.bias
2019-05-09 19:54:22,355 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.weight
2019-05-09 19:54:22,355 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.bias
2019-05-09 19:54:22,356 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.weight
2019-05-09 19:54:22,356 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.bias
2019-05-09 19:54:22,356 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.weight
2019-05-09 19:54:22,356 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.bias
2019-05-09 19:54:22,356 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.weight
2019-05-09 19:54:22,356 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.bias
2019-05-09 19:54:22,356 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.weight
2019-05-09 19:54:22,356 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.bias
2019-05-09 19:54:22,356 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.weight
2019-05-09 19:54:22,356 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.bias
2019-05-09 19:54:22,356 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.weight
2019-05-09 19:54:22,356 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.bias
2019-05-09 19:54:22,357 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.weight
2019-05-09 19:54:22,357 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.bias
2019-05-09 19:54:22,357 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.weight
2019-05-09 19:54:22,357 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.bias
2019-05-09 19:54:22,357 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.weight
2019-05-09 19:54:22,357 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.bias
2019-05-09 19:54:22,357 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.weight
2019-05-09 19:54:22,357 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.bias
2019-05-09 19:54:22,357 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.weight
2019-05-09 19:54:22,357 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.bias
2019-05-09 19:54:22,357 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.weight
2019-05-09 19:54:22,357 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.bias
2019-05-09 19:54:22,357 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.weight
2019-05-09 19:54:22,357 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.bias
2019-05-09 19:54:22,357 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.weight
2019-05-09 19:54:22,358 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.bias
2019-05-09 19:54:22,358 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.weight
2019-05-09 19:54:22,358 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.bias
2019-05-09 19:54:22,358 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.weight
2019-05-09 19:54:22,358 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.bias
2019-05-09 19:54:22,358 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.weight
2019-05-09 19:54:22,358 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.bias
2019-05-09 19:54:22,358 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.weight
2019-05-09 19:54:22,358 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.bias
2019-05-09 19:54:22,358 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.weight
2019-05-09 19:54:22,358 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.bias
2019-05-09 19:54:22,358 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.weight
2019-05-09 19:54:22,358 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.bias
2019-05-09 19:54:22,358 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.weight
2019-05-09 19:54:22,358 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.bias
2019-05-09 19:54:22,358 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.weight
2019-05-09 19:54:22,358 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.bias
2019-05-09 19:54:22,359 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.weight
2019-05-09 19:54:22,359 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.bias
2019-05-09 19:54:22,359 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.weight
2019-05-09 19:54:22,359 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.bias
2019-05-09 19:54:22,359 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.weight
2019-05-09 19:54:22,359 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.bias
2019-05-09 19:54:22,359 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.weight
2019-05-09 19:54:22,359 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.bias
2019-05-09 19:54:22,359 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.weight
2019-05-09 19:54:22,359 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.bias
2019-05-09 19:54:22,359 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.weight
2019-05-09 19:54:22,359 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.bias
2019-05-09 19:54:22,359 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.weight
2019-05-09 19:54:22,359 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.bias
2019-05-09 19:54:22,359 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.weight
2019-05-09 19:54:22,359 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.bias
2019-05-09 19:54:22,359 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.weight
2019-05-09 19:54:22,359 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.bias
2019-05-09 19:54:22,360 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.weight
2019-05-09 19:54:22,360 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.bias
2019-05-09 19:54:22,360 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.weight
2019-05-09 19:54:22,360 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.bias
2019-05-09 19:54:22,360 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.weight
2019-05-09 19:54:22,360 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.bias
2019-05-09 19:54:22,360 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.weight
2019-05-09 19:54:22,360 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.bias
2019-05-09 19:54:22,360 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.weight
2019-05-09 19:54:22,360 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.bias
2019-05-09 19:54:22,360 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.weight
2019-05-09 19:54:22,360 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.bias
2019-05-09 19:54:22,360 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.weight
2019-05-09 19:54:22,360 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.bias
2019-05-09 19:54:22,360 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.weight
2019-05-09 19:54:22,360 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.bias
2019-05-09 19:54:22,360 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.weight
2019-05-09 19:54:22,361 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.bias
2019-05-09 19:54:22,361 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.weight
2019-05-09 19:54:22,361 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.bias
2019-05-09 19:54:22,361 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.weight
2019-05-09 19:54:22,361 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.bias
2019-05-09 19:54:22,361 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.weight
2019-05-09 19:54:22,361 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.bias
2019-05-09 19:54:22,361 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.weight
2019-05-09 19:54:22,361 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.bias
2019-05-09 19:54:22,361 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.weight
2019-05-09 19:54:22,361 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.bias
2019-05-09 19:54:22,361 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.weight
2019-05-09 19:54:22,361 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.bias
2019-05-09 19:54:22,361 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.weight
2019-05-09 19:54:22,361 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.bias
2019-05-09 19:54:22,361 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.weight
2019-05-09 19:54:22,361 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.bias
2019-05-09 19:54:22,362 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.weight
2019-05-09 19:54:22,362 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.bias
2019-05-09 19:54:22,362 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.weight
2019-05-09 19:54:22,362 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.bias
2019-05-09 19:54:22,362 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.weight
2019-05-09 19:54:22,362 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.bias
2019-05-09 19:54:22,362 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.weight
2019-05-09 19:54:22,362 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.bias
2019-05-09 19:54:22,362 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.weight
2019-05-09 19:54:22,362 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.bias
2019-05-09 19:54:22,362 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.weight
2019-05-09 19:54:22,362 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.bias
2019-05-09 19:54:22,362 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.weight
2019-05-09 19:54:22,362 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.bias
2019-05-09 19:54:22,362 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.weight
2019-05-09 19:54:22,362 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.bias
2019-05-09 19:54:22,362 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.weight
2019-05-09 19:54:22,363 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.bias
2019-05-09 19:54:22,363 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.weight
2019-05-09 19:54:22,363 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.bias
2019-05-09 19:54:22,363 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.weight
2019-05-09 19:54:22,363 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.bias
2019-05-09 19:54:22,363 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.weight
2019-05-09 19:54:22,363 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.bias
2019-05-09 19:54:22,363 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.weight
2019-05-09 19:54:22,363 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.bias
2019-05-09 19:54:22,363 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.weight
2019-05-09 19:54:22,363 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.bias
2019-05-09 19:54:22,363 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.weight
2019-05-09 19:54:22,363 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.bias
2019-05-09 19:54:22,363 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.weight
2019-05-09 19:54:22,363 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.bias
2019-05-09 19:54:22,363 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.weight
2019-05-09 19:54:22,363 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.bias
2019-05-09 19:54:22,363 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.weight
2019-05-09 19:54:22,364 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.bias
2019-05-09 19:54:22,364 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.weight
2019-05-09 19:54:22,364 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.bias
2019-05-09 19:54:22,364 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.weight
2019-05-09 19:54:22,364 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.bias
2019-05-09 19:54:22,364 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.weight
2019-05-09 19:54:22,364 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.bias
2019-05-09 19:54:22,364 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.weight
2019-05-09 19:54:22,364 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.bias
2019-05-09 19:54:22,364 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.weight
2019-05-09 19:54:22,364 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.bias
2019-05-09 19:54:22,364 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.weight
2019-05-09 19:54:22,364 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.bias
2019-05-09 19:54:22,364 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.weight
2019-05-09 19:54:22,364 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.bias
2019-05-09 19:54:22,364 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.weight
2019-05-09 19:54:22,364 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.bias
2019-05-09 19:54:22,365 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.weight
2019-05-09 19:54:22,365 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.bias
2019-05-09 19:54:22,365 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.pooler.dense.weight
2019-05-09 19:54:22,365 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.pooler.dense.bias
2019-05-09 19:54:22,365 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_tokens.weight
2019-05-09 19:54:22,365 - INFO - allennlp.training.trainer - Following parameters are Tunable (with gradient):
2019-05-09 19:54:22,365 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_token_characters._embedding._module.weight
2019-05-09 19:54:22,365 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight
2019-05-09 19:54:22,365 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias
2019-05-09 19:54:22,365 - INFO - allennlp.training.trainer - _embedding_proj_layer.weight
2019-05-09 19:54:22,365 - INFO - allennlp.training.trainer - _embedding_proj_layer.bias
2019-05-09 19:54:22,365 - INFO - allennlp.training.trainer - _highway_layer._layers.0.weight
2019-05-09 19:54:22,365 - INFO - allennlp.training.trainer - _highway_layer._layers.0.bias
2019-05-09 19:54:22,365 - INFO - allennlp.training.trainer - _highway_layer._layers.1.weight
2019-05-09 19:54:22,365 - INFO - allennlp.training.trainer - _highway_layer._layers.1.bias
2019-05-09 19:54:22,365 - INFO - allennlp.training.trainer - _encoding_proj_layer.weight
2019-05-09 19:54:22,365 - INFO - allennlp.training.trainer - _encoding_proj_layer.bias
2019-05-09 19:54:22,366 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_norm_layers.0.weight
2019-05-09 19:54:22,366 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_norm_layers.0.bias
2019-05-09 19:54:22,366 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_norm_layers.1.weight
2019-05-09 19:54:22,366 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_norm_layers.1.bias
2019-05-09 19:54:22,366 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_norm_layers.2.weight
2019-05-09 19:54:22,366 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_norm_layers.2.bias
2019-05-09 19:54:22,366 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_norm_layers.3.weight
2019-05-09 19:54:22,366 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_norm_layers.3.bias
2019-05-09 19:54:22,366 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.0.1.weight
2019-05-09 19:54:22,366 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.0.1.bias
2019-05-09 19:54:22,366 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.0.2.weight
2019-05-09 19:54:22,366 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.0.2.bias
2019-05-09 19:54:22,366 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.1.1.weight
2019-05-09 19:54:22,366 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.1.1.bias
2019-05-09 19:54:22,366 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.1.2.weight
2019-05-09 19:54:22,366 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.1.2.bias
2019-05-09 19:54:22,366 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.2.1.weight
2019-05-09 19:54:22,367 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.2.1.bias
2019-05-09 19:54:22,367 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.2.2.weight
2019-05-09 19:54:22,367 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.2.2.bias
2019-05-09 19:54:22,367 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.3.1.weight
2019-05-09 19:54:22,367 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.3.1.bias
2019-05-09 19:54:22,367 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.3.2.weight
2019-05-09 19:54:22,367 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.3.2.bias
2019-05-09 19:54:22,367 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.attention_norm_layer.weight
2019-05-09 19:54:22,367 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.attention_norm_layer.bias
2019-05-09 19:54:22,367 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.attention_layer._combined_projection.weight
2019-05-09 19:54:22,367 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.attention_layer._combined_projection.bias
2019-05-09 19:54:22,367 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.attention_layer._output_projection.weight
2019-05-09 19:54:22,367 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.attention_layer._output_projection.bias
2019-05-09 19:54:22,367 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.feedforward_norm_layer.weight
2019-05-09 19:54:22,367 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.feedforward_norm_layer.bias
2019-05-09 19:54:22,367 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.feedforward._linear_layers.0.weight
2019-05-09 19:54:22,367 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.feedforward._linear_layers.0.bias
2019-05-09 19:54:22,367 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.feedforward._linear_layers.1.weight
2019-05-09 19:54:22,368 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.feedforward._linear_layers.1.bias
2019-05-09 19:54:22,368 - INFO - allennlp.training.trainer - _matrix_attention._weight_vector
2019-05-09 19:54:22,368 - INFO - allennlp.training.trainer - _matrix_attention._bias
2019-05-09 19:54:22,368 - INFO - allennlp.training.trainer - _modeling_proj_layer.weight
2019-05-09 19:54:22,368 - INFO - allennlp.training.trainer - _modeling_proj_layer.bias
2019-05-09 19:54:22,368 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_norm_layers.0.weight
2019-05-09 19:54:22,368 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_norm_layers.0.bias
2019-05-09 19:54:22,368 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_norm_layers.1.weight
2019-05-09 19:54:22,368 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_norm_layers.1.bias
2019-05-09 19:54:22,368 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_layers.0.1.weight
2019-05-09 19:54:22,368 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_layers.0.1.bias
2019-05-09 19:54:22,368 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_layers.0.2.weight
2019-05-09 19:54:22,368 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_layers.0.2.bias
2019-05-09 19:54:22,368 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_layers.1.1.weight
2019-05-09 19:54:22,368 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_layers.1.1.bias
2019-05-09 19:54:22,368 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_layers.1.2.weight
2019-05-09 19:54:22,368 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_layers.1.2.bias
2019-05-09 19:54:22,369 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.attention_norm_layer.weight
2019-05-09 19:54:22,369 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.attention_norm_layer.bias
2019-05-09 19:54:22,369 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.attention_layer._combined_projection.weight
2019-05-09 19:54:22,369 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.attention_layer._combined_projection.bias
2019-05-09 19:54:22,369 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.attention_layer._output_projection.weight
2019-05-09 19:54:22,369 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.attention_layer._output_projection.bias
2019-05-09 19:54:22,369 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.feedforward_norm_layer.weight
2019-05-09 19:54:22,369 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.feedforward_norm_layer.bias
2019-05-09 19:54:22,369 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.feedforward._linear_layers.0.weight
2019-05-09 19:54:22,369 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.feedforward._linear_layers.0.bias
2019-05-09 19:54:22,369 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.feedforward._linear_layers.1.weight
2019-05-09 19:54:22,369 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.feedforward._linear_layers.1.bias
2019-05-09 19:54:22,369 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_norm_layers.0.weight
2019-05-09 19:54:22,369 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_norm_layers.0.bias
2019-05-09 19:54:22,369 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_norm_layers.1.weight
2019-05-09 19:54:22,369 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_norm_layers.1.bias
2019-05-09 19:54:22,369 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_layers.0.1.weight
2019-05-09 19:54:22,370 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_layers.0.1.bias
2019-05-09 19:54:22,370 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_layers.0.2.weight
2019-05-09 19:54:22,370 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_layers.0.2.bias
2019-05-09 19:54:22,370 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_layers.1.1.weight
2019-05-09 19:54:22,370 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_layers.1.1.bias
2019-05-09 19:54:22,370 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_layers.1.2.weight
2019-05-09 19:54:22,370 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_layers.1.2.bias
2019-05-09 19:54:22,370 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.attention_norm_layer.weight
2019-05-09 19:54:22,370 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.attention_norm_layer.bias
2019-05-09 19:54:22,370 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.attention_layer._combined_projection.weight
2019-05-09 19:54:22,370 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.attention_layer._combined_projection.bias
2019-05-09 19:54:22,370 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.attention_layer._output_projection.weight
2019-05-09 19:54:22,370 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.attention_layer._output_projection.bias
2019-05-09 19:54:22,370 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.feedforward_norm_layer.weight
2019-05-09 19:54:22,370 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.feedforward_norm_layer.bias
2019-05-09 19:54:22,370 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.feedforward._linear_layers.0.weight
2019-05-09 19:54:22,370 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.feedforward._linear_layers.0.bias
2019-05-09 19:54:22,370 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.feedforward._linear_layers.1.weight
2019-05-09 19:54:22,371 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.feedforward._linear_layers.1.bias
2019-05-09 19:54:22,371 - INFO - allennlp.training.trainer - _passage_weights_predictor.weight
2019-05-09 19:54:22,371 - INFO - allennlp.training.trainer - _passage_weights_predictor.bias
2019-05-09 19:54:22,371 - INFO - allennlp.training.trainer - _question_weights_predictor.weight
2019-05-09 19:54:22,371 - INFO - allennlp.training.trainer - _question_weights_predictor.bias
2019-05-09 19:54:22,371 - INFO - allennlp.training.trainer - _answer_ability_predictor._linear_layers.0.weight
2019-05-09 19:54:22,371 - INFO - allennlp.training.trainer - _answer_ability_predictor._linear_layers.0.bias
2019-05-09 19:54:22,371 - INFO - allennlp.training.trainer - _answer_ability_predictor._linear_layers.1.weight
2019-05-09 19:54:22,371 - INFO - allennlp.training.trainer - _answer_ability_predictor._linear_layers.1.bias
2019-05-09 19:54:22,371 - INFO - allennlp.training.trainer - _passage_span_start_predictor._linear_layers.0.weight
2019-05-09 19:54:22,371 - INFO - allennlp.training.trainer - _passage_span_start_predictor._linear_layers.0.bias
2019-05-09 19:54:22,371 - INFO - allennlp.training.trainer - _passage_span_start_predictor._linear_layers.1.weight
2019-05-09 19:54:22,371 - INFO - allennlp.training.trainer - _passage_span_start_predictor._linear_layers.1.bias
2019-05-09 19:54:22,371 - INFO - allennlp.training.trainer - _passage_span_end_predictor._linear_layers.0.weight
2019-05-09 19:54:22,371 - INFO - allennlp.training.trainer - _passage_span_end_predictor._linear_layers.0.bias
2019-05-09 19:54:22,371 - INFO - allennlp.training.trainer - _passage_span_end_predictor._linear_layers.1.weight
2019-05-09 19:54:22,371 - INFO - allennlp.training.trainer - _passage_span_end_predictor._linear_layers.1.bias
2019-05-09 19:54:22,372 - INFO - allennlp.training.trainer - _question_span_start_predictor._linear_layers.0.weight
2019-05-09 19:54:22,372 - INFO - allennlp.training.trainer - _question_span_start_predictor._linear_layers.0.bias
2019-05-09 19:54:22,372 - INFO - allennlp.training.trainer - _question_span_start_predictor._linear_layers.1.weight
2019-05-09 19:54:22,372 - INFO - allennlp.training.trainer - _question_span_start_predictor._linear_layers.1.bias
2019-05-09 19:54:22,372 - INFO - allennlp.training.trainer - _question_span_end_predictor._linear_layers.0.weight
2019-05-09 19:54:22,372 - INFO - allennlp.training.trainer - _question_span_end_predictor._linear_layers.0.bias
2019-05-09 19:54:22,372 - INFO - allennlp.training.trainer - _question_span_end_predictor._linear_layers.1.weight
2019-05-09 19:54:22,372 - INFO - allennlp.training.trainer - _question_span_end_predictor._linear_layers.1.bias
2019-05-09 19:54:22,372 - INFO - allennlp.training.trainer - _number_sign_predictor._linear_layers.0.weight
2019-05-09 19:54:22,372 - INFO - allennlp.training.trainer - _number_sign_predictor._linear_layers.0.bias
2019-05-09 19:54:22,372 - INFO - allennlp.training.trainer - _number_sign_predictor._linear_layers.1.weight
2019-05-09 19:54:22,372 - INFO - allennlp.training.trainer - _number_sign_predictor._linear_layers.1.bias
2019-05-09 19:54:22,372 - INFO - allennlp.training.trainer - _count_number_predictor._linear_layers.0.weight
2019-05-09 19:54:22,372 - INFO - allennlp.training.trainer - _count_number_predictor._linear_layers.0.bias
2019-05-09 19:54:22,372 - INFO - allennlp.training.trainer - _count_number_predictor._linear_layers.1.weight
2019-05-09 19:54:22,372 - INFO - allennlp.training.trainer - _count_number_predictor._linear_layers.1.bias
2019-05-09 19:54:22,372 - INFO - allennlp.common.params - trainer.patience = 10
2019-05-09 19:54:22,373 - INFO - allennlp.common.params - trainer.validation_metric = +f1
2019-05-09 19:54:22,373 - INFO - allennlp.common.params - trainer.shuffle = True
2019-05-09 19:54:22,373 - INFO - allennlp.common.params - trainer.num_epochs = 10
2019-05-09 19:54:22,373 - INFO - allennlp.common.params - trainer.cuda_device = 0
2019-05-09 19:54:22,373 - INFO - allennlp.common.params - trainer.grad_norm = 5
2019-05-09 19:54:22,373 - INFO - allennlp.common.params - trainer.grad_clipping = None
2019-05-09 19:54:22,373 - INFO - allennlp.common.params - trainer.learning_rate_scheduler = None
2019-05-09 19:54:22,373 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2019-05-09 19:54:27,503 - INFO - allennlp.common.params - trainer.optimizer.type = adam
2019-05-09 19:54:27,503 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None
2019-05-09 19:54:27,503 - INFO - allennlp.training.optimizers - Number of trainable parameters: 1055072
2019-05-09 19:54:27,503 - INFO - allennlp.common.params - trainer.optimizer.infer_type_and_cast = True
2019-05-09 19:54:27,503 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
2019-05-09 19:54:27,503 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: 
2019-05-09 19:54:27,503 - INFO - allennlp.common.params - trainer.optimizer.betas = [0.8, 0.999]
2019-05-09 19:54:27,503 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-07
2019-05-09 19:54:27,503 - INFO - allennlp.common.params - trainer.optimizer.lr = 0.0005
2019-05-09 19:54:27,504 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.training.moving_average.MovingAverage'> from params {'decay': 0.9999, 'type': 'exponential'} and extras {'parameters'}
2019-05-09 19:54:27,504 - INFO - allennlp.common.params - trainer.moving_average.type = exponential
2019-05-09 19:54:27,504 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.training.moving_average.ExponentialMovingAverage'> from params {'decay': 0.9999} and extras {'parameters'}
2019-05-09 19:54:27,504 - INFO - allennlp.common.params - trainer.moving_average.decay = 0.9999
2019-05-09 19:54:27,504 - INFO - allennlp.common.params - trainer.moving_average.numerator = 1.0
2019-05-09 19:54:27,504 - INFO - allennlp.common.params - trainer.moving_average.denominator = 10.0
2019-05-09 19:54:27,510 - INFO - allennlp.common.params - trainer.num_serialized_models_to_keep = 1
2019-05-09 19:54:27,510 - INFO - allennlp.common.params - trainer.keep_serialized_model_every_num_seconds = None
2019-05-09 19:54:27,510 - INFO - allennlp.common.params - trainer.model_save_interval = None
2019-05-09 19:54:27,510 - INFO - allennlp.common.params - trainer.summary_interval = 100
2019-05-09 19:54:27,510 - INFO - allennlp.common.params - trainer.histogram_interval = None
2019-05-09 19:54:27,510 - INFO - allennlp.common.params - trainer.should_log_parameter_statistics = True
2019-05-09 19:54:27,510 - INFO - allennlp.common.params - trainer.should_log_learning_rate = False
2019-05-09 19:54:27,510 - INFO - allennlp.common.params - trainer.log_batch_size_period = None
2019-05-09 19:54:27,778 - INFO - allennlp.training.trainer - Beginning training.
2019-05-09 19:54:27,778 - INFO - allennlp.training.trainer - Epoch 0/9
2019-05-09 19:54:27,778 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5332.732
2019-05-09 19:54:27,888 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1723
2019-05-09 19:54:27,894 - INFO - allennlp.training.trainer - Training
  0%|          | 0/2445 [00:00<?, ?it/s]
em: 0.0000, f1: 0.0278, loss: 10.1973 ||:   0%|          | 12/2445 [00:10<33:53,  1.20it/s]
em: 0.0127, f1: 0.0486, loss: 8.9616 ||:   1%|1         | 30/2445 [00:20<30:34,  1.32it/s] 
em: 0.0329, f1: 0.0713, loss: 8.3953 ||:   2%|1         | 48/2445 [00:34<30:18,  1.32it/s]
em: 0.0394, f1: 0.0804, loss: 8.0536 ||:   3%|2         | 64/2445 [00:44<28:35,  1.39it/s]
em: 0.0435, f1: 0.0849, loss: 7.8327 ||:   3%|3         | 80/2445 [00:56<28:51,  1.37it/s]
em: 0.0537, f1: 0.1005, loss: 7.5558 ||:   4%|3         | 97/2445 [01:06<27:13,  1.44it/s]
em: 0.0672, f1: 0.1150, loss: 7.3188 ||:   5%|4         | 114/2445 [01:18<26:40,  1.46it/s]
em: 0.0718, f1: 0.1238, loss: 7.1213 ||:   5%|5         | 130/2445 [01:30<27:19,  1.41it/s]
em: 0.0772, f1: 0.1311, loss: 6.9475 ||:   6%|6         | 147/2445 [01:40<26:07,  1.47it/s]
em: 0.0872, f1: 0.1415, loss: 6.8506 ||:   7%|6         | 164/2445 [01:53<26:54,  1.41it/s]
em: 0.0980, f1: 0.1524, loss: 6.7223 ||:   7%|7         | 178/2445 [02:04<27:42,  1.36it/s]
em: 0.1003, f1: 0.1588, loss: 6.6280 ||:   8%|7         | 192/2445 [02:15<27:30,  1.37it/s]
em: 0.1027, f1: 0.1614, loss: 6.5660 ||:   8%|8         | 207/2445 [02:25<26:38,  1.40it/s]
em: 0.1048, f1: 0.1629, loss: 6.5441 ||:   9%|9         | 223/2445 [02:35<25:37,  1.45it/s]
em: 0.1070, f1: 0.1651, loss: 6.4731 ||:  10%|9         | 239/2445 [02:47<26:08,  1.41it/s]
em: 0.1131, f1: 0.1708, loss: 6.4215 ||:  10%|#         | 255/2445 [02:57<25:09,  1.45it/s]
em: 0.1150, f1: 0.1725, loss: 6.3543 ||:  11%|#1        | 271/2445 [03:09<25:38,  1.41it/s]
em: 0.1171, f1: 0.1752, loss: 6.2950 ||:  12%|#1        | 288/2445 [03:20<24:31,  1.47it/s]
em: 0.1196, f1: 0.1785, loss: 6.2349 ||:  12%|#2        | 305/2445 [03:33<25:06,  1.42it/s]
em: 0.1224, f1: 0.1803, loss: 6.1910 ||:  13%|#3        | 321/2445 [03:43<24:18,  1.46it/s]
em: 0.1263, f1: 0.1844, loss: 6.1353 ||:  14%|#3        | 337/2445 [03:53<23:40,  1.48it/s]
em: 0.1282, f1: 0.1863, loss: 6.1004 ||:  14%|#4        | 353/2445 [04:05<24:10,  1.44it/s]
em: 0.1281, f1: 0.1862, loss: 6.0742 ||:  15%|#5        | 369/2445 [04:16<23:42,  1.46it/s]
em: 0.1321, f1: 0.1893, loss: 6.0393 ||:  16%|#5        | 385/2445 [04:28<24:21,  1.41it/s]
em: 0.1332, f1: 0.1913, loss: 5.9954 ||:  16%|#6        | 402/2445 [04:38<23:03,  1.48it/s]
em: 0.1344, f1: 0.1931, loss: 5.9625 ||:  17%|#7        | 419/2445 [04:51<23:34,  1.43it/s]
em: 0.1350, f1: 0.1947, loss: 5.9391 ||:  18%|#7        | 436/2445 [05:02<22:36,  1.48it/s]
em: 0.1370, f1: 0.1965, loss: 5.8961 ||:  19%|#8        | 453/2445 [05:13<22:06,  1.50it/s]
em: 0.1393, f1: 0.1992, loss: 5.8765 ||:  19%|#9        | 469/2445 [05:24<22:26,  1.47it/s]
em: 0.1390, f1: 0.1991, loss: 5.8646 ||:  20%|#9        | 485/2445 [05:34<21:51,  1.49it/s]
em: 0.1397, f1: 0.2002, loss: 5.8404 ||:  20%|##        | 501/2445 [05:47<22:43,  1.43it/s]
em: 0.1399, f1: 0.2010, loss: 5.8197 ||:  21%|##1       | 519/2445 [05:57<21:24,  1.50it/s]
em: 0.1426, f1: 0.2043, loss: 5.7855 ||:  22%|##1       | 537/2445 [06:10<21:51,  1.46it/s]
em: 0.1438, f1: 0.2056, loss: 5.7639 ||:  23%|##2       | 553/2445 [06:21<21:10,  1.49it/s]
em: 0.1438, f1: 0.2064, loss: 5.7487 ||:  23%|##3       | 569/2445 [06:31<20:46,  1.51it/s]
em: 0.1450, f1: 0.2079, loss: 5.7325 ||:  24%|##3       | 585/2445 [06:43<21:17,  1.46it/s]
em: 0.1466, f1: 0.2100, loss: 5.7154 ||:  25%|##4       | 600/2445 [06:53<20:56,  1.47it/s]
em: 0.1483, f1: 0.2117, loss: 5.7025 ||:  25%|##5       | 615/2445 [07:04<21:23,  1.43it/s]
em: 0.1480, f1: 0.2113, loss: 5.7003 ||:  26%|##5       | 631/2445 [07:15<20:46,  1.45it/s]
em: 0.1496, f1: 0.2125, loss: 5.6837 ||:  26%|##6       | 647/2445 [07:26<20:46,  1.44it/s]
em: 0.1504, f1: 0.2133, loss: 5.6723 ||:  27%|##7       | 663/2445 [07:36<20:07,  1.48it/s]
em: 0.1504, f1: 0.2137, loss: 5.6593 ||:  28%|##7       | 679/2445 [07:47<19:47,  1.49it/s]
em: 0.1511, f1: 0.2143, loss: 5.6455 ||:  28%|##8       | 695/2445 [08:01<21:40,  1.35it/s]
em: 0.1524, f1: 0.2156, loss: 5.6269 ||:  29%|##9       | 711/2445 [08:11<20:27,  1.41it/s]
em: 0.1532, f1: 0.2163, loss: 5.6140 ||:  30%|##9       | 727/2445 [08:24<20:56,  1.37it/s]
em: 0.1552, f1: 0.2185, loss: 5.5966 ||:  30%|###       | 743/2445 [08:34<20:05,  1.41it/s]
em: 0.1563, f1: 0.2192, loss: 5.5828 ||:  31%|###1      | 759/2445 [08:45<19:28,  1.44it/s]
em: 0.1568, f1: 0.2197, loss: 5.5747 ||:  32%|###1      | 775/2445 [08:57<19:55,  1.40it/s]
em: 0.1570, f1: 0.2200, loss: 5.5617 ||:  32%|###2      | 792/2445 [09:08<18:58,  1.45it/s]
em: 0.1574, f1: 0.2203, loss: 5.5510 ||:  33%|###3      | 808/2445 [09:21<19:46,  1.38it/s]
em: 0.1586, f1: 0.2212, loss: 5.5352 ||:  34%|###3      | 823/2445 [09:31<19:07,  1.41it/s]
em: 0.1592, f1: 0.2223, loss: 5.5199 ||:  34%|###4      | 838/2445 [09:42<19:33,  1.37it/s]
em: 0.1597, f1: 0.2225, loss: 5.5083 ||:  35%|###4      | 853/2445 [09:53<18:59,  1.40it/s]
em: 0.1611, f1: 0.2240, loss: 5.4918 ||:  36%|###5      | 868/2445 [10:03<18:28,  1.42it/s]
em: 0.1619, f1: 0.2246, loss: 5.4812 ||:  36%|###6      | 883/2445 [10:14<18:54,  1.38it/s]
em: 0.1627, f1: 0.2257, loss: 5.4721 ||:  37%|###6      | 898/2445 [10:24<18:15,  1.41it/s]
em: 0.1631, f1: 0.2259, loss: 5.4592 ||:  37%|###7      | 913/2445 [10:37<18:55,  1.35it/s]
em: 0.1640, f1: 0.2267, loss: 5.4492 ||:  38%|###7      | 929/2445 [10:47<18:05,  1.40it/s]
em: 0.1641, f1: 0.2270, loss: 5.4381 ||:  39%|###8      | 945/2445 [10:58<17:27,  1.43it/s]
em: 0.1648, f1: 0.2277, loss: 5.4325 ||:  39%|###9      | 961/2445 [11:10<17:37,  1.40it/s]
em: 0.1650, f1: 0.2279, loss: 5.4259 ||:  40%|####      | 978/2445 [11:20<16:45,  1.46it/s]
em: 0.1659, f1: 0.2286, loss: 5.4145 ||:  41%|####      | 995/2445 [11:34<17:25,  1.39it/s]
em: 0.1668, f1: 0.2294, loss: 5.4011 ||:  41%|####1     | 1011/2445 [11:44<16:48,  1.42it/s]
em: 0.1672, f1: 0.2300, loss: 5.3913 ||:  42%|####2     | 1027/2445 [11:57<16:59,  1.39it/s]
em: 0.1676, f1: 0.2304, loss: 5.3809 ||:  43%|####2     | 1043/2445 [12:07<16:23,  1.43it/s]
em: 0.1680, f1: 0.2308, loss: 5.3685 ||:  43%|####3     | 1059/2445 [12:17<15:42,  1.47it/s]
em: 0.1682, f1: 0.2313, loss: 5.3597 ||:  44%|####3     | 1075/2445 [12:30<16:24,  1.39it/s]
em: 0.1697, f1: 0.2328, loss: 5.3527 ||:  45%|####4     | 1091/2445 [12:40<15:40,  1.44it/s]
em: 0.1703, f1: 0.2336, loss: 5.3434 ||:  45%|####5     | 1107/2445 [12:53<15:58,  1.40it/s]
em: 0.1704, f1: 0.2335, loss: 5.3351 ||:  46%|####5     | 1123/2445 [13:03<15:19,  1.44it/s]
em: 0.1700, f1: 0.2331, loss: 5.3292 ||:  47%|####6     | 1139/2445 [13:13<14:48,  1.47it/s]
em: 0.1707, f1: 0.2339, loss: 5.3197 ||:  47%|####7     | 1155/2445 [13:25<15:04,  1.43it/s]
em: 0.1708, f1: 0.2341, loss: 5.3104 ||:  48%|####7     | 1171/2445 [13:36<14:36,  1.45it/s]
em: 0.1718, f1: 0.2351, loss: 5.2987 ||:  49%|####8     | 1187/2445 [13:48<15:02,  1.39it/s]
em: 0.1723, f1: 0.2356, loss: 5.2897 ||:  49%|####9     | 1202/2445 [13:59<14:42,  1.41it/s]
em: 0.1726, f1: 0.2359, loss: 5.2838 ||:  50%|####9     | 1217/2445 [14:12<15:30,  1.32it/s]
em: 0.1728, f1: 0.2359, loss: 5.2821 ||:  50%|#####     | 1233/2445 [14:22<14:40,  1.38it/s]
em: 0.1732, f1: 0.2363, loss: 5.2793 ||:  51%|#####1    | 1249/2445 [14:33<13:58,  1.43it/s]
em: 0.1748, f1: 0.2379, loss: 5.2691 ||:  52%|#####1    | 1265/2445 [14:45<14:20,  1.37it/s]
em: 0.1753, f1: 0.2384, loss: 5.2625 ||:  52%|#####2    | 1281/2445 [14:55<13:34,  1.43it/s]
em: 0.1761, f1: 0.2392, loss: 5.2533 ||:  53%|#####3    | 1297/2445 [15:07<13:42,  1.40it/s]
em: 0.1767, f1: 0.2400, loss: 5.2392 ||:  54%|#####3    | 1314/2445 [15:18<12:58,  1.45it/s]
em: 0.1781, f1: 0.2414, loss: 5.2268 ||:  54%|#####4    | 1331/2445 [15:34<14:08,  1.31it/s]
em: 0.1792, f1: 0.2424, loss: 5.2176 ||:  55%|#####5    | 1347/2445 [15:44<13:12,  1.39it/s]
em: 0.1804, f1: 0.2435, loss: 5.2061 ||:  56%|#####5    | 1364/2445 [15:55<12:28,  1.44it/s]
em: 0.1808, f1: 0.2440, loss: 5.2018 ||:  56%|#####6    | 1380/2445 [16:07<12:40,  1.40it/s]
em: 0.1809, f1: 0.2441, loss: 5.1965 ||:  57%|#####7    | 1396/2445 [16:17<12:02,  1.45it/s]
em: 0.1813, f1: 0.2445, loss: 5.1909 ||:  58%|#####7    | 1412/2445 [16:29<12:19,  1.40it/s]
em: 0.1817, f1: 0.2449, loss: 5.1875 ||:  58%|#####8    | 1427/2445 [16:39<11:56,  1.42it/s]
em: 0.1824, f1: 0.2457, loss: 5.1784 ||:  59%|#####9    | 1443/2445 [16:50<11:31,  1.45it/s]
em: 0.1825, f1: 0.2461, loss: 5.1730 ||:  60%|#####9    | 1459/2445 [17:02<11:30,  1.43it/s]
em: 0.1832, f1: 0.2466, loss: 5.1668 ||:  60%|######    | 1475/2445 [17:12<11:00,  1.47it/s]
em: 0.1834, f1: 0.2468, loss: 5.1624 ||:  61%|######    | 1491/2445 [17:24<11:10,  1.42it/s]
em: 0.1838, f1: 0.2471, loss: 5.1540 ||:  62%|######1   | 1507/2445 [17:34<10:45,  1.45it/s]
em: 0.1846, f1: 0.2478, loss: 5.1453 ||:  62%|######2   | 1523/2445 [17:46<10:51,  1.42it/s]
em: 0.1847, f1: 0.2478, loss: 5.1392 ||:  63%|######2   | 1538/2445 [17:57<10:35,  1.43it/s]
em: 0.1854, f1: 0.2486, loss: 5.1334 ||:  64%|######3   | 1554/2445 [18:07<10:10,  1.46it/s]
em: 0.1861, f1: 0.2492, loss: 5.1246 ||:  64%|######4   | 1570/2445 [18:19<10:23,  1.40it/s]
em: 0.1862, f1: 0.2493, loss: 5.1180 ||:  65%|######4   | 1586/2445 [18:30<09:55,  1.44it/s]
em: 0.1870, f1: 0.2498, loss: 5.1096 ||:  66%|######5   | 1602/2445 [18:43<10:17,  1.37it/s]
em: 0.1874, f1: 0.2502, loss: 5.1060 ||:  66%|######6   | 1618/2445 [18:53<09:46,  1.41it/s]
em: 0.1876, f1: 0.2505, loss: 5.1022 ||:  67%|######6   | 1634/2445 [19:04<09:23,  1.44it/s]
em: 0.1881, f1: 0.2510, loss: 5.0977 ||:  67%|######7   | 1650/2445 [19:17<09:35,  1.38it/s]
em: 0.1884, f1: 0.2515, loss: 5.0945 ||:  68%|######8   | 1666/2445 [19:27<09:09,  1.42it/s]
em: 0.1893, f1: 0.2526, loss: 5.0854 ||:  69%|######8   | 1682/2445 [19:40<09:13,  1.38it/s]
em: 0.1903, f1: 0.2537, loss: 5.0804 ||:  69%|######9   | 1698/2445 [19:50<08:47,  1.42it/s]
em: 0.1909, f1: 0.2544, loss: 5.0716 ||:  70%|#######   | 1714/2445 [20:02<08:41,  1.40it/s]
em: 0.1915, f1: 0.2549, loss: 5.0699 ||:  71%|#######   | 1730/2445 [20:13<08:20,  1.43it/s]
em: 0.1919, f1: 0.2551, loss: 5.0711 ||:  71%|#######1  | 1746/2445 [20:23<08:00,  1.45it/s]
em: 0.1922, f1: 0.2555, loss: 5.0701 ||:  72%|#######2  | 1762/2445 [20:35<08:04,  1.41it/s]
em: 0.1925, f1: 0.2556, loss: 5.0694 ||:  73%|#######2  | 1778/2445 [20:46<07:38,  1.45it/s]
em: 0.1931, f1: 0.2560, loss: 5.0623 ||:  73%|#######3  | 1794/2445 [20:58<07:42,  1.41it/s]
em: 0.1931, f1: 0.2561, loss: 5.0607 ||:  74%|#######3  | 1809/2445 [21:08<07:24,  1.43it/s]
em: 0.1936, f1: 0.2564, loss: 5.0552 ||:  75%|#######4  | 1825/2445 [21:20<07:21,  1.40it/s]
em: 0.1941, f1: 0.2570, loss: 5.0488 ||:  75%|#######5  | 1841/2445 [21:30<06:58,  1.44it/s]
em: 0.1947, f1: 0.2575, loss: 5.0409 ||:  76%|#######5  | 1857/2445 [21:40<06:38,  1.48it/s]
em: 0.1951, f1: 0.2578, loss: 5.0353 ||:  77%|#######6  | 1873/2445 [21:53<06:42,  1.42it/s]
em: 0.1956, f1: 0.2583, loss: 5.0314 ||:  77%|#######7  | 1889/2445 [22:03<06:24,  1.45it/s]
em: 0.1963, f1: 0.2589, loss: 5.0251 ||:  78%|#######7  | 1905/2445 [22:16<06:26,  1.40it/s]
em: 0.1966, f1: 0.2590, loss: 5.0186 ||:  79%|#######8  | 1922/2445 [22:26<05:57,  1.46it/s]
em: 0.1969, f1: 0.2592, loss: 5.0116 ||:  79%|#######9  | 1939/2445 [22:39<06:00,  1.40it/s]
em: 0.1973, f1: 0.2596, loss: 5.0088 ||:  80%|#######9  | 1955/2445 [22:50<05:40,  1.44it/s]
em: 0.1978, f1: 0.2602, loss: 5.0037 ||:  81%|########  | 1972/2445 [23:00<05:18,  1.48it/s]
em: 0.1981, f1: 0.2604, loss: 4.9998 ||:  81%|########1 | 1988/2445 [23:12<05:17,  1.44it/s]
em: 0.1987, f1: 0.2609, loss: 4.9931 ||:  82%|########1 | 2004/2445 [23:23<05:02,  1.46it/s]
em: 0.1988, f1: 0.2611, loss: 4.9899 ||:  83%|########2 | 2020/2445 [23:35<05:03,  1.40it/s]
em: 0.1988, f1: 0.2612, loss: 4.9861 ||:  83%|########3 | 2036/2445 [23:46<04:44,  1.44it/s]
em: 0.1993, f1: 0.2617, loss: 4.9830 ||:  84%|########3 | 2052/2445 [23:56<04:29,  1.46it/s]
em: 0.2000, f1: 0.2624, loss: 4.9789 ||:  85%|########4 | 2068/2445 [24:09<04:29,  1.40it/s]
em: 0.2005, f1: 0.2628, loss: 4.9741 ||:  85%|########5 | 2084/2445 [24:19<04:08,  1.45it/s]
em: 0.2012, f1: 0.2635, loss: 4.9686 ||:  86%|########5 | 2100/2445 [24:31<04:02,  1.42it/s]
em: 0.2015, f1: 0.2638, loss: 4.9653 ||:  87%|########6 | 2116/2445 [24:41<03:46,  1.45it/s]
em: 0.2019, f1: 0.2640, loss: 4.9613 ||:  87%|########7 | 2132/2445 [24:57<04:05,  1.28it/s]
em: 0.2026, f1: 0.2647, loss: 4.9595 ||:  88%|########7 | 2148/2445 [25:08<03:40,  1.35it/s]
em: 0.2028, f1: 0.2650, loss: 4.9564 ||:  89%|########8 | 2165/2445 [25:18<03:18,  1.41it/s]
em: 0.2033, f1: 0.2653, loss: 4.9533 ||:  89%|########9 | 2181/2445 [25:31<03:12,  1.37it/s]
em: 0.2034, f1: 0.2653, loss: 4.9470 ||:  90%|########9 | 2197/2445 [25:41<02:54,  1.42it/s]
em: 0.2036, f1: 0.2655, loss: 4.9452 ||:  91%|######### | 2213/2445 [25:53<02:47,  1.39it/s]
em: 0.2042, f1: 0.2660, loss: 4.9413 ||:  91%|#########1| 2229/2445 [26:04<02:30,  1.43it/s]
em: 0.2048, f1: 0.2665, loss: 4.9372 ||:  92%|#########1| 2245/2445 [26:16<02:23,  1.39it/s]
em: 0.2050, f1: 0.2667, loss: 4.9331 ||:  92%|#########2| 2261/2445 [26:26<02:08,  1.43it/s]
em: 0.2054, f1: 0.2671, loss: 4.9303 ||:  93%|#########3| 2277/2445 [26:36<01:54,  1.47it/s]
em: 0.2060, f1: 0.2678, loss: 4.9244 ||:  94%|#########3| 2293/2445 [26:48<01:46,  1.43it/s]
em: 0.2061, f1: 0.2681, loss: 4.9203 ||:  94%|#########4| 2308/2445 [26:58<01:34,  1.44it/s]
em: 0.2064, f1: 0.2684, loss: 4.9160 ||:  95%|#########5| 2323/2445 [27:10<01:27,  1.39it/s]
em: 0.2063, f1: 0.2685, loss: 4.9110 ||:  96%|#########5| 2340/2445 [27:21<01:12,  1.45it/s]
em: 0.2067, f1: 0.2689, loss: 4.9047 ||:  96%|#########6| 2357/2445 [27:33<01:02,  1.41it/s]
em: 0.2064, f1: 0.2688, loss: 4.9015 ||:  97%|#########7| 2373/2445 [27:44<00:49,  1.46it/s]
em: 0.2066, f1: 0.2691, loss: 4.8950 ||:  98%|#########7| 2389/2445 [27:54<00:37,  1.49it/s]
em: 0.2068, f1: 0.2694, loss: 4.8906 ||:  98%|#########8| 2405/2445 [28:05<00:27,  1.45it/s]
em: 0.2068, f1: 0.2695, loss: 4.8872 ||:  99%|#########9| 2421/2445 [28:16<00:16,  1.47it/s]
em: 0.2066, f1: 0.2695, loss: 4.8831 ||: 100%|#########9| 2437/2445 [28:28<00:05,  1.42it/s]
em: 0.2066, f1: 0.2696, loss: 4.8789 ||: : 2452it [28:38,  1.44it/s]                        
em: 0.2064, f1: 0.2694, loss: 4.8761 ||: : 2468it [28:49,  1.47it/s]
em: 0.2064, f1: 0.2695, loss: 4.8732 ||: : 2478it [28:55,  1.43it/s]

2019-05-09 20:23:23,488 - INFO - allennlp.training.trainer - Validating
  0%|          | 0/596 [00:00<?, ?it/s]
em: 0.2446, f1: 0.2674, loss: 1277182.8236 ||:   4%|3         | 23/596 [00:10<04:20,  2.20it/s]
em: 0.2418, f1: 0.2671, loss: 1127727.0608 ||:   8%|7         | 46/596 [00:26<04:47,  1.91it/s]
em: 0.2376, f1: 0.2667, loss: 1373115.3408 ||:  11%|#1        | 66/596 [00:36<04:36,  1.91it/s]
em: 0.2331, f1: 0.2640, loss: 1627915.6782 ||:  14%|#4        | 86/596 [00:49<04:47,  1.78it/s]
em: 0.2296, f1: 0.2586, loss: 1654770.7174 ||:  18%|#7        | 105/596 [00:59<04:31,  1.81it/s]
em: 0.2245, f1: 0.2560, loss: 1688516.8991 ||:  21%|##        | 124/596 [01:11<04:30,  1.75it/s]
em: 0.2158, f1: 0.2475, loss: 1671016.0615 ||:  24%|##4       | 144/596 [01:21<04:11,  1.79it/s]
em: 0.2091, f1: 0.2395, loss: 1726381.1382 ||:  28%|##7       | 164/596 [01:35<04:15,  1.69it/s]
em: 0.2045, f1: 0.2349, loss: 1703305.9883 ||:  31%|###       | 182/596 [01:45<04:03,  1.70it/s]
em: 0.1997, f1: 0.2289, loss: 1690634.4897 ||:  34%|###3      | 200/596 [01:56<03:55,  1.68it/s]
em: 0.1966, f1: 0.2246, loss: 1642055.0149 ||:  37%|###6      | 220/596 [02:07<03:34,  1.75it/s]
em: 0.1962, f1: 0.2239, loss: 1638030.4147 ||:  40%|####      | 240/596 [02:18<03:23,  1.75it/s]
em: 0.1985, f1: 0.2278, loss: 1584312.0164 ||:  43%|####3     | 258/596 [02:28<03:12,  1.76it/s]
em: 0.2007, f1: 0.2292, loss: 1616857.5054 ||:  46%|####6     | 276/596 [02:40<03:08,  1.70it/s]
em: 0.2000, f1: 0.2285, loss: 1545617.9911 ||:  50%|####9     | 296/596 [02:50<02:51,  1.75it/s]
em: 0.1998, f1: 0.2273, loss: 1640882.7094 ||:  53%|#####2    | 315/596 [03:01<02:39,  1.76it/s]
em: 0.1983, f1: 0.2258, loss: 1578463.3684 ||:  56%|#####5    | 333/596 [03:11<02:29,  1.76it/s]
em: 0.1984, f1: 0.2251, loss: 1615038.4986 ||:  59%|#####8    | 351/596 [03:24<02:28,  1.65it/s]
em: 0.1999, f1: 0.2269, loss: 1548190.6378 ||:  62%|######2   | 371/596 [03:34<02:10,  1.72it/s]
em: 0.2009, f1: 0.2280, loss: 1579293.8425 ||:  66%|######5   | 391/596 [03:47<02:03,  1.66it/s]
em: 0.1998, f1: 0.2261, loss: 1558708.3677 ||:  69%|######8   | 411/596 [03:58<01:47,  1.72it/s]
em: 0.1991, f1: 0.2263, loss: 1543614.7902 ||:  72%|#######2  | 430/596 [04:10<01:39,  1.67it/s]
em: 0.1976, f1: 0.2252, loss: 1507526.9356 ||:  75%|#######5  | 449/596 [04:21<01:26,  1.71it/s]
em: 0.1948, f1: 0.2221, loss: 1579239.2754 ||:  78%|#######8  | 467/596 [04:31<01:16,  1.70it/s]
em: 0.1902, f1: 0.2177, loss: 1713967.0442 ||:  82%|########2 | 489/596 [04:42<00:59,  1.81it/s]
em: 0.1900, f1: 0.2176, loss: 1805565.3202 ||:  86%|########6 | 513/596 [04:52<00:42,  1.95it/s]
em: 0.1902, f1: 0.2174, loss: 1851732.1924 ||:  90%|######### | 537/596 [05:04<00:30,  1.93it/s]
em: 0.1858, f1: 0.2128, loss: 1909484.8208 ||:  94%|#########4| 562/596 [05:15<00:16,  2.06it/s]
em: 0.1819, f1: 0.2088, loss: 1982548.0446 ||:  98%|#########8| 587/596 [05:30<00:04,  1.90it/s]
em: 0.1797, f1: 0.2066, loss: 1971243.3202 ||: : 604it [05:43,  1.69it/s]                       

2019-05-09 20:29:06,858 - INFO - allennlp.training.tensorboard_writer -                     Training |  Validation
2019-05-09 20:29:06,859 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB |  1723.000  |       N/A
2019-05-09 20:29:06,860 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB   |  5332.732  |       N/A
2019-05-09 20:29:06,860 - INFO - allennlp.training.tensorboard_writer - loss            |     4.873  |  1971243.320
2019-05-09 20:29:06,861 - INFO - allennlp.training.tensorboard_writer - em              |     0.206  |     0.180
2019-05-09 20:29:06,861 - INFO - allennlp.training.tensorboard_writer - f1              |     0.270  |     0.207
2019-05-09 20:29:49,464 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to 'out/rc_bert_simple_v2/best.th'.
2019-05-09 20:30:31,810 - INFO - allennlp.training.trainer - Epoch duration: 00:36:04
2019-05-09 20:30:31,810 - INFO - allennlp.training.trainer - Estimated training time remaining: 5:24:36
2019-05-09 20:30:31,811 - INFO - allennlp.training.trainer - Epoch 1/9
2019-05-09 20:30:31,811 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 6465.984
2019-05-09 20:30:31,933 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 4963
2019-05-09 20:30:31,940 - INFO - allennlp.training.trainer - Training
  0%|          | 0/2445 [00:00<?, ?it/s]
em: 0.2750, f1: 0.3488, loss: 3.9179 ||:   1%|          | 15/2445 [00:10<27:47,  1.46it/s]
em: 0.2937, f1: 0.3735, loss: 3.7844 ||:   1%|1         | 32/2445 [00:20<26:52,  1.50it/s]
em: 0.2855, f1: 0.3549, loss: 3.9837 ||:   2%|1         | 48/2445 [00:31<26:37,  1.50it/s]
em: 0.2808, f1: 0.3440, loss: 4.1477 ||:   3%|2         | 64/2445 [00:42<26:22,  1.50it/s]
em: 0.2818, f1: 0.3443, loss: 4.1507 ||:   3%|3         | 80/2445 [00:52<25:53,  1.52it/s]
em: 0.2751, f1: 0.3455, loss: 4.1407 ||:   4%|3         | 96/2445 [01:02<25:42,  1.52it/s]
em: 0.2749, f1: 0.3451, loss: 4.1258 ||:   5%|4         | 112/2445 [01:13<25:23,  1.53it/s]
em: 0.2816, f1: 0.3497, loss: 4.0936 ||:   5%|5         | 128/2445 [01:23<25:25,  1.52it/s]
em: 0.2811, f1: 0.3466, loss: 4.1030 ||:   6%|5         | 144/2445 [01:34<25:06,  1.53it/s]
em: 0.2816, f1: 0.3459, loss: 4.0963 ||:   7%|6         | 160/2445 [01:45<25:26,  1.50it/s]
em: 0.2828, f1: 0.3486, loss: 4.0983 ||:   7%|7         | 176/2445 [01:55<25:11,  1.50it/s]
em: 0.2889, f1: 0.3575, loss: 4.0778 ||:   8%|7         | 192/2445 [02:06<24:54,  1.51it/s]
em: 0.2872, f1: 0.3561, loss: 4.0914 ||:   9%|8         | 208/2445 [02:16<24:35,  1.52it/s]
em: 0.2902, f1: 0.3581, loss: 4.1305 ||:   9%|9         | 224/2445 [02:28<24:50,  1.49it/s]
em: 0.2908, f1: 0.3578, loss: 4.1254 ||:  10%|9         | 239/2445 [02:38<24:50,  1.48it/s]
em: 0.2941, f1: 0.3601, loss: 4.1148 ||:  10%|#         | 255/2445 [02:49<24:33,  1.49it/s]
em: 0.2935, f1: 0.3593, loss: 4.1233 ||:  11%|#1        | 271/2445 [02:59<24:20,  1.49it/s]
em: 0.2966, f1: 0.3622, loss: 4.1105 ||:  12%|#1        | 287/2445 [03:09<23:44,  1.52it/s]
em: 0.2987, f1: 0.3635, loss: 4.0943 ||:  12%|#2        | 303/2445 [03:20<23:20,  1.53it/s]
em: 0.2975, f1: 0.3622, loss: 4.0704 ||:  13%|#3        | 319/2445 [03:30<23:22,  1.52it/s]
em: 0.2953, f1: 0.3592, loss: 4.0783 ||:  14%|#3        | 334/2445 [03:40<23:21,  1.51it/s]
em: 0.2938, f1: 0.3572, loss: 4.0852 ||:  14%|#4        | 349/2445 [03:50<23:15,  1.50it/s]
em: 0.2921, f1: 0.3563, loss: 4.0945 ||:  15%|#4        | 364/2445 [04:01<23:10,  1.50it/s]
em: 0.2915, f1: 0.3558, loss: 4.0843 ||:  16%|#5        | 380/2445 [04:11<22:43,  1.51it/s]
em: 0.2925, f1: 0.3564, loss: 4.0763 ||:  16%|#6        | 396/2445 [04:21<22:24,  1.52it/s]
em: 0.2915, f1: 0.3553, loss: 4.0862 ||:  17%|#6        | 412/2445 [04:31<22:02,  1.54it/s]
em: 0.2917, f1: 0.3554, loss: 4.0909 ||:  18%|#7        | 428/2445 [04:42<21:56,  1.53it/s]
em: 0.2914, f1: 0.3548, loss: 4.0832 ||:  18%|#8        | 445/2445 [04:53<21:29,  1.55it/s]
em: 0.2916, f1: 0.3547, loss: 4.0813 ||:  19%|#8        | 461/2445 [05:03<21:11,  1.56it/s]
em: 0.2899, f1: 0.3532, loss: 4.0942 ||:  20%|#9        | 477/2445 [05:13<21:15,  1.54it/s]
em: 0.2893, f1: 0.3524, loss: 4.1027 ||:  20%|##        | 493/2445 [05:24<21:14,  1.53it/s]
em: 0.2862, f1: 0.3498, loss: 4.1186 ||:  21%|##        | 509/2445 [05:34<20:58,  1.54it/s]
em: 0.2871, f1: 0.3511, loss: 4.1110 ||:  22%|##1       | 526/2445 [05:45<20:24,  1.57it/s]
em: 0.2880, f1: 0.3523, loss: 4.1120 ||:  22%|##2       | 543/2445 [05:56<20:24,  1.55it/s]
em: 0.2882, f1: 0.3527, loss: 4.1113 ||:  23%|##2       | 560/2445 [06:06<19:55,  1.58it/s]
em: 0.2894, f1: 0.3543, loss: 4.1129 ||:  24%|##3       | 577/2445 [06:18<20:03,  1.55it/s]
em: 0.2896, f1: 0.3545, loss: 4.1107 ||:  24%|##4       | 593/2445 [06:28<19:52,  1.55it/s]
em: 0.2905, f1: 0.3556, loss: 4.1139 ||:  25%|##4       | 609/2445 [06:39<20:18,  1.51it/s]
em: 0.2891, f1: 0.3541, loss: 4.1268 ||:  26%|##5       | 625/2445 [06:49<19:54,  1.52it/s]
em: 0.2901, f1: 0.3552, loss: 4.1249 ||:  26%|##6       | 641/2445 [07:00<19:43,  1.52it/s]
em: 0.2904, f1: 0.3549, loss: 4.1313 ||:  27%|##6       | 657/2445 [07:11<19:51,  1.50it/s]
em: 0.2899, f1: 0.3550, loss: 4.1294 ||:  28%|##7       | 674/2445 [07:22<19:15,  1.53it/s]
em: 0.2901, f1: 0.3553, loss: 4.1325 ||:  28%|##8       | 691/2445 [07:33<19:27,  1.50it/s]
em: 0.2895, f1: 0.3544, loss: 4.1269 ||:  29%|##8       | 707/2445 [07:44<19:04,  1.52it/s]
em: 0.2904, f1: 0.3550, loss: 4.1221 ||:  30%|##9       | 723/2445 [07:54<19:03,  1.51it/s]
em: 0.2900, f1: 0.3544, loss: 4.1232 ||:  30%|###       | 739/2445 [08:05<18:39,  1.52it/s]
em: 0.2906, f1: 0.3550, loss: 4.1230 ||:  31%|###       | 755/2445 [08:15<18:34,  1.52it/s]
em: 0.2911, f1: 0.3556, loss: 4.1229 ||:  32%|###1      | 771/2445 [08:26<18:19,  1.52it/s]
em: 0.2906, f1: 0.3551, loss: 4.1268 ||:  32%|###2      | 788/2445 [08:36<17:51,  1.55it/s]
em: 0.2901, f1: 0.3546, loss: 4.1337 ||:  33%|###2      | 805/2445 [08:48<17:51,  1.53it/s]
em: 0.2900, f1: 0.3550, loss: 4.1269 ||:  34%|###3      | 821/2445 [08:58<17:34,  1.54it/s]
em: 0.2895, f1: 0.3548, loss: 4.1226 ||:  34%|###4      | 837/2445 [09:09<17:43,  1.51it/s]
em: 0.2906, f1: 0.3557, loss: 4.1184 ||:  35%|###4      | 853/2445 [09:19<17:29,  1.52it/s]
em: 0.2902, f1: 0.3555, loss: 4.1115 ||:  36%|###5      | 869/2445 [09:30<17:25,  1.51it/s]
em: 0.2907, f1: 0.3561, loss: 4.1083 ||:  36%|###6      | 884/2445 [09:40<17:20,  1.50it/s]
em: 0.2910, f1: 0.3562, loss: 4.1036 ||:  37%|###6      | 900/2445 [09:50<16:55,  1.52it/s]
em: 0.2906, f1: 0.3557, loss: 4.1052 ||:  37%|###7      | 916/2445 [10:01<16:41,  1.53it/s]
em: 0.2902, f1: 0.3554, loss: 4.1026 ||:  38%|###8      | 932/2445 [10:12<16:37,  1.52it/s]
em: 0.2909, f1: 0.3559, loss: 4.0981 ||:  39%|###8      | 947/2445 [10:22<16:44,  1.49it/s]
em: 0.2912, f1: 0.3561, loss: 4.0990 ||:  39%|###9      | 963/2445 [10:33<16:31,  1.50it/s]
em: 0.2911, f1: 0.3561, loss: 4.1017 ||:  40%|####      | 979/2445 [10:43<16:06,  1.52it/s]
em: 0.2905, f1: 0.3556, loss: 4.1005 ||:  41%|####      | 995/2445 [10:54<16:03,  1.50it/s]
em: 0.2909, f1: 0.3560, loss: 4.0988 ||:  41%|####1     | 1011/2445 [11:04<15:46,  1.52it/s]
em: 0.2911, f1: 0.3562, loss: 4.0960 ||:  42%|####2     | 1027/2445 [11:14<15:30,  1.52it/s]
em: 0.2895, f1: 0.3548, loss: 4.0965 ||:  43%|####2     | 1043/2445 [11:25<15:32,  1.50it/s]
em: 0.2903, f1: 0.3554, loss: 4.0920 ||:  43%|####3     | 1059/2445 [11:36<15:13,  1.52it/s]
em: 0.2909, f1: 0.3559, loss: 4.0891 ||:  44%|####3     | 1075/2445 [11:46<14:50,  1.54it/s]
em: 0.2913, f1: 0.3565, loss: 4.0870 ||:  45%|####4     | 1091/2445 [11:56<14:32,  1.55it/s]
em: 0.2911, f1: 0.3560, loss: 4.0887 ||:  45%|####5     | 1107/2445 [12:06<14:26,  1.54it/s]
em: 0.2914, f1: 0.3563, loss: 4.0875 ||:  46%|####5     | 1123/2445 [12:17<14:21,  1.54it/s]
em: 0.2910, f1: 0.3559, loss: 4.0857 ||:  47%|####6     | 1139/2445 [12:27<14:11,  1.53it/s]
em: 0.2915, f1: 0.3566, loss: 4.0842 ||:  47%|####7     | 1155/2445 [12:38<14:12,  1.51it/s]
em: 0.2924, f1: 0.3573, loss: 4.0781 ||:  48%|####7     | 1171/2445 [12:49<13:58,  1.52it/s]
em: 0.2930, f1: 0.3577, loss: 4.0763 ||:  49%|####8     | 1187/2445 [12:59<13:42,  1.53it/s]
em: 0.2921, f1: 0.3570, loss: 4.0788 ||:  49%|####9     | 1203/2445 [13:10<13:35,  1.52it/s]
em: 0.2922, f1: 0.3572, loss: 4.0763 ||:  50%|####9     | 1219/2445 [13:20<13:27,  1.52it/s]
em: 0.2928, f1: 0.3578, loss: 4.0757 ||:  51%|#####     | 1235/2445 [13:31<13:15,  1.52it/s]
em: 0.2924, f1: 0.3578, loss: 4.0797 ||:  51%|#####1    | 1252/2445 [13:41<12:46,  1.56it/s]
em: 0.2923, f1: 0.3578, loss: 4.0760 ||:  52%|#####1    | 1269/2445 [13:52<12:45,  1.54it/s]
em: 0.2928, f1: 0.3586, loss: 4.0737 ||:  53%|#####2    | 1285/2445 [14:03<12:28,  1.55it/s]
em: 0.2934, f1: 0.3588, loss: 4.0725 ||:  53%|#####3    | 1301/2445 [14:13<12:21,  1.54it/s]
em: 0.2934, f1: 0.3587, loss: 4.0679 ||:  54%|#####3    | 1317/2445 [14:23<12:07,  1.55it/s]
em: 0.2937, f1: 0.3591, loss: 4.0643 ||:  55%|#####4    | 1333/2445 [14:33<11:55,  1.55it/s]
em: 0.2938, f1: 0.3594, loss: 4.0593 ||:  55%|#####5    | 1350/2445 [14:44<11:29,  1.59it/s]
em: 0.2942, f1: 0.3598, loss: 4.0542 ||:  56%|#####5    | 1367/2445 [14:54<11:20,  1.58it/s]
em: 0.2936, f1: 0.3593, loss: 4.0549 ||:  57%|#####6    | 1383/2445 [15:05<11:11,  1.58it/s]
em: 0.2935, f1: 0.3591, loss: 4.0524 ||:  57%|#####7    | 1399/2445 [15:15<11:05,  1.57it/s]
em: 0.2931, f1: 0.3585, loss: 4.0516 ||:  58%|#####7    | 1415/2445 [15:26<11:06,  1.55it/s]
em: 0.2930, f1: 0.3585, loss: 4.0526 ||:  58%|#####8    | 1430/2445 [15:36<11:10,  1.51it/s]
em: 0.2929, f1: 0.3585, loss: 4.0544 ||:  59%|#####9    | 1445/2445 [15:46<11:08,  1.50it/s]
em: 0.2933, f1: 0.3592, loss: 4.0509 ||:  60%|#####9    | 1462/2445 [15:57<10:41,  1.53it/s]
em: 0.2930, f1: 0.3590, loss: 4.0503 ||:  60%|######    | 1479/2445 [16:07<10:21,  1.56it/s]
em: 0.2937, f1: 0.3596, loss: 4.0421 ||:  61%|######1   | 1496/2445 [16:19<10:15,  1.54it/s]
em: 0.2936, f1: 0.3594, loss: 4.0405 ||:  62%|######1   | 1513/2445 [16:29<09:57,  1.56it/s]
em: 0.2936, f1: 0.3594, loss: 4.0407 ||:  63%|######2   | 1530/2445 [16:40<09:48,  1.55it/s]
em: 0.2942, f1: 0.3599, loss: 4.0358 ||:  63%|######3   | 1546/2445 [16:51<09:46,  1.53it/s]
em: 0.2951, f1: 0.3606, loss: 4.0318 ||:  64%|######3   | 1561/2445 [17:02<09:50,  1.50it/s]
em: 0.2953, f1: 0.3605, loss: 4.0279 ||:  64%|######4   | 1576/2445 [17:12<09:41,  1.49it/s]
em: 0.2956, f1: 0.3608, loss: 4.0244 ||:  65%|######5   | 1593/2445 [17:22<09:12,  1.54it/s]
em: 0.2957, f1: 0.3607, loss: 4.0277 ||:  66%|######5   | 1610/2445 [17:33<09:09,  1.52it/s]
em: 0.2959, f1: 0.3610, loss: 4.0283 ||:  67%|######6   | 1626/2445 [17:44<09:00,  1.51it/s]
em: 0.2961, f1: 0.3613, loss: 4.0288 ||:  67%|######7   | 1641/2445 [17:54<08:58,  1.49it/s]
em: 0.2963, f1: 0.3615, loss: 4.0261 ||:  68%|######7   | 1656/2445 [18:05<08:54,  1.48it/s]
em: 0.2964, f1: 0.3617, loss: 4.0275 ||:  68%|######8   | 1672/2445 [18:15<08:38,  1.49it/s]
em: 0.2976, f1: 0.3628, loss: 4.0214 ||:  69%|######9   | 1688/2445 [18:26<08:25,  1.50it/s]
em: 0.2981, f1: 0.3634, loss: 4.0160 ||:  70%|######9   | 1704/2445 [18:36<08:05,  1.53it/s]
em: 0.2982, f1: 0.3635, loss: 4.0172 ||:  70%|#######   | 1720/2445 [18:47<08:03,  1.50it/s]
em: 0.2980, f1: 0.3632, loss: 4.0210 ||:  71%|#######   | 1735/2445 [18:57<07:53,  1.50it/s]
em: 0.2982, f1: 0.3634, loss: 4.0241 ||:  72%|#######1  | 1751/2445 [19:08<07:40,  1.51it/s]
em: 0.2988, f1: 0.3639, loss: 4.0252 ||:  72%|#######2  | 1768/2445 [19:18<07:17,  1.55it/s]
em: 0.2993, f1: 0.3644, loss: 4.0281 ||:  73%|#######3  | 1785/2445 [19:29<07:11,  1.53it/s]
em: 0.2996, f1: 0.3647, loss: 4.0261 ||:  74%|#######3  | 1801/2445 [19:39<06:57,  1.54it/s]
em: 0.2999, f1: 0.3649, loss: 4.0247 ||:  74%|#######4  | 1817/2445 [19:50<06:47,  1.54it/s]
em: 0.3008, f1: 0.3657, loss: 4.0226 ||:  75%|#######4  | 1833/2445 [20:00<06:38,  1.54it/s]
em: 0.3010, f1: 0.3659, loss: 4.0209 ||:  76%|#######5  | 1849/2445 [20:11<06:26,  1.54it/s]
em: 0.3014, f1: 0.3664, loss: 4.0180 ||:  76%|#######6  | 1865/2445 [20:22<06:23,  1.51it/s]
em: 0.3017, f1: 0.3666, loss: 4.0153 ||:  77%|#######6  | 1882/2445 [20:32<06:05,  1.54it/s]
em: 0.3020, f1: 0.3669, loss: 4.0127 ||:  78%|#######7  | 1899/2445 [20:43<05:52,  1.55it/s]
em: 0.3026, f1: 0.3673, loss: 4.0099 ||:  78%|#######8  | 1916/2445 [20:54<05:38,  1.56it/s]
em: 0.3029, f1: 0.3677, loss: 4.0055 ||:  79%|#######9  | 1933/2445 [21:05<05:27,  1.56it/s]
em: 0.3027, f1: 0.3676, loss: 4.0054 ||:  80%|#######9  | 1949/2445 [21:15<05:21,  1.54it/s]
em: 0.3029, f1: 0.3677, loss: 4.0051 ||:  80%|########  | 1966/2445 [21:25<05:03,  1.58it/s]
em: 0.3032, f1: 0.3679, loss: 4.0060 ||:  81%|########1 | 1983/2445 [21:37<04:55,  1.56it/s]
em: 0.3036, f1: 0.3681, loss: 4.0042 ||:  82%|########1 | 2000/2445 [21:47<04:43,  1.57it/s]
em: 0.3037, f1: 0.3681, loss: 4.0035 ||:  82%|########2 | 2016/2445 [21:58<04:36,  1.55it/s]
em: 0.3037, f1: 0.3681, loss: 4.0046 ||:  83%|########3 | 2032/2445 [22:09<04:30,  1.53it/s]
em: 0.3035, f1: 0.3682, loss: 4.0050 ||:  84%|########3 | 2049/2445 [22:19<04:16,  1.54it/s]
em: 0.3040, f1: 0.3689, loss: 4.0027 ||:  84%|########4 | 2065/2445 [22:30<04:05,  1.55it/s]
em: 0.3041, f1: 0.3690, loss: 4.0021 ||:  85%|########5 | 2081/2445 [22:40<03:54,  1.55it/s]
em: 0.3041, f1: 0.3691, loss: 4.0040 ||:  86%|########5 | 2097/2445 [22:51<03:48,  1.52it/s]
em: 0.3044, f1: 0.3692, loss: 4.0005 ||:  86%|########6 | 2114/2445 [23:01<03:33,  1.55it/s]
em: 0.3051, f1: 0.3698, loss: 3.9988 ||:  87%|########7 | 2131/2445 [23:13<03:24,  1.53it/s]
em: 0.3053, f1: 0.3698, loss: 4.0015 ||:  88%|########7 | 2147/2445 [23:23<03:13,  1.54it/s]
em: 0.3055, f1: 0.3701, loss: 4.0017 ||:  88%|########8 | 2163/2445 [23:33<03:02,  1.55it/s]
em: 0.3059, f1: 0.3705, loss: 3.9977 ||:  89%|########9 | 2180/2445 [23:44<02:49,  1.56it/s]
em: 0.3058, f1: 0.3705, loss: 3.9946 ||:  90%|########9 | 2197/2445 [23:55<02:37,  1.57it/s]
em: 0.3059, f1: 0.3707, loss: 3.9936 ||:  91%|######### | 2213/2445 [24:05<02:29,  1.55it/s]
em: 0.3061, f1: 0.3709, loss: 3.9926 ||:  91%|#########1| 2229/2445 [24:16<02:19,  1.55it/s]
em: 0.3060, f1: 0.3707, loss: 3.9933 ||:  92%|#########1| 2245/2445 [24:26<02:08,  1.55it/s]
em: 0.3059, f1: 0.3707, loss: 3.9939 ||:  92%|#########2| 2261/2445 [24:36<01:59,  1.54it/s]
em: 0.3058, f1: 0.3705, loss: 3.9933 ||:  93%|#########3| 2277/2445 [24:47<01:48,  1.54it/s]
em: 0.3058, f1: 0.3707, loss: 3.9909 ||:  94%|#########3| 2293/2445 [24:57<01:38,  1.54it/s]
em: 0.3057, f1: 0.3707, loss: 3.9898 ||:  94%|#########4| 2309/2445 [25:08<01:28,  1.54it/s]
em: 0.3053, f1: 0.3705, loss: 3.9882 ||:  95%|#########5| 2325/2445 [25:18<01:18,  1.53it/s]
em: 0.3049, f1: 0.3703, loss: 3.9865 ||:  96%|#########5| 2342/2445 [25:29<01:06,  1.56it/s]
em: 0.3045, f1: 0.3699, loss: 3.9869 ||:  96%|#########6| 2359/2445 [25:40<00:56,  1.53it/s]
em: 0.3040, f1: 0.3693, loss: 3.9867 ||:  97%|#########7| 2375/2445 [25:50<00:45,  1.55it/s]
em: 0.3034, f1: 0.3689, loss: 3.9863 ||:  98%|#########7| 2391/2445 [26:01<00:34,  1.55it/s]
em: 0.3029, f1: 0.3686, loss: 3.9855 ||:  98%|#########8| 2407/2445 [26:11<00:24,  1.53it/s]
em: 0.3023, f1: 0.3684, loss: 3.9845 ||:  99%|#########9| 2424/2445 [26:22<00:13,  1.56it/s]
em: 0.3019, f1: 0.3681, loss: 3.9860 ||: 100%|#########9| 2441/2445 [26:33<00:02,  1.56it/s]
em: 0.3015, f1: 0.3678, loss: 3.9847 ||: : 2458it [26:43,  1.57it/s]                        
em: 0.3007, f1: 0.3673, loss: 3.9845 ||: : 2475it [26:54,  1.56it/s]
em: 0.3006, f1: 0.3672, loss: 3.9846 ||: : 2478it [26:56,  1.53it/s]

2019-05-09 20:57:28,447 - INFO - allennlp.training.trainer - Validating
  0%|          | 0/596 [00:00<?, ?it/s]
em: 0.2909, f1: 0.3123, loss: 1225971.1186 ||:   4%|4         | 26/596 [00:10<03:40,  2.58it/s]
em: 0.2621, f1: 0.2897, loss: 1237990.9303 ||:   9%|8         | 52/596 [00:25<04:01,  2.25it/s]
em: 0.2545, f1: 0.2859, loss: 1500009.5848 ||:  12%|#1        | 70/596 [00:35<04:16,  2.05it/s]
em: 0.2562, f1: 0.2877, loss: 1579679.5755 ||:  15%|#5        | 91/596 [00:45<04:07,  2.04it/s]
em: 0.2635, f1: 0.2921, loss: 1629473.5681 ||:  19%|#8        | 112/596 [01:01<04:35,  1.76it/s]
em: 0.2509, f1: 0.2812, loss: 1742710.1988 ||:  23%|##2       | 137/596 [01:12<03:59,  1.92it/s]
em: 0.2379, f1: 0.2674, loss: 1739978.7758 ||:  27%|##7       | 162/596 [01:27<03:57,  1.83it/s]
em: 0.2319, f1: 0.2601, loss: 1699872.4562 ||:  31%|###       | 182/596 [01:37<03:44,  1.84it/s]
em: 0.2266, f1: 0.2538, loss: 1699517.4434 ||:  34%|###4      | 203/596 [01:48<03:26,  1.90it/s]
em: 0.2257, f1: 0.2534, loss: 1618313.7113 ||:  38%|###7      | 224/596 [01:59<03:17,  1.88it/s]
em: 0.2290, f1: 0.2561, loss: 1632095.0965 ||:  41%|####1     | 247/596 [02:09<02:56,  1.98it/s]
em: 0.2317, f1: 0.2598, loss: 1599547.2639 ||:  45%|####5     | 270/596 [02:23<02:55,  1.86it/s]
em: 0.2338, f1: 0.2615, loss: 1568931.5777 ||:  49%|####8     | 292/596 [02:34<02:37,  1.93it/s]
em: 0.2321, f1: 0.2583, loss: 1630185.3958 ||:  53%|#####2    | 314/596 [02:45<02:24,  1.95it/s]
em: 0.2320, f1: 0.2582, loss: 1581223.0284 ||:  56%|#####6    | 334/596 [02:56<02:18,  1.89it/s]
em: 0.2319, f1: 0.2578, loss: 1612970.9120 ||:  59%|#####9    | 353/596 [03:06<02:08,  1.89it/s]
em: 0.2372, f1: 0.2636, loss: 1549069.7191 ||:  62%|######2   | 372/596 [03:17<02:00,  1.86it/s]
em: 0.2366, f1: 0.2637, loss: 1585688.2207 ||:  66%|######5   | 391/596 [03:27<01:50,  1.86it/s]
em: 0.2372, f1: 0.2634, loss: 1563270.9266 ||:  69%|######8   | 411/596 [03:37<01:37,  1.89it/s]
em: 0.2366, f1: 0.2635, loss: 1548734.5296 ||:  72%|#######2  | 431/596 [03:48<01:26,  1.91it/s]
em: 0.2350, f1: 0.2621, loss: 1511928.7123 ||:  76%|#######5  | 451/596 [03:59<01:19,  1.83it/s]
em: 0.2294, f1: 0.2565, loss: 1619208.9283 ||:  80%|#######9  | 474/596 [04:10<01:02,  1.94it/s]
em: 0.2262, f1: 0.2533, loss: 1726620.0788 ||:  83%|########3 | 497/596 [04:21<00:49,  1.98it/s]
em: 0.2237, f1: 0.2507, loss: 1838175.5449 ||:  88%|########8 | 526/596 [04:31<00:32,  2.18it/s]
em: 0.2227, f1: 0.2492, loss: 1913298.3627 ||:  93%|#########3| 555/596 [04:43<00:18,  2.25it/s]
em: 0.2172, f1: 0.2433, loss: 1967682.5624 ||:  97%|#########7| 580/596 [04:56<00:07,  2.14it/s]
em: 0.2148, f1: 0.2409, loss: 1978135.2261 ||: : 600it [05:08,  1.96it/s]                       
em: 0.2140, f1: 0.2402, loss: 1970208.9717 ||: : 604it [05:11,  1.94it/s]

2019-05-09 21:02:40,277 - INFO - allennlp.training.tensorboard_writer -                     Training |  Validation
2019-05-09 21:02:40,278 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB |  4963.000  |       N/A
2019-05-09 21:02:40,278 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB   |  6465.984  |       N/A
2019-05-09 21:02:40,278 - INFO - allennlp.training.tensorboard_writer - loss            |     3.985  |  1970208.972
2019-05-09 21:02:40,279 - INFO - allennlp.training.tensorboard_writer - em              |     0.301  |     0.214
2019-05-09 21:02:40,279 - INFO - allennlp.training.tensorboard_writer - f1              |     0.367  |     0.240
2019-05-09 21:03:22,813 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to 'out/rc_bert_simple_v2/best.th'.
2019-05-09 21:04:04,954 - INFO - allennlp.training.trainer - Epoch duration: 00:33:33
2019-05-09 21:04:04,955 - INFO - allennlp.training.trainer - Estimated training time remaining: 4:38:28
2019-05-09 21:04:04,955 - INFO - allennlp.training.trainer - Epoch 2/9
2019-05-09 21:04:04,955 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 6466.128
2019-05-09 21:04:05,083 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 5036
2019-05-09 21:04:05,089 - INFO - allennlp.training.trainer - Training
  0%|          | 0/2445 [00:00<?, ?it/s]
em: 0.3867, f1: 0.4406, loss: 3.2179 ||:   1%|          | 16/2445 [00:10<25:20,  1.60it/s]
em: 0.3887, f1: 0.4399, loss: 3.2761 ||:   1%|1         | 32/2445 [00:20<25:20,  1.59it/s]
em: 0.3829, f1: 0.4331, loss: 3.3361 ||:   2%|1         | 48/2445 [00:30<25:12,  1.59it/s]
em: 0.3621, f1: 0.4146, loss: 3.5270 ||:   3%|2         | 64/2445 [00:40<24:58,  1.59it/s]
em: 0.3489, f1: 0.4062, loss: 3.6017 ||:   3%|3         | 80/2445 [00:50<25:06,  1.57it/s]
em: 0.3568, f1: 0.4186, loss: 3.5425 ||:   4%|3         | 97/2445 [01:01<24:42,  1.58it/s]
em: 0.3550, f1: 0.4163, loss: 3.5429 ||:   5%|4         | 114/2445 [01:12<24:45,  1.57it/s]
em: 0.3521, f1: 0.4146, loss: 3.5427 ||:   5%|5         | 130/2445 [01:22<24:28,  1.58it/s]
em: 0.3468, f1: 0.4096, loss: 3.5471 ||:   6%|5         | 146/2445 [01:32<24:17,  1.58it/s]
em: 0.3574, f1: 0.4206, loss: 3.5046 ||:   7%|6         | 162/2445 [01:43<24:21,  1.56it/s]
em: 0.3548, f1: 0.4173, loss: 3.5538 ||:   7%|7         | 179/2445 [01:53<23:51,  1.58it/s]
em: 0.3579, f1: 0.4196, loss: 3.5692 ||:   8%|8         | 196/2445 [02:04<24:08,  1.55it/s]
em: 0.3535, f1: 0.4155, loss: 3.5944 ||:   9%|8         | 212/2445 [02:15<23:58,  1.55it/s]
em: 0.3489, f1: 0.4112, loss: 3.6325 ||:   9%|9         | 228/2445 [02:25<23:47,  1.55it/s]
em: 0.3491, f1: 0.4104, loss: 3.6385 ||:  10%|9         | 244/2445 [02:36<24:03,  1.52it/s]
em: 0.3500, f1: 0.4108, loss: 3.6393 ||:  11%|#         | 261/2445 [02:46<23:15,  1.57it/s]
em: 0.3538, f1: 0.4142, loss: 3.6244 ||:  11%|#1        | 278/2445 [02:58<23:23,  1.54it/s]
em: 0.3559, f1: 0.4161, loss: 3.6274 ||:  12%|#2        | 294/2445 [03:08<23:03,  1.56it/s]
em: 0.3569, f1: 0.4171, loss: 3.6200 ||:  13%|#2        | 311/2445 [03:18<22:37,  1.57it/s]
em: 0.3582, f1: 0.4185, loss: 3.6047 ||:  13%|#3        | 328/2445 [03:29<22:30,  1.57it/s]
em: 0.3557, f1: 0.4156, loss: 3.5997 ||:  14%|#4        | 344/2445 [03:40<22:35,  1.55it/s]
em: 0.3551, f1: 0.4167, loss: 3.5966 ||:  15%|#4        | 360/2445 [03:50<22:32,  1.54it/s]
em: 0.3550, f1: 0.4163, loss: 3.5953 ||:  15%|#5        | 376/2445 [04:01<22:21,  1.54it/s]
em: 0.3555, f1: 0.4175, loss: 3.5855 ||:  16%|#6        | 393/2445 [04:11<21:48,  1.57it/s]
em: 0.3563, f1: 0.4174, loss: 3.5878 ||:  17%|#6        | 410/2445 [04:21<21:24,  1.58it/s]
em: 0.3556, f1: 0.4171, loss: 3.6020 ||:  17%|#7        | 427/2445 [04:33<21:33,  1.56it/s]
em: 0.3559, f1: 0.4171, loss: 3.5954 ||:  18%|#8        | 443/2445 [04:43<21:15,  1.57it/s]
em: 0.3564, f1: 0.4175, loss: 3.5999 ||:  19%|#8        | 460/2445 [04:53<20:53,  1.58it/s]
em: 0.3555, f1: 0.4166, loss: 3.6151 ||:  20%|#9        | 477/2445 [05:04<20:54,  1.57it/s]
em: 0.3554, f1: 0.4166, loss: 3.6250 ||:  20%|##        | 494/2445 [05:15<20:32,  1.58it/s]
em: 0.3527, f1: 0.4145, loss: 3.6344 ||:  21%|##        | 511/2445 [05:26<20:38,  1.56it/s]
em: 0.3514, f1: 0.4140, loss: 3.6442 ||:  22%|##1       | 529/2445 [05:36<19:49,  1.61it/s]
em: 0.3506, f1: 0.4133, loss: 3.6486 ||:  22%|##2       | 547/2445 [05:48<19:59,  1.58it/s]
em: 0.3516, f1: 0.4145, loss: 3.6403 ||:  23%|##3       | 563/2445 [05:59<19:56,  1.57it/s]
em: 0.3512, f1: 0.4143, loss: 3.6517 ||:  24%|##3       | 579/2445 [06:09<20:07,  1.54it/s]
em: 0.3516, f1: 0.4147, loss: 3.6508 ||:  24%|##4       | 596/2445 [06:20<19:39,  1.57it/s]
em: 0.3528, f1: 0.4159, loss: 3.6491 ||:  25%|##5       | 613/2445 [06:31<19:32,  1.56it/s]
em: 0.3518, f1: 0.4145, loss: 3.6570 ||:  26%|##5       | 630/2445 [06:41<19:09,  1.58it/s]
em: 0.3519, f1: 0.4144, loss: 3.6643 ||:  26%|##6       | 647/2445 [06:52<19:06,  1.57it/s]
em: 0.3524, f1: 0.4149, loss: 3.6707 ||:  27%|##7       | 663/2445 [07:03<19:07,  1.55it/s]
em: 0.3507, f1: 0.4130, loss: 3.6807 ||:  28%|##7       | 679/2445 [07:13<18:52,  1.56it/s]
em: 0.3502, f1: 0.4128, loss: 3.6729 ||:  28%|##8       | 695/2445 [07:23<18:48,  1.55it/s]
em: 0.3505, f1: 0.4132, loss: 3.6679 ||:  29%|##9       | 711/2445 [07:34<18:34,  1.56it/s]
em: 0.3506, f1: 0.4131, loss: 3.6748 ||:  30%|##9       | 727/2445 [07:44<18:35,  1.54it/s]
em: 0.3522, f1: 0.4147, loss: 3.6676 ||:  30%|###       | 743/2445 [07:54<18:16,  1.55it/s]
em: 0.3522, f1: 0.4148, loss: 3.6658 ||:  31%|###1      | 759/2445 [08:05<18:11,  1.54it/s]
em: 0.3497, f1: 0.4128, loss: 3.6782 ||:  32%|###1      | 775/2445 [08:15<18:00,  1.55it/s]
em: 0.3508, f1: 0.4139, loss: 3.6788 ||:  32%|###2      | 792/2445 [08:26<17:30,  1.57it/s]
em: 0.3505, f1: 0.4134, loss: 3.6828 ||:  33%|###3      | 809/2445 [08:37<17:31,  1.56it/s]
em: 0.3498, f1: 0.4133, loss: 3.6806 ||:  34%|###3      | 825/2445 [08:47<17:22,  1.55it/s]
em: 0.3492, f1: 0.4128, loss: 3.6745 ||:  34%|###4      | 841/2445 [08:57<17:13,  1.55it/s]
em: 0.3493, f1: 0.4129, loss: 3.6773 ||:  35%|###5      | 857/2445 [09:08<17:05,  1.55it/s]
em: 0.3494, f1: 0.4130, loss: 3.6708 ||:  36%|###5      | 873/2445 [09:19<17:07,  1.53it/s]
em: 0.3488, f1: 0.4125, loss: 3.6783 ||:  36%|###6      | 889/2445 [09:29<16:48,  1.54it/s]
em: 0.3490, f1: 0.4128, loss: 3.6739 ||:  37%|###7      | 905/2445 [09:39<16:38,  1.54it/s]
em: 0.3489, f1: 0.4128, loss: 3.6726 ||:  38%|###7      | 921/2445 [09:49<16:23,  1.55it/s]
em: 0.3488, f1: 0.4128, loss: 3.6666 ||:  38%|###8      | 937/2445 [10:00<16:16,  1.54it/s]
em: 0.3489, f1: 0.4126, loss: 3.6696 ||:  39%|###8      | 953/2445 [10:11<16:18,  1.52it/s]
em: 0.3481, f1: 0.4120, loss: 3.6759 ||:  40%|###9      | 969/2445 [10:21<15:58,  1.54it/s]
em: 0.3473, f1: 0.4113, loss: 3.6796 ||:  40%|####      | 986/2445 [10:31<15:31,  1.57it/s]
em: 0.3466, f1: 0.4109, loss: 3.6867 ||:  41%|####1     | 1003/2445 [10:41<15:04,  1.59it/s]
em: 0.3470, f1: 0.4111, loss: 3.6772 ||:  42%|####1     | 1020/2445 [10:53<15:05,  1.57it/s]
em: 0.3469, f1: 0.4109, loss: 3.6759 ||:  42%|####2     | 1036/2445 [11:03<15:07,  1.55it/s]
em: 0.3467, f1: 0.4107, loss: 3.6721 ||:  43%|####3     | 1052/2445 [11:14<15:02,  1.54it/s]
em: 0.3468, f1: 0.4111, loss: 3.6717 ||:  44%|####3     | 1068/2445 [11:24<14:46,  1.55it/s]
em: 0.3473, f1: 0.4116, loss: 3.6747 ||:  44%|####4     | 1085/2445 [11:34<14:25,  1.57it/s]
em: 0.3468, f1: 0.4110, loss: 3.6772 ||:  45%|####5     | 1102/2445 [11:45<14:13,  1.57it/s]
em: 0.3460, f1: 0.4104, loss: 3.6777 ||:  46%|####5     | 1118/2445 [11:56<14:10,  1.56it/s]
em: 0.3455, f1: 0.4099, loss: 3.6746 ||:  46%|####6     | 1135/2445 [12:06<13:49,  1.58it/s]
em: 0.3458, f1: 0.4103, loss: 3.6696 ||:  47%|####7     | 1152/2445 [12:18<13:57,  1.54it/s]
em: 0.3463, f1: 0.4107, loss: 3.6689 ||:  48%|####7     | 1168/2445 [12:28<13:44,  1.55it/s]
em: 0.3468, f1: 0.4114, loss: 3.6686 ||:  48%|####8     | 1184/2445 [12:39<13:43,  1.53it/s]
em: 0.3462, f1: 0.4107, loss: 3.6681 ||:  49%|####9     | 1201/2445 [12:49<13:21,  1.55it/s]
em: 0.3457, f1: 0.4104, loss: 3.6692 ||:  50%|####9     | 1218/2445 [13:00<13:15,  1.54it/s]
em: 0.3456, f1: 0.4103, loss: 3.6731 ||:  50%|#####     | 1234/2445 [13:11<12:58,  1.56it/s]
em: 0.3448, f1: 0.4096, loss: 3.6761 ||:  51%|#####1    | 1250/2445 [13:21<12:47,  1.56it/s]
em: 0.3445, f1: 0.4098, loss: 3.6760 ||:  52%|#####1    | 1266/2445 [13:31<12:37,  1.56it/s]
em: 0.3451, f1: 0.4104, loss: 3.6735 ||:  52%|#####2    | 1282/2445 [13:41<12:26,  1.56it/s]
em: 0.3456, f1: 0.4108, loss: 3.6713 ||:  53%|#####3    | 1298/2445 [13:52<12:16,  1.56it/s]
em: 0.3455, f1: 0.4107, loss: 3.6716 ||:  54%|#####3    | 1315/2445 [14:02<11:53,  1.58it/s]
em: 0.3451, f1: 0.4104, loss: 3.6674 ||:  54%|#####4    | 1332/2445 [14:13<11:47,  1.57it/s]
em: 0.3455, f1: 0.4106, loss: 3.6632 ||:  55%|#####5    | 1349/2445 [14:23<11:30,  1.59it/s]
em: 0.3455, f1: 0.4106, loss: 3.6631 ||:  56%|#####5    | 1366/2445 [14:34<11:16,  1.60it/s]
em: 0.3452, f1: 0.4104, loss: 3.6641 ||:  57%|#####6    | 1383/2445 [14:45<11:08,  1.59it/s]
em: 0.3452, f1: 0.4102, loss: 3.6640 ||:  57%|#####7    | 1399/2445 [14:55<11:00,  1.58it/s]
em: 0.3452, f1: 0.4101, loss: 3.6632 ||:  58%|#####7    | 1415/2445 [15:06<11:00,  1.56it/s]
em: 0.3451, f1: 0.4103, loss: 3.6673 ||:  59%|#####8    | 1431/2445 [15:16<10:54,  1.55it/s]
em: 0.3451, f1: 0.4102, loss: 3.6665 ||:  59%|#####9    | 1447/2445 [15:26<10:42,  1.55it/s]
em: 0.3449, f1: 0.4102, loss: 3.6657 ||:  60%|#####9    | 1464/2445 [15:36<10:16,  1.59it/s]
em: 0.3447, f1: 0.4101, loss: 3.6658 ||:  61%|######    | 1481/2445 [15:47<10:06,  1.59it/s]
em: 0.3452, f1: 0.4103, loss: 3.6637 ||:  61%|######1   | 1497/2445 [15:57<09:55,  1.59it/s]
em: 0.3455, f1: 0.4107, loss: 3.6607 ||:  62%|######1   | 1514/2445 [16:08<09:42,  1.60it/s]
em: 0.3460, f1: 0.4110, loss: 3.6594 ||:  63%|######2   | 1531/2445 [16:19<09:41,  1.57it/s]
em: 0.3459, f1: 0.4109, loss: 3.6581 ||:  63%|######3   | 1547/2445 [16:29<09:37,  1.56it/s]
em: 0.3459, f1: 0.4108, loss: 3.6559 ||:  64%|######3   | 1563/2445 [16:40<09:25,  1.56it/s]
em: 0.3456, f1: 0.4103, loss: 3.6554 ||:  65%|######4   | 1579/2445 [16:50<09:15,  1.56it/s]
em: 0.3459, f1: 0.4104, loss: 3.6510 ||:  65%|######5   | 1595/2445 [17:00<09:00,  1.57it/s]
em: 0.3452, f1: 0.4100, loss: 3.6528 ||:  66%|######5   | 1611/2445 [17:10<08:52,  1.57it/s]
em: 0.3453, f1: 0.4100, loss: 3.6536 ||:  67%|######6   | 1628/2445 [17:21<08:36,  1.58it/s]
em: 0.3455, f1: 0.4102, loss: 3.6561 ||:  67%|######7   | 1645/2445 [17:32<08:36,  1.55it/s]
em: 0.3452, f1: 0.4101, loss: 3.6591 ||:  68%|######7   | 1661/2445 [17:43<08:28,  1.54it/s]
em: 0.3454, f1: 0.4104, loss: 3.6572 ||:  69%|######8   | 1677/2445 [17:53<08:19,  1.54it/s]
em: 0.3460, f1: 0.4109, loss: 3.6570 ||:  69%|######9   | 1693/2445 [18:03<08:06,  1.55it/s]
em: 0.3469, f1: 0.4118, loss: 3.6509 ||:  70%|######9   | 1710/2445 [18:13<07:44,  1.58it/s]
em: 0.3461, f1: 0.4113, loss: 3.6572 ||:  71%|#######   | 1727/2445 [18:25<07:43,  1.55it/s]
em: 0.3457, f1: 0.4110, loss: 3.6620 ||:  71%|#######1  | 1743/2445 [18:35<07:31,  1.56it/s]
em: 0.3459, f1: 0.4111, loss: 3.6638 ||:  72%|#######1  | 1759/2445 [18:45<07:18,  1.56it/s]
em: 0.3464, f1: 0.4114, loss: 3.6660 ||:  73%|#######2  | 1775/2445 [18:56<07:12,  1.55it/s]
em: 0.3466, f1: 0.4116, loss: 3.6668 ||:  73%|#######3  | 1791/2445 [19:06<07:00,  1.56it/s]
em: 0.3468, f1: 0.4117, loss: 3.6645 ||:  74%|#######3  | 1807/2445 [19:16<06:51,  1.55it/s]
em: 0.3471, f1: 0.4119, loss: 3.6654 ||:  75%|#######4  | 1824/2445 [19:27<06:34,  1.57it/s]
em: 0.3470, f1: 0.4119, loss: 3.6639 ||:  75%|#######5  | 1841/2445 [19:38<06:26,  1.56it/s]
em: 0.3472, f1: 0.4121, loss: 3.6621 ||:  76%|#######5  | 1857/2445 [19:48<06:17,  1.56it/s]
em: 0.3476, f1: 0.4124, loss: 3.6601 ||:  77%|#######6  | 1873/2445 [19:59<06:11,  1.54it/s]
em: 0.3482, f1: 0.4130, loss: 3.6553 ||:  77%|#######7  | 1889/2445 [20:09<05:58,  1.55it/s]
em: 0.3484, f1: 0.4131, loss: 3.6550 ||:  78%|#######7  | 1905/2445 [20:19<05:47,  1.55it/s]
em: 0.3488, f1: 0.4134, loss: 3.6525 ||:  79%|#######8  | 1922/2445 [20:29<05:28,  1.59it/s]
em: 0.3483, f1: 0.4128, loss: 3.6513 ||:  79%|#######9  | 1939/2445 [20:40<05:20,  1.58it/s]
em: 0.3480, f1: 0.4126, loss: 3.6533 ||:  80%|########  | 1956/2445 [20:51<05:07,  1.59it/s]
em: 0.3480, f1: 0.4126, loss: 3.6555 ||:  81%|########  | 1973/2445 [21:02<04:57,  1.59it/s]
em: 0.3484, f1: 0.4130, loss: 3.6544 ||:  81%|########1 | 1990/2445 [21:12<04:44,  1.60it/s]
em: 0.3487, f1: 0.4130, loss: 3.6560 ||:  82%|########2 | 2007/2445 [21:23<04:33,  1.60it/s]
em: 0.3487, f1: 0.4131, loss: 3.6531 ||:  83%|########2 | 2023/2445 [21:33<04:27,  1.58it/s]
em: 0.3483, f1: 0.4128, loss: 3.6565 ||:  83%|########3 | 2039/2445 [21:43<04:17,  1.58it/s]
em: 0.3482, f1: 0.4128, loss: 3.6580 ||:  84%|########4 | 2055/2445 [21:54<04:13,  1.54it/s]
em: 0.3482, f1: 0.4128, loss: 3.6600 ||:  85%|########4 | 2071/2445 [22:05<04:05,  1.53it/s]
em: 0.3486, f1: 0.4133, loss: 3.6565 ||:  85%|########5 | 2087/2445 [22:15<03:52,  1.54it/s]
em: 0.3488, f1: 0.4135, loss: 3.6554 ||:  86%|########6 | 2104/2445 [22:26<03:38,  1.56it/s]
em: 0.3487, f1: 0.4134, loss: 3.6566 ||:  87%|########6 | 2121/2445 [22:36<03:26,  1.57it/s]
em: 0.3482, f1: 0.4130, loss: 3.6590 ||:  87%|########7 | 2137/2445 [22:48<03:21,  1.53it/s]
em: 0.3481, f1: 0.4129, loss: 3.6599 ||:  88%|########8 | 2153/2445 [22:58<03:10,  1.54it/s]
em: 0.3487, f1: 0.4134, loss: 3.6584 ||:  89%|########8 | 2169/2445 [23:08<02:57,  1.55it/s]
em: 0.3486, f1: 0.4133, loss: 3.6568 ||:  89%|########9 | 2185/2445 [23:18<02:45,  1.57it/s]
em: 0.3481, f1: 0.4128, loss: 3.6555 ||:  90%|######### | 2201/2445 [23:28<02:36,  1.56it/s]
em: 0.3482, f1: 0.4130, loss: 3.6554 ||:  91%|######### | 2217/2445 [23:39<02:26,  1.55it/s]
em: 0.3485, f1: 0.4134, loss: 3.6542 ||:  91%|#########1| 2233/2445 [23:49<02:17,  1.55it/s]
em: 0.3486, f1: 0.4133, loss: 3.6544 ||:  92%|#########1| 2249/2445 [24:00<02:07,  1.54it/s]
em: 0.3484, f1: 0.4131, loss: 3.6562 ||:  93%|#########2| 2265/2445 [24:10<01:57,  1.53it/s]
em: 0.3482, f1: 0.4128, loss: 3.6569 ||:  93%|#########3| 2281/2445 [24:21<01:46,  1.53it/s]
em: 0.3478, f1: 0.4127, loss: 3.6575 ||:  94%|#########3| 2297/2445 [24:31<01:36,  1.54it/s]
em: 0.3476, f1: 0.4126, loss: 3.6562 ||:  95%|#########4| 2313/2445 [24:41<01:26,  1.53it/s]
em: 0.3470, f1: 0.4121, loss: 3.6569 ||:  95%|#########5| 2329/2445 [24:52<01:15,  1.53it/s]
em: 0.3462, f1: 0.4114, loss: 3.6572 ||:  96%|#########5| 2345/2445 [25:02<01:05,  1.53it/s]
em: 0.3457, f1: 0.4111, loss: 3.6563 ||:  97%|#########6| 2361/2445 [25:13<00:55,  1.52it/s]
em: 0.3452, f1: 0.4107, loss: 3.6555 ||:  97%|#########7| 2377/2445 [25:23<00:44,  1.53it/s]
em: 0.3446, f1: 0.4102, loss: 3.6558 ||:  98%|#########7| 2394/2445 [25:34<00:32,  1.56it/s]
em: 0.3439, f1: 0.4096, loss: 3.6568 ||:  99%|#########8| 2411/2445 [25:45<00:21,  1.56it/s]
em: 0.3432, f1: 0.4090, loss: 3.6585 ||:  99%|#########9| 2427/2445 [25:55<00:11,  1.57it/s]
em: 0.3428, f1: 0.4087, loss: 3.6573 ||: 100%|#########9| 2444/2445 [26:05<00:00,  1.59it/s]
em: 0.3423, f1: 0.4082, loss: 3.6591 ||: : 2461it [26:17,  1.54it/s]                        
em: 0.3415, f1: 0.4074, loss: 3.6612 ||: : 2478it [26:27,  1.57it/s]

2019-05-09 21:30:32,931 - INFO - allennlp.training.trainer - Validating
  0%|          | 0/596 [00:00<?, ?it/s]
em: 0.3101, f1: 0.3271, loss: 1201936.0317 ||:   4%|4         | 26/596 [00:10<03:42,  2.56it/s]
em: 0.2803, f1: 0.3047, loss: 1237994.3810 ||:   9%|8         | 52/596 [00:24<04:01,  2.25it/s]
em: 0.2761, f1: 0.3068, loss: 1517869.7095 ||:  12%|#1        | 70/596 [00:35<04:15,  2.05it/s]
em: 0.2792, f1: 0.3099, loss: 1565946.2766 ||:  15%|#5        | 91/596 [00:45<04:07,  2.04it/s]
em: 0.2798, f1: 0.3081, loss: 1640637.5030 ||:  19%|#8        | 112/596 [00:58<04:13,  1.91it/s]
em: 0.2620, f1: 0.2910, loss: 1733589.3004 ||:  23%|##2       | 137/596 [01:08<03:44,  2.04it/s]
em: 0.2480, f1: 0.2758, loss: 1732265.9271 ||:  27%|##7       | 162/596 [01:23<03:47,  1.91it/s]
em: 0.2410, f1: 0.2694, loss: 1706743.8154 ||:  31%|###       | 182/596 [01:34<03:37,  1.90it/s]
em: 0.2360, f1: 0.2637, loss: 1708757.2200 ||:  34%|###4      | 203/596 [01:44<03:22,  1.94it/s]
em: 0.2350, f1: 0.2626, loss: 1618317.1341 ||:  38%|###7      | 224/596 [01:56<03:14,  1.91it/s]
em: 0.2385, f1: 0.2657, loss: 1619446.7357 ||:  41%|####1     | 247/596 [02:06<02:54,  2.00it/s]
em: 0.2413, f1: 0.2688, loss: 1601865.4992 ||:  45%|####5     | 270/596 [02:20<02:53,  1.88it/s]
em: 0.2431, f1: 0.2702, loss: 1568935.0878 ||:  49%|####8     | 292/596 [02:31<02:36,  1.94it/s]
em: 0.2425, f1: 0.2686, loss: 1632179.2919 ||:  53%|#####2    | 314/596 [02:42<02:24,  1.95it/s]
em: 0.2417, f1: 0.2675, loss: 1581226.5633 ||:  56%|#####6    | 334/596 [02:53<02:18,  1.90it/s]
em: 0.2412, f1: 0.2669, loss: 1612974.4726 ||:  59%|#####9    | 353/596 [03:03<02:08,  1.89it/s]
em: 0.2468, f1: 0.2728, loss: 1550753.3790 ||:  62%|######2   | 372/596 [03:13<01:59,  1.88it/s]
em: 0.2471, f1: 0.2735, loss: 1588888.6391 ||:  66%|######5   | 391/596 [03:23<01:49,  1.87it/s]
em: 0.2466, f1: 0.2722, loss: 1564795.1169 ||:  69%|######8   | 411/596 [03:34<01:37,  1.89it/s]
em: 0.2459, f1: 0.2718, loss: 1547287.9377 ||:  72%|#######2  | 431/596 [03:44<01:26,  1.92it/s]
em: 0.2455, f1: 0.2717, loss: 1509160.6538 ||:  76%|#######5  | 451/596 [03:56<01:19,  1.83it/s]
em: 0.2399, f1: 0.2662, loss: 1617893.8687 ||:  80%|#######9  | 474/596 [04:06<01:02,  1.94it/s]
em: 0.2362, f1: 0.2628, loss: 1724108.4588 ||:  83%|########3 | 497/596 [04:17<00:49,  1.98it/s]
em: 0.2328, f1: 0.2595, loss: 1839367.1215 ||:  88%|########8 | 526/596 [04:27<00:32,  2.18it/s]
em: 0.2314, f1: 0.2575, loss: 1916680.0457 ||:  93%|#########3| 555/596 [04:39<00:18,  2.24it/s]
em: 0.2261, f1: 0.2522, loss: 1968763.4791 ||:  97%|#########7| 580/596 [04:53<00:07,  2.13it/s]
em: 0.2234, f1: 0.2497, loss: 1984388.5632 ||: : 600it [05:05,  1.96it/s]                       
em: 0.2225, f1: 0.2488, loss: 1975386.1545 ||: : 604it [05:08,  1.96it/s]

2019-05-09 21:35:41,423 - INFO - allennlp.training.tensorboard_writer -                     Training |  Validation
2019-05-09 21:35:41,424 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB |  5036.000  |       N/A
2019-05-09 21:35:41,424 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB   |  6466.128  |       N/A
2019-05-09 21:35:41,425 - INFO - allennlp.training.tensorboard_writer - loss            |     3.661  |  1975386.155
2019-05-09 21:35:41,425 - INFO - allennlp.training.tensorboard_writer - em              |     0.341  |     0.223
2019-05-09 21:35:41,426 - INFO - allennlp.training.tensorboard_writer - f1              |     0.407  |     0.249
2019-05-09 21:36:24,025 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to 'out/rc_bert_simple_v2/best.th'.
2019-05-09 21:37:06,251 - INFO - allennlp.training.trainer - Epoch duration: 00:33:01
2019-05-09 21:37:06,252 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:59:29
2019-05-09 21:37:06,252 - INFO - allennlp.training.trainer - Epoch 3/9
2019-05-09 21:37:06,252 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 6467.052
2019-05-09 21:37:06,370 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 5037
2019-05-09 21:37:06,376 - INFO - allennlp.training.trainer - Training
  0%|          | 0/2445 [00:00<?, ?it/s]
em: 0.3945, f1: 0.4652, loss: 3.1439 ||:   1%|          | 16/2445 [00:10<25:42,  1.58it/s]
em: 0.4019, f1: 0.4562, loss: 3.1399 ||:   1%|1         | 33/2445 [00:20<25:21,  1.59it/s]
em: 0.4066, f1: 0.4603, loss: 3.2583 ||:   2%|2         | 50/2445 [00:31<25:25,  1.57it/s]
em: 0.3856, f1: 0.4393, loss: 3.4274 ||:   3%|2         | 66/2445 [00:41<25:09,  1.58it/s]
em: 0.3781, f1: 0.4355, loss: 3.3991 ||:   3%|3         | 82/2445 [00:51<24:58,  1.58it/s]
em: 0.3724, f1: 0.4358, loss: 3.3891 ||:   4%|4         | 98/2445 [01:02<24:48,  1.58it/s]
em: 0.3728, f1: 0.4373, loss: 3.3509 ||:   5%|4         | 114/2445 [01:12<24:55,  1.56it/s]
em: 0.3696, f1: 0.4327, loss: 3.3451 ||:   5%|5         | 130/2445 [01:23<25:02,  1.54it/s]
em: 0.3707, f1: 0.4342, loss: 3.3366 ||:   6%|6         | 147/2445 [01:33<24:18,  1.58it/s]
em: 0.3742, f1: 0.4365, loss: 3.3260 ||:   7%|6         | 164/2445 [01:44<24:17,  1.57it/s]
em: 0.3754, f1: 0.4383, loss: 3.3270 ||:   7%|7         | 180/2445 [01:54<24:00,  1.57it/s]
em: 0.3769, f1: 0.4397, loss: 3.3492 ||:   8%|8         | 196/2445 [02:04<23:46,  1.58it/s]
em: 0.3714, f1: 0.4355, loss: 3.3910 ||:   9%|8         | 212/2445 [02:15<23:49,  1.56it/s]
em: 0.3683, f1: 0.4337, loss: 3.4023 ||:   9%|9         | 228/2445 [02:25<23:47,  1.55it/s]
em: 0.3734, f1: 0.4384, loss: 3.3804 ||:  10%|9         | 244/2445 [02:36<23:56,  1.53it/s]
em: 0.3728, f1: 0.4383, loss: 3.4088 ||:  11%|#         | 261/2445 [02:46<23:07,  1.57it/s]
em: 0.3752, f1: 0.4401, loss: 3.3942 ||:  11%|#1        | 278/2445 [02:57<22:59,  1.57it/s]
em: 0.3772, f1: 0.4422, loss: 3.3874 ||:  12%|#2        | 294/2445 [03:07<22:53,  1.57it/s]
em: 0.3779, f1: 0.4426, loss: 3.3740 ||:  13%|#2        | 310/2445 [03:17<22:43,  1.57it/s]
em: 0.3773, f1: 0.4411, loss: 3.3756 ||:  13%|#3        | 326/2445 [03:28<22:48,  1.55it/s]
em: 0.3756, f1: 0.4392, loss: 3.3757 ||:  14%|#3        | 342/2445 [03:38<22:25,  1.56it/s]
em: 0.3750, f1: 0.4388, loss: 3.3801 ||:  15%|#4        | 359/2445 [03:49<22:03,  1.58it/s]
em: 0.3762, f1: 0.4401, loss: 3.3806 ||:  15%|#5        | 376/2445 [04:00<22:17,  1.55it/s]
em: 0.3758, f1: 0.4406, loss: 3.3749 ||:  16%|#5        | 391/2445 [04:10<22:21,  1.53it/s]
em: 0.3767, f1: 0.4414, loss: 3.3695 ||:  17%|#6        | 409/2445 [04:20<21:22,  1.59it/s]
em: 0.3772, f1: 0.4416, loss: 3.3693 ||:  17%|#7        | 427/2445 [04:32<21:19,  1.58it/s]
em: 0.3769, f1: 0.4413, loss: 3.3826 ||:  18%|#8        | 444/2445 [04:43<21:00,  1.59it/s]
em: 0.3772, f1: 0.4422, loss: 3.3762 ||:  19%|#8        | 461/2445 [04:54<20:59,  1.58it/s]
em: 0.3769, f1: 0.4419, loss: 3.3790 ||:  20%|#9        | 477/2445 [05:04<20:58,  1.56it/s]
em: 0.3755, f1: 0.4404, loss: 3.4057 ||:  20%|##        | 494/2445 [05:14<20:33,  1.58it/s]
em: 0.3746, f1: 0.4399, loss: 3.4163 ||:  21%|##        | 511/2445 [05:25<20:26,  1.58it/s]
em: 0.3732, f1: 0.4388, loss: 3.4310 ||:  22%|##1       | 528/2445 [05:36<20:10,  1.58it/s]
em: 0.3738, f1: 0.4398, loss: 3.4260 ||:  22%|##2       | 544/2445 [05:46<20:08,  1.57it/s]
em: 0.3740, f1: 0.4399, loss: 3.4242 ||:  23%|##2       | 560/2445 [05:56<19:52,  1.58it/s]
em: 0.3737, f1: 0.4397, loss: 3.4291 ||:  24%|##3       | 576/2445 [06:07<19:55,  1.56it/s]
em: 0.3768, f1: 0.4424, loss: 3.4191 ||:  24%|##4       | 592/2445 [06:17<19:40,  1.57it/s]
em: 0.3761, f1: 0.4419, loss: 3.4305 ||:  25%|##4       | 608/2445 [06:27<19:32,  1.57it/s]
em: 0.3752, f1: 0.4409, loss: 3.4442 ||:  26%|##5       | 625/2445 [06:38<19:11,  1.58it/s]
em: 0.3745, f1: 0.4395, loss: 3.4503 ||:  26%|##6       | 642/2445 [06:48<18:53,  1.59it/s]
em: 0.3746, f1: 0.4396, loss: 3.4528 ||:  27%|##6       | 659/2445 [07:00<19:05,  1.56it/s]
em: 0.3741, f1: 0.4391, loss: 3.4588 ||:  28%|##7       | 675/2445 [07:10<18:50,  1.57it/s]
em: 0.3729, f1: 0.4382, loss: 3.4633 ||:  28%|##8       | 691/2445 [07:20<18:48,  1.55it/s]
em: 0.3742, f1: 0.4390, loss: 3.4546 ||:  29%|##8       | 707/2445 [07:30<18:33,  1.56it/s]
em: 0.3743, f1: 0.4389, loss: 3.4545 ||:  30%|##9       | 723/2445 [07:41<18:32,  1.55it/s]
em: 0.3747, f1: 0.4389, loss: 3.4571 ||:  30%|###       | 740/2445 [07:51<18:03,  1.57it/s]
em: 0.3741, f1: 0.4382, loss: 3.4597 ||:  31%|###       | 757/2445 [08:02<18:03,  1.56it/s]
em: 0.3730, f1: 0.4373, loss: 3.4659 ||:  32%|###1      | 773/2445 [08:13<17:57,  1.55it/s]
em: 0.3720, f1: 0.4368, loss: 3.4694 ||:  32%|###2      | 789/2445 [08:23<17:41,  1.56it/s]
em: 0.3716, f1: 0.4365, loss: 3.4729 ||:  33%|###2      | 805/2445 [08:33<17:35,  1.55it/s]
em: 0.3716, f1: 0.4368, loss: 3.4698 ||:  34%|###3      | 821/2445 [08:43<17:20,  1.56it/s]
em: 0.3708, f1: 0.4362, loss: 3.4662 ||:  34%|###4      | 837/2445 [08:54<17:14,  1.55it/s]
em: 0.3705, f1: 0.4356, loss: 3.4627 ||:  35%|###4      | 853/2445 [09:05<17:17,  1.54it/s]
em: 0.3699, f1: 0.4352, loss: 3.4591 ||:  36%|###5      | 869/2445 [09:15<17:08,  1.53it/s]
em: 0.3701, f1: 0.4353, loss: 3.4601 ||:  36%|###6      | 885/2445 [09:26<17:11,  1.51it/s]
em: 0.3703, f1: 0.4354, loss: 3.4569 ||:  37%|###6      | 902/2445 [09:36<16:40,  1.54it/s]
em: 0.3701, f1: 0.4355, loss: 3.4618 ||:  38%|###7      | 919/2445 [09:48<16:41,  1.52it/s]
em: 0.3701, f1: 0.4353, loss: 3.4576 ||:  38%|###8      | 936/2445 [09:58<16:12,  1.55it/s]
em: 0.3695, f1: 0.4344, loss: 3.4589 ||:  39%|###8      | 953/2445 [10:09<16:01,  1.55it/s]
em: 0.3695, f1: 0.4346, loss: 3.4675 ||:  40%|###9      | 969/2445 [10:20<15:52,  1.55it/s]
em: 0.3682, f1: 0.4333, loss: 3.4710 ||:  40%|####      | 985/2445 [10:30<15:33,  1.56it/s]
em: 0.3685, f1: 0.4337, loss: 3.4682 ||:  41%|####      | 1001/2445 [10:40<15:22,  1.57it/s]
em: 0.3686, f1: 0.4337, loss: 3.4690 ||:  42%|####1     | 1017/2445 [10:50<15:13,  1.56it/s]
em: 0.3676, f1: 0.4329, loss: 3.4697 ||:  42%|####2     | 1033/2445 [11:01<15:11,  1.55it/s]
em: 0.3675, f1: 0.4330, loss: 3.4656 ||:  43%|####2     | 1049/2445 [11:11<14:58,  1.55it/s]
em: 0.3676, f1: 0.4330, loss: 3.4667 ||:  44%|####3     | 1065/2445 [11:21<14:42,  1.56it/s]
em: 0.3675, f1: 0.4329, loss: 3.4669 ||:  44%|####4     | 1081/2445 [11:32<14:39,  1.55it/s]
em: 0.3674, f1: 0.4329, loss: 3.4704 ||:  45%|####4     | 1097/2445 [11:42<14:21,  1.56it/s]
em: 0.3666, f1: 0.4325, loss: 3.4686 ||:  46%|####5     | 1113/2445 [11:52<14:13,  1.56it/s]
em: 0.3667, f1: 0.4327, loss: 3.4683 ||:  46%|####6     | 1129/2445 [12:02<14:03,  1.56it/s]
em: 0.3670, f1: 0.4326, loss: 3.4672 ||:  47%|####6     | 1145/2445 [12:12<13:51,  1.56it/s]
em: 0.3678, f1: 0.4334, loss: 3.4628 ||:  47%|####7     | 1161/2445 [12:23<13:43,  1.56it/s]
em: 0.3680, f1: 0.4335, loss: 3.4635 ||:  48%|####8     | 1177/2445 [12:33<13:41,  1.54it/s]
em: 0.3680, f1: 0.4337, loss: 3.4664 ||:  49%|####8     | 1193/2445 [12:44<13:34,  1.54it/s]
em: 0.3670, f1: 0.4329, loss: 3.4675 ||:  49%|####9     | 1209/2445 [12:54<13:21,  1.54it/s]
em: 0.3680, f1: 0.4337, loss: 3.4662 ||:  50%|#####     | 1225/2445 [13:04<13:08,  1.55it/s]
em: 0.3672, f1: 0.4331, loss: 3.4721 ||:  51%|#####     | 1241/2445 [13:14<12:51,  1.56it/s]
em: 0.3675, f1: 0.4335, loss: 3.4749 ||:  51%|#####1    | 1257/2445 [13:25<12:42,  1.56it/s]
em: 0.3677, f1: 0.4342, loss: 3.4708 ||:  52%|#####2    | 1273/2445 [13:35<12:42,  1.54it/s]
em: 0.3672, f1: 0.4339, loss: 3.4721 ||:  53%|#####2    | 1289/2445 [13:45<12:22,  1.56it/s]
em: 0.3670, f1: 0.4338, loss: 3.4706 ||:  53%|#####3    | 1306/2445 [13:56<12:03,  1.57it/s]
em: 0.3673, f1: 0.4339, loss: 3.4683 ||:  54%|#####4    | 1323/2445 [14:07<11:49,  1.58it/s]
em: 0.3670, f1: 0.4339, loss: 3.4645 ||:  55%|#####4    | 1340/2445 [14:17<11:39,  1.58it/s]
em: 0.3673, f1: 0.4338, loss: 3.4633 ||:  56%|#####5    | 1357/2445 [14:28<11:22,  1.59it/s]
em: 0.3674, f1: 0.4336, loss: 3.4664 ||:  56%|#####6    | 1374/2445 [14:39<11:17,  1.58it/s]
em: 0.3675, f1: 0.4337, loss: 3.4651 ||:  57%|#####6    | 1390/2445 [14:49<11:12,  1.57it/s]
em: 0.3672, f1: 0.4332, loss: 3.4673 ||:  58%|#####7    | 1407/2445 [15:00<10:58,  1.58it/s]
em: 0.3667, f1: 0.4329, loss: 3.4717 ||:  58%|#####8    | 1423/2445 [15:10<10:55,  1.56it/s]
em: 0.3671, f1: 0.4334, loss: 3.4696 ||:  59%|#####8    | 1439/2445 [15:21<10:45,  1.56it/s]
em: 0.3666, f1: 0.4329, loss: 3.4699 ||:  60%|#####9    | 1455/2445 [15:31<10:38,  1.55it/s]
em: 0.3668, f1: 0.4330, loss: 3.4707 ||:  60%|######    | 1472/2445 [15:42<10:19,  1.57it/s]
em: 0.3673, f1: 0.4335, loss: 3.4682 ||:  61%|######    | 1489/2445 [15:52<10:02,  1.59it/s]
em: 0.3674, f1: 0.4334, loss: 3.4692 ||:  62%|######1   | 1506/2445 [16:02<09:46,  1.60it/s]
em: 0.3674, f1: 0.4332, loss: 3.4686 ||:  62%|######2   | 1523/2445 [16:14<09:45,  1.58it/s]
em: 0.3675, f1: 0.4334, loss: 3.4655 ||:  63%|######2   | 1539/2445 [16:24<09:36,  1.57it/s]
em: 0.3672, f1: 0.4329, loss: 3.4616 ||:  64%|######3   | 1555/2445 [16:34<09:24,  1.58it/s]
em: 0.3670, f1: 0.4327, loss: 3.4582 ||:  64%|######4   | 1571/2445 [16:45<09:21,  1.56it/s]
em: 0.3670, f1: 0.4325, loss: 3.4587 ||:  65%|######4   | 1587/2445 [16:55<09:07,  1.57it/s]
em: 0.3671, f1: 0.4327, loss: 3.4572 ||:  66%|######5   | 1603/2445 [17:05<08:55,  1.57it/s]
em: 0.3672, f1: 0.4325, loss: 3.4603 ||:  66%|######6   | 1619/2445 [17:15<08:46,  1.57it/s]
em: 0.3665, f1: 0.4320, loss: 3.4638 ||:  67%|######6   | 1635/2445 [17:25<08:37,  1.56it/s]
em: 0.3667, f1: 0.4324, loss: 3.4646 ||:  68%|######7   | 1651/2445 [17:36<08:33,  1.55it/s]
em: 0.3670, f1: 0.4327, loss: 3.4637 ||:  68%|######8   | 1667/2445 [17:46<08:24,  1.54it/s]
em: 0.3672, f1: 0.4331, loss: 3.4619 ||:  69%|######8   | 1683/2445 [17:57<08:18,  1.53it/s]
em: 0.3677, f1: 0.4337, loss: 3.4586 ||:  69%|######9   | 1699/2445 [18:07<08:08,  1.53it/s]
em: 0.3677, f1: 0.4339, loss: 3.4584 ||:  70%|#######   | 1715/2445 [18:18<08:00,  1.52it/s]
em: 0.3674, f1: 0.4337, loss: 3.4636 ||:  71%|#######   | 1731/2445 [18:29<07:51,  1.51it/s]
em: 0.3674, f1: 0.4336, loss: 3.4678 ||:  71%|#######1  | 1747/2445 [18:39<07:37,  1.52it/s]
em: 0.3673, f1: 0.4336, loss: 3.4713 ||:  72%|#######2  | 1763/2445 [18:50<07:36,  1.49it/s]
em: 0.3674, f1: 0.4335, loss: 3.4729 ||:  73%|#######2  | 1779/2445 [19:01<07:26,  1.49it/s]
em: 0.3679, f1: 0.4337, loss: 3.4741 ||:  73%|#######3  | 1794/2445 [19:11<07:16,  1.49it/s]
em: 0.3677, f1: 0.4334, loss: 3.4737 ||:  74%|#######4  | 1810/2445 [19:21<06:57,  1.52it/s]
em: 0.3678, f1: 0.4335, loss: 3.4728 ||:  75%|#######4  | 1826/2445 [19:31<06:43,  1.53it/s]
em: 0.3682, f1: 0.4338, loss: 3.4715 ||:  75%|#######5  | 1842/2445 [19:42<06:33,  1.53it/s]
em: 0.3680, f1: 0.4338, loss: 3.4704 ||:  76%|#######5  | 1858/2445 [19:52<06:19,  1.55it/s]
em: 0.3684, f1: 0.4342, loss: 3.4679 ||:  77%|#######6  | 1874/2445 [20:02<06:10,  1.54it/s]
em: 0.3688, f1: 0.4346, loss: 3.4676 ||:  77%|#######7  | 1890/2445 [20:13<05:58,  1.55it/s]
em: 0.3688, f1: 0.4346, loss: 3.4667 ||:  78%|#######7  | 1906/2445 [20:23<05:47,  1.55it/s]
em: 0.3687, f1: 0.4345, loss: 3.4662 ||:  79%|#######8  | 1922/2445 [20:33<05:35,  1.56it/s]
em: 0.3692, f1: 0.4348, loss: 3.4632 ||:  79%|#######9  | 1939/2445 [20:43<05:19,  1.58it/s]
em: 0.3690, f1: 0.4347, loss: 3.4655 ||:  80%|########  | 1956/2445 [20:54<05:06,  1.60it/s]
em: 0.3690, f1: 0.4347, loss: 3.4670 ||:  81%|########  | 1973/2445 [21:05<04:56,  1.59it/s]
em: 0.3689, f1: 0.4347, loss: 3.4668 ||:  81%|########1 | 1990/2445 [21:15<04:44,  1.60it/s]
em: 0.3691, f1: 0.4349, loss: 3.4654 ||:  82%|########2 | 2007/2445 [21:26<04:36,  1.58it/s]
em: 0.3691, f1: 0.4349, loss: 3.4660 ||:  83%|########2 | 2023/2445 [21:37<04:29,  1.57it/s]
em: 0.3689, f1: 0.4347, loss: 3.4665 ||:  83%|########3 | 2039/2445 [21:47<04:17,  1.57it/s]
em: 0.3681, f1: 0.4341, loss: 3.4700 ||:  84%|########4 | 2055/2445 [21:57<04:11,  1.55it/s]
em: 0.3687, f1: 0.4347, loss: 3.4692 ||:  85%|########4 | 2071/2445 [22:08<04:01,  1.55it/s]
em: 0.3688, f1: 0.4349, loss: 3.4697 ||:  85%|########5 | 2087/2445 [22:18<03:49,  1.56it/s]
em: 0.3685, f1: 0.4346, loss: 3.4716 ||:  86%|########6 | 2103/2445 [22:29<03:43,  1.53it/s]
em: 0.3688, f1: 0.4349, loss: 3.4705 ||:  87%|########6 | 2119/2445 [22:39<03:31,  1.54it/s]
em: 0.3690, f1: 0.4351, loss: 3.4707 ||:  87%|########7 | 2135/2445 [22:49<03:21,  1.54it/s]
em: 0.3688, f1: 0.4349, loss: 3.4732 ||:  88%|########7 | 2151/2445 [22:59<03:09,  1.55it/s]
em: 0.3691, f1: 0.4351, loss: 3.4711 ||:  89%|########8 | 2167/2445 [23:10<02:59,  1.55it/s]
em: 0.3685, f1: 0.4345, loss: 3.4737 ||:  89%|########9 | 2183/2445 [23:20<02:48,  1.56it/s]
em: 0.3683, f1: 0.4344, loss: 3.4708 ||:  90%|########9 | 2200/2445 [23:31<02:36,  1.57it/s]
em: 0.3683, f1: 0.4344, loss: 3.4696 ||:  91%|######### | 2216/2445 [23:41<02:28,  1.54it/s]
em: 0.3684, f1: 0.4345, loss: 3.4712 ||:  91%|#########1| 2232/2445 [23:52<02:18,  1.54it/s]
em: 0.3682, f1: 0.4343, loss: 3.4708 ||:  92%|#########1| 2248/2445 [24:02<02:06,  1.55it/s]
em: 0.3682, f1: 0.4344, loss: 3.4711 ||:  93%|#########2| 2264/2445 [24:12<01:56,  1.56it/s]
em: 0.3684, f1: 0.4345, loss: 3.4714 ||:  93%|#########3| 2280/2445 [24:23<01:46,  1.55it/s]
em: 0.3682, f1: 0.4344, loss: 3.4718 ||:  94%|#########3| 2296/2445 [24:33<01:37,  1.53it/s]
em: 0.3678, f1: 0.4342, loss: 3.4722 ||:  95%|#########4| 2312/2445 [24:44<01:27,  1.52it/s]
em: 0.3676, f1: 0.4341, loss: 3.4704 ||:  95%|#########5| 2328/2445 [24:54<01:16,  1.54it/s]
em: 0.3674, f1: 0.4339, loss: 3.4697 ||:  96%|#########5| 2345/2445 [25:05<01:04,  1.56it/s]
em: 0.3665, f1: 0.4331, loss: 3.4724 ||:  97%|#########6| 2361/2445 [25:15<00:54,  1.55it/s]
em: 0.3659, f1: 0.4324, loss: 3.4745 ||:  97%|#########7| 2377/2445 [25:25<00:43,  1.56it/s]
em: 0.3657, f1: 0.4322, loss: 3.4734 ||:  98%|#########7| 2393/2445 [25:36<00:33,  1.56it/s]
em: 0.3652, f1: 0.4318, loss: 3.4737 ||:  99%|#########8| 2409/2445 [25:46<00:23,  1.56it/s]
em: 0.3647, f1: 0.4313, loss: 3.4749 ||:  99%|#########9| 2426/2445 [25:56<00:12,  1.58it/s]
em: 0.3640, f1: 0.4308, loss: 3.4771 ||: 100%|#########9| 2443/2445 [26:07<00:01,  1.56it/s]
em: 0.3633, f1: 0.4302, loss: 3.4780 ||: : 2459it [26:18,  1.55it/s]                        
em: 0.3626, f1: 0.4296, loss: 3.4785 ||: : 2476it [26:28,  1.58it/s]
em: 0.3624, f1: 0.4295, loss: 3.4781 ||: : 2478it [26:30,  1.56it/s]

2019-05-09 22:03:36,412 - INFO - allennlp.training.trainer - Validating
  0%|          | 0/596 [00:00<?, ?it/s]
em: 0.3293, f1: 0.3435, loss: 1201936.5718 ||:   4%|4         | 26/596 [00:10<03:41,  2.57it/s]
em: 0.2961, f1: 0.3172, loss: 1237995.0514 ||:   9%|8         | 52/596 [00:24<04:00,  2.26it/s]
em: 0.2896, f1: 0.3183, loss: 1517870.3045 ||:  12%|#1        | 70/596 [00:35<04:15,  2.06it/s]
em: 0.2882, f1: 0.3169, loss: 1572815.0298 ||:  15%|#5        | 91/596 [00:45<04:05,  2.06it/s]
em: 0.2900, f1: 0.3169, loss: 1635057.8014 ||:  19%|#8        | 112/596 [00:58<04:11,  1.93it/s]
em: 0.2721, f1: 0.3007, loss: 1742714.0417 ||:  23%|##2       | 137/596 [01:08<03:43,  2.05it/s]
em: 0.2582, f1: 0.2856, loss: 1709118.5566 ||:  27%|##7       | 162/596 [01:23<03:46,  1.92it/s]
em: 0.2486, f1: 0.2765, loss: 1712720.9523 ||:  30%|###       | 181/596 [01:33<03:37,  1.91it/s]
em: 0.2431, f1: 0.2709, loss: 1703994.2173 ||:  34%|###3      | 201/596 [01:43<03:24,  1.93it/s]
em: 0.2423, f1: 0.2699, loss: 1634629.6658 ||:  37%|###7      | 221/596 [01:53<03:13,  1.94it/s]
em: 0.2419, f1: 0.2694, loss: 1628100.7084 ||:  41%|####      | 243/596 [02:04<02:57,  1.99it/s]
em: 0.2462, f1: 0.2742, loss: 1547184.4140 ||:  44%|####4     | 265/596 [02:18<03:00,  1.83it/s]
em: 0.2485, f1: 0.2757, loss: 1578734.2852 ||:  48%|####8     | 289/596 [02:28<02:36,  1.96it/s]
em: 0.2480, f1: 0.2741, loss: 1619423.4670 ||:  53%|#####2    | 313/596 [02:41<02:25,  1.95it/s]
em: 0.2470, f1: 0.2730, loss: 1582221.9556 ||:  56%|#####5    | 333/596 [02:51<02:16,  1.93it/s]
em: 0.2459, f1: 0.2715, loss: 1614006.3930 ||:  59%|#####9    | 352/596 [03:02<02:08,  1.89it/s]
em: 0.2522, f1: 0.2780, loss: 1547393.9312 ||:  62%|######2   | 372/596 [03:12<01:58,  1.89it/s]
em: 0.2528, f1: 0.2790, loss: 1585692.4963 ||:  66%|######5   | 391/596 [03:23<01:49,  1.87it/s]
em: 0.2540, f1: 0.2795, loss: 1561754.5580 ||:  69%|######8   | 411/596 [03:33<01:38,  1.89it/s]
em: 0.2538, f1: 0.2796, loss: 1544388.5185 ||:  72%|#######2  | 431/596 [03:43<01:26,  1.91it/s]
em: 0.2522, f1: 0.2786, loss: 1509161.4638 ||:  76%|#######5  | 451/596 [03:55<01:19,  1.82it/s]
em: 0.2465, f1: 0.2730, loss: 1609422.9401 ||:  79%|#######9  | 473/596 [04:06<01:04,  1.91it/s]
em: 0.2426, f1: 0.2693, loss: 1710873.2777 ||:  83%|########3 | 495/596 [04:17<00:52,  1.91it/s]
em: 0.2396, f1: 0.2666, loss: 1828497.0848 ||:  88%|########7 | 524/596 [04:27<00:33,  2.13it/s]
em: 0.2376, f1: 0.2641, loss: 1905529.4622 ||:  93%|#########2| 553/596 [04:40<00:19,  2.19it/s]
em: 0.2312, f1: 0.2578, loss: 1954086.9786 ||:  97%|#########6| 577/596 [04:53<00:09,  2.06it/s]
em: 0.2284, f1: 0.2554, loss: 1984074.5926 ||: 100%|##########| 596/596 [05:03<00:00,  1.97it/s]
em: 0.2270, f1: 0.2540, loss: 1968143.4736 ||: : 604it [05:10,  1.94it/s]                       

2019-05-09 22:08:47,315 - INFO - allennlp.training.tensorboard_writer -                     Training |  Validation
2019-05-09 22:08:47,316 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB |  5037.000  |       N/A
2019-05-09 22:08:47,317 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB   |  6467.052  |       N/A
2019-05-09 22:08:47,317 - INFO - allennlp.training.tensorboard_writer - loss            |     3.478  |  1968143.474
2019-05-09 22:08:47,317 - INFO - allennlp.training.tensorboard_writer - em              |     0.362  |     0.227
2019-05-09 22:08:47,318 - INFO - allennlp.training.tensorboard_writer - f1              |     0.430  |     0.254
2019-05-09 22:09:30,418 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to 'out/rc_bert_simple_v2/best.th'.
2019-05-09 22:10:13,217 - INFO - allennlp.training.trainer - Epoch duration: 00:33:06
2019-05-09 22:10:13,218 - INFO - allennlp.training.trainer - Estimated training time remaining: 3:23:38
2019-05-09 22:10:13,218 - INFO - allennlp.training.trainer - Epoch 4/9
2019-05-09 22:10:13,218 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 6468.936
2019-05-09 22:10:13,323 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 5024
2019-05-09 22:10:13,331 - INFO - allennlp.training.trainer - Training
  0%|          | 0/2445 [00:00<?, ?it/s]
em: 0.3417, f1: 0.4279, loss: 3.3690 ||:   1%|          | 15/2445 [00:10<27:36,  1.47it/s]
em: 0.3965, f1: 0.4594, loss: 3.0239 ||:   1%|1         | 32/2445 [00:20<26:34,  1.51it/s]
em: 0.3969, f1: 0.4599, loss: 3.1773 ||:   2%|2         | 49/2445 [00:31<26:26,  1.51it/s]
em: 0.3788, f1: 0.4391, loss: 3.3052 ||:   3%|2         | 66/2445 [00:42<25:48,  1.54it/s]
em: 0.3742, f1: 0.4373, loss: 3.3441 ||:   3%|3         | 83/2445 [00:53<25:45,  1.53it/s]
em: 0.3853, f1: 0.4504, loss: 3.2535 ||:   4%|4         | 99/2445 [01:03<25:20,  1.54it/s]
em: 0.3860, f1: 0.4550, loss: 3.2166 ||:   5%|4         | 115/2445 [01:14<25:29,  1.52it/s]
em: 0.3875, f1: 0.4537, loss: 3.2209 ||:   5%|5         | 131/2445 [01:25<25:20,  1.52it/s]
em: 0.3887, f1: 0.4552, loss: 3.2094 ||:   6%|6         | 147/2445 [01:35<25:07,  1.52it/s]
em: 0.3921, f1: 0.4591, loss: 3.2138 ||:   7%|6         | 163/2445 [01:46<24:50,  1.53it/s]
em: 0.3945, f1: 0.4607, loss: 3.2214 ||:   7%|7         | 179/2445 [01:56<24:43,  1.53it/s]
em: 0.3961, f1: 0.4620, loss: 3.2394 ||:   8%|7         | 195/2445 [02:07<24:36,  1.52it/s]
em: 0.3909, f1: 0.4579, loss: 3.2678 ||:   9%|8         | 211/2445 [02:17<24:27,  1.52it/s]
em: 0.3890, f1: 0.4575, loss: 3.2950 ||:   9%|9         | 227/2445 [02:28<24:13,  1.53it/s]
em: 0.3912, f1: 0.4588, loss: 3.2858 ||:  10%|9         | 243/2445 [02:38<23:44,  1.55it/s]
em: 0.3909, f1: 0.4578, loss: 3.2853 ||:  11%|#         | 259/2445 [02:49<23:53,  1.52it/s]
em: 0.3934, f1: 0.4602, loss: 3.2772 ||:  11%|#1        | 275/2445 [02:59<23:44,  1.52it/s]
em: 0.3954, f1: 0.4618, loss: 3.2616 ||:  12%|#1        | 292/2445 [03:10<23:08,  1.55it/s]
em: 0.3961, f1: 0.4627, loss: 3.2655 ||:  13%|#2        | 309/2445 [03:21<23:10,  1.54it/s]
em: 0.3966, f1: 0.4627, loss: 3.2528 ||:  13%|#3        | 325/2445 [03:31<22:56,  1.54it/s]
em: 0.3958, f1: 0.4616, loss: 3.2499 ||:  14%|#3        | 341/2445 [03:41<22:33,  1.55it/s]
em: 0.3951, f1: 0.4616, loss: 3.2509 ||:  15%|#4        | 357/2445 [03:52<22:30,  1.55it/s]
em: 0.3944, f1: 0.4611, loss: 3.2573 ||:  15%|#5        | 373/2445 [04:02<22:06,  1.56it/s]
em: 0.3911, f1: 0.4593, loss: 3.2659 ||:  16%|#5        | 389/2445 [04:12<22:03,  1.55it/s]
em: 0.3920, f1: 0.4600, loss: 3.2651 ||:  17%|#6        | 406/2445 [04:23<21:32,  1.58it/s]
em: 0.3936, f1: 0.4611, loss: 3.2603 ||:  17%|#7        | 423/2445 [04:34<21:29,  1.57it/s]
em: 0.3924, f1: 0.4598, loss: 3.2574 ||:  18%|#7        | 439/2445 [04:44<21:26,  1.56it/s]
em: 0.3935, f1: 0.4607, loss: 3.2587 ||:  19%|#8        | 456/2445 [04:54<20:46,  1.60it/s]
em: 0.3921, f1: 0.4588, loss: 3.2635 ||:  19%|#9        | 473/2445 [05:05<20:58,  1.57it/s]
em: 0.3905, f1: 0.4576, loss: 3.2807 ||:  20%|##        | 490/2445 [05:16<20:32,  1.59it/s]
em: 0.3895, f1: 0.4567, loss: 3.2806 ||:  21%|##        | 507/2445 [05:26<20:22,  1.59it/s]
em: 0.3881, f1: 0.4554, loss: 3.2944 ||:  21%|##1       | 524/2445 [05:37<20:00,  1.60it/s]
em: 0.3862, f1: 0.4542, loss: 3.3029 ||:  22%|##2       | 541/2445 [05:48<20:02,  1.58it/s]
em: 0.3852, f1: 0.4526, loss: 3.3070 ||:  23%|##2       | 557/2445 [05:59<20:29,  1.54it/s]
em: 0.3854, f1: 0.4529, loss: 3.3049 ||:  23%|##3       | 573/2445 [06:09<20:12,  1.54it/s]
em: 0.3848, f1: 0.4525, loss: 3.3119 ||:  24%|##4       | 589/2445 [06:19<19:51,  1.56it/s]
em: 0.3862, f1: 0.4537, loss: 3.3097 ||:  25%|##4       | 605/2445 [06:30<19:42,  1.56it/s]
em: 0.3855, f1: 0.4529, loss: 3.3169 ||:  25%|##5       | 621/2445 [06:40<19:41,  1.54it/s]
em: 0.3844, f1: 0.4522, loss: 3.3193 ||:  26%|##6       | 638/2445 [06:51<19:14,  1.57it/s]
em: 0.3858, f1: 0.4529, loss: 3.3250 ||:  27%|##6       | 655/2445 [07:01<18:58,  1.57it/s]
em: 0.3859, f1: 0.4530, loss: 3.3292 ||:  27%|##7       | 671/2445 [07:12<18:47,  1.57it/s]
em: 0.3856, f1: 0.4529, loss: 3.3362 ||:  28%|##8       | 687/2445 [07:22<19:02,  1.54it/s]
em: 0.3854, f1: 0.4526, loss: 3.3350 ||:  29%|##8       | 702/2445 [07:33<19:04,  1.52it/s]
em: 0.3854, f1: 0.4525, loss: 3.3276 ||:  29%|##9       | 718/2445 [07:43<18:38,  1.54it/s]
em: 0.3862, f1: 0.4530, loss: 3.3253 ||:  30%|###       | 734/2445 [07:53<18:32,  1.54it/s]
em: 0.3866, f1: 0.4533, loss: 3.3235 ||:  31%|###       | 750/2445 [08:03<18:21,  1.54it/s]
em: 0.3859, f1: 0.4529, loss: 3.3326 ||:  31%|###1      | 766/2445 [08:14<18:26,  1.52it/s]
em: 0.3854, f1: 0.4525, loss: 3.3347 ||:  32%|###1      | 781/2445 [08:24<18:21,  1.51it/s]
em: 0.3846, f1: 0.4515, loss: 3.3414 ||:  33%|###2      | 798/2445 [08:35<17:48,  1.54it/s]
em: 0.3837, f1: 0.4508, loss: 3.3430 ||:  33%|###3      | 815/2445 [08:47<17:59,  1.51it/s]
em: 0.3840, f1: 0.4518, loss: 3.3376 ||:  34%|###3      | 830/2445 [08:57<17:59,  1.50it/s]
em: 0.3838, f1: 0.4520, loss: 3.3335 ||:  35%|###4      | 845/2445 [09:07<17:52,  1.49it/s]
em: 0.3843, f1: 0.4524, loss: 3.3313 ||:  35%|###5      | 860/2445 [09:17<17:47,  1.49it/s]
em: 0.3835, f1: 0.4514, loss: 3.3282 ||:  36%|###5      | 875/2445 [09:28<18:00,  1.45it/s]
em: 0.3834, f1: 0.4514, loss: 3.3290 ||:  36%|###6      | 891/2445 [09:38<17:32,  1.48it/s]
em: 0.3841, f1: 0.4520, loss: 3.3289 ||:  37%|###7      | 907/2445 [09:49<17:05,  1.50it/s]
em: 0.3842, f1: 0.4521, loss: 3.3274 ||:  38%|###7      | 923/2445 [10:00<17:00,  1.49it/s]
em: 0.3840, f1: 0.4519, loss: 3.3299 ||:  38%|###8      | 938/2445 [10:10<16:49,  1.49it/s]
em: 0.3833, f1: 0.4511, loss: 3.3318 ||:  39%|###9      | 954/2445 [10:20<16:33,  1.50it/s]
em: 0.3825, f1: 0.4502, loss: 3.3373 ||:  40%|###9      | 970/2445 [10:31<16:24,  1.50it/s]
em: 0.3825, f1: 0.4501, loss: 3.3422 ||:  40%|####      | 987/2445 [10:41<15:49,  1.54it/s]
em: 0.3814, f1: 0.4491, loss: 3.3454 ||:  41%|####1     | 1004/2445 [10:53<15:40,  1.53it/s]
em: 0.3815, f1: 0.4490, loss: 3.3419 ||:  42%|####1     | 1020/2445 [11:03<15:34,  1.52it/s]
em: 0.3822, f1: 0.4493, loss: 3.3380 ||:  42%|####2     | 1036/2445 [11:13<15:20,  1.53it/s]
em: 0.3813, f1: 0.4485, loss: 3.3394 ||:  43%|####3     | 1052/2445 [11:24<15:14,  1.52it/s]
em: 0.3812, f1: 0.4485, loss: 3.3359 ||:  44%|####3     | 1068/2445 [11:35<15:14,  1.50it/s]
em: 0.3816, f1: 0.4489, loss: 3.3367 ||:  44%|####4     | 1084/2445 [11:46<15:05,  1.50it/s]
em: 0.3805, f1: 0.4478, loss: 3.3422 ||:  45%|####4     | 1100/2445 [11:56<14:46,  1.52it/s]
em: 0.3802, f1: 0.4476, loss: 3.3416 ||:  46%|####5     | 1116/2445 [12:07<14:35,  1.52it/s]
em: 0.3807, f1: 0.4482, loss: 3.3399 ||:  46%|####6     | 1132/2445 [12:17<14:26,  1.52it/s]
em: 0.3800, f1: 0.4476, loss: 3.3392 ||:  47%|####6     | 1148/2445 [12:28<14:26,  1.50it/s]
em: 0.3809, f1: 0.4487, loss: 3.3369 ||:  48%|####7     | 1163/2445 [12:38<14:23,  1.48it/s]
em: 0.3817, f1: 0.4492, loss: 3.3341 ||:  48%|####8     | 1178/2445 [12:49<14:14,  1.48it/s]
em: 0.3810, f1: 0.4486, loss: 3.3344 ||:  49%|####8     | 1193/2445 [12:59<14:11,  1.47it/s]
em: 0.3810, f1: 0.4487, loss: 3.3342 ||:  49%|####9     | 1209/2445 [13:09<13:51,  1.49it/s]
em: 0.3803, f1: 0.4481, loss: 3.3410 ||:  50%|#####     | 1225/2445 [13:21<13:51,  1.47it/s]
em: 0.3807, f1: 0.4487, loss: 3.3418 ||:  51%|#####     | 1241/2445 [13:31<13:23,  1.50it/s]
em: 0.3805, f1: 0.4485, loss: 3.3445 ||:  51%|#####1    | 1257/2445 [13:42<13:18,  1.49it/s]
em: 0.3807, f1: 0.4488, loss: 3.3422 ||:  52%|#####2    | 1272/2445 [13:52<13:10,  1.48it/s]
em: 0.3805, f1: 0.4489, loss: 3.3423 ||:  53%|#####2    | 1288/2445 [14:03<12:54,  1.49it/s]
em: 0.3801, f1: 0.4487, loss: 3.3400 ||:  53%|#####3    | 1304/2445 [14:13<12:40,  1.50it/s]
em: 0.3806, f1: 0.4487, loss: 3.3380 ||:  54%|#####3    | 1320/2445 [14:23<12:20,  1.52it/s]
em: 0.3800, f1: 0.4482, loss: 3.3383 ||:  55%|#####4    | 1336/2445 [14:34<12:05,  1.53it/s]
em: 0.3806, f1: 0.4488, loss: 3.3356 ||:  55%|#####5    | 1352/2445 [14:44<11:50,  1.54it/s]
em: 0.3809, f1: 0.4490, loss: 3.3334 ||:  56%|#####5    | 1368/2445 [14:55<11:51,  1.51it/s]
em: 0.3805, f1: 0.4487, loss: 3.3351 ||:  57%|#####6    | 1384/2445 [15:05<11:42,  1.51it/s]
em: 0.3797, f1: 0.4478, loss: 3.3363 ||:  57%|#####7    | 1400/2445 [15:16<11:27,  1.52it/s]
em: 0.3790, f1: 0.4471, loss: 3.3411 ||:  58%|#####7    | 1416/2445 [15:26<11:13,  1.53it/s]
em: 0.3791, f1: 0.4473, loss: 3.3428 ||:  59%|#####8    | 1432/2445 [15:37<11:12,  1.51it/s]
em: 0.3788, f1: 0.4471, loss: 3.3429 ||:  59%|#####9    | 1448/2445 [15:47<10:56,  1.52it/s]
em: 0.3786, f1: 0.4469, loss: 3.3433 ||:  60%|#####9    | 1464/2445 [15:58<10:49,  1.51it/s]
em: 0.3785, f1: 0.4468, loss: 3.3412 ||:  61%|######    | 1480/2445 [16:08<10:32,  1.53it/s]
em: 0.3781, f1: 0.4463, loss: 3.3410 ||:  61%|######1   | 1496/2445 [16:19<10:29,  1.51it/s]
em: 0.3789, f1: 0.4470, loss: 3.3360 ||:  62%|######1   | 1512/2445 [16:30<10:14,  1.52it/s]
em: 0.3796, f1: 0.4476, loss: 3.3354 ||:  62%|######2   | 1528/2445 [16:40<10:02,  1.52it/s]
em: 0.3794, f1: 0.4473, loss: 3.3352 ||:  63%|######3   | 1544/2445 [16:51<09:52,  1.52it/s]
em: 0.3799, f1: 0.4477, loss: 3.3303 ||:  64%|######3   | 1560/2445 [17:02<09:50,  1.50it/s]
em: 0.3800, f1: 0.4476, loss: 3.3271 ||:  64%|######4   | 1576/2445 [17:12<09:35,  1.51it/s]
em: 0.3801, f1: 0.4475, loss: 3.3258 ||:  65%|######5   | 1592/2445 [17:23<09:26,  1.51it/s]
em: 0.3801, f1: 0.4475, loss: 3.3247 ||:  66%|######5   | 1607/2445 [17:33<09:19,  1.50it/s]
em: 0.3800, f1: 0.4474, loss: 3.3291 ||:  66%|######6   | 1623/2445 [17:43<09:03,  1.51it/s]
em: 0.3799, f1: 0.4472, loss: 3.3330 ||:  67%|######7   | 1639/2445 [17:54<08:57,  1.50it/s]
em: 0.3795, f1: 0.4472, loss: 3.3340 ||:  68%|######7   | 1654/2445 [18:04<08:48,  1.50it/s]
em: 0.3799, f1: 0.4478, loss: 3.3327 ||:  68%|######8   | 1669/2445 [18:15<08:46,  1.47it/s]
em: 0.3805, f1: 0.4482, loss: 3.3293 ||:  69%|######8   | 1684/2445 [18:25<08:34,  1.48it/s]
em: 0.3809, f1: 0.4487, loss: 3.3280 ||:  69%|######9   | 1699/2445 [18:35<08:24,  1.48it/s]
em: 0.3808, f1: 0.4488, loss: 3.3267 ||:  70%|#######   | 1714/2445 [18:45<08:18,  1.47it/s]
em: 0.3806, f1: 0.4486, loss: 3.3314 ||:  71%|#######   | 1730/2445 [18:56<07:59,  1.49it/s]
em: 0.3802, f1: 0.4482, loss: 3.3362 ||:  71%|#######1  | 1746/2445 [19:07<07:54,  1.47it/s]
em: 0.3802, f1: 0.4479, loss: 3.3407 ||:  72%|#######2  | 1761/2445 [19:17<07:45,  1.47it/s]
em: 0.3804, f1: 0.4481, loss: 3.3415 ||:  73%|#######2  | 1776/2445 [19:28<07:41,  1.45it/s]
em: 0.3804, f1: 0.4480, loss: 3.3431 ||:  73%|#######3  | 1791/2445 [19:38<07:27,  1.46it/s]
em: 0.3806, f1: 0.4481, loss: 3.3411 ||:  74%|#######3  | 1807/2445 [19:49<07:13,  1.47it/s]
em: 0.3807, f1: 0.4482, loss: 3.3403 ||:  75%|#######4  | 1824/2445 [19:59<06:51,  1.51it/s]
em: 0.3809, f1: 0.4483, loss: 3.3397 ||:  75%|#######5  | 1841/2445 [20:11<06:41,  1.50it/s]
em: 0.3812, f1: 0.4487, loss: 3.3378 ||:  76%|#######5  | 1856/2445 [20:21<06:32,  1.50it/s]
em: 0.3816, f1: 0.4489, loss: 3.3379 ||:  77%|#######6  | 1872/2445 [20:31<06:17,  1.52it/s]
em: 0.3821, f1: 0.4496, loss: 3.3341 ||:  77%|#######7  | 1888/2445 [20:41<06:05,  1.52it/s]
em: 0.3824, f1: 0.4496, loss: 3.3331 ||:  78%|#######7  | 1904/2445 [20:52<05:56,  1.52it/s]
em: 0.3823, f1: 0.4495, loss: 3.3322 ||:  79%|#######8  | 1920/2445 [21:02<05:41,  1.54it/s]
em: 0.3825, f1: 0.4495, loss: 3.3300 ||:  79%|#######9  | 1936/2445 [21:12<05:27,  1.55it/s]
em: 0.3824, f1: 0.4495, loss: 3.3321 ||:  80%|#######9  | 1952/2445 [21:22<05:17,  1.55it/s]
em: 0.3824, f1: 0.4495, loss: 3.3311 ||:  80%|########  | 1968/2445 [21:33<05:11,  1.53it/s]
em: 0.3825, f1: 0.4496, loss: 3.3336 ||:  81%|########1 | 1983/2445 [21:44<05:08,  1.50it/s]
em: 0.3826, f1: 0.4496, loss: 3.3325 ||:  82%|########1 | 1999/2445 [21:54<04:53,  1.52it/s]
em: 0.3829, f1: 0.4499, loss: 3.3320 ||:  82%|########2 | 2015/2445 [22:05<04:43,  1.52it/s]
em: 0.3826, f1: 0.4497, loss: 3.3321 ||:  83%|########3 | 2031/2445 [22:15<04:35,  1.51it/s]
em: 0.3825, f1: 0.4498, loss: 3.3345 ||:  84%|########3 | 2047/2445 [22:26<04:23,  1.51it/s]
em: 0.3825, f1: 0.4499, loss: 3.3355 ||:  84%|########4 | 2063/2445 [22:37<04:14,  1.50it/s]
em: 0.3824, f1: 0.4499, loss: 3.3373 ||:  85%|########4 | 2078/2445 [22:47<04:04,  1.50it/s]
em: 0.3829, f1: 0.4503, loss: 3.3347 ||:  86%|########5 | 2093/2445 [22:57<03:59,  1.47it/s]
em: 0.3827, f1: 0.4501, loss: 3.3351 ||:  86%|########6 | 2108/2445 [23:07<03:48,  1.48it/s]
em: 0.3828, f1: 0.4503, loss: 3.3365 ||:  87%|########6 | 2124/2445 [23:18<03:34,  1.49it/s]
em: 0.3831, f1: 0.4505, loss: 3.3345 ||:  88%|########7 | 2140/2445 [23:28<03:20,  1.52it/s]
em: 0.3833, f1: 0.4506, loss: 3.3349 ||:  88%|########8 | 2156/2445 [23:38<03:07,  1.54it/s]
em: 0.3832, f1: 0.4505, loss: 3.3365 ||:  89%|########8 | 2172/2445 [23:48<02:56,  1.54it/s]
em: 0.3830, f1: 0.4502, loss: 3.3346 ||:  90%|########9 | 2189/2445 [23:59<02:44,  1.56it/s]
em: 0.3826, f1: 0.4498, loss: 3.3333 ||:  90%|######### | 2205/2445 [24:10<02:35,  1.55it/s]
em: 0.3827, f1: 0.4501, loss: 3.3329 ||:  91%|######### | 2221/2445 [24:20<02:24,  1.55it/s]
em: 0.3831, f1: 0.4504, loss: 3.3313 ||:  91%|#########1| 2237/2445 [24:30<02:13,  1.56it/s]
em: 0.3832, f1: 0.4504, loss: 3.3313 ||:  92%|#########2| 2253/2445 [24:41<02:05,  1.53it/s]
em: 0.3831, f1: 0.4503, loss: 3.3340 ||:  93%|#########2| 2269/2445 [24:51<01:54,  1.54it/s]
em: 0.3829, f1: 0.4502, loss: 3.3325 ||:  93%|#########3| 2285/2445 [25:02<01:44,  1.53it/s]
em: 0.3830, f1: 0.4505, loss: 3.3308 ||:  94%|#########4| 2301/2445 [25:12<01:33,  1.54it/s]
em: 0.3828, f1: 0.4503, loss: 3.3324 ||:  95%|#########4| 2317/2445 [25:22<01:23,  1.54it/s]
em: 0.3822, f1: 0.4499, loss: 3.3346 ||:  95%|#########5| 2333/2445 [25:33<01:13,  1.51it/s]
em: 0.3821, f1: 0.4498, loss: 3.3329 ||:  96%|#########6| 2350/2445 [25:44<01:01,  1.54it/s]
em: 0.3816, f1: 0.4493, loss: 3.3341 ||:  97%|#########6| 2367/2445 [25:55<00:50,  1.55it/s]
em: 0.3809, f1: 0.4485, loss: 3.3356 ||:  98%|#########7| 2384/2445 [26:05<00:39,  1.56it/s]
em: 0.3801, f1: 0.4478, loss: 3.3359 ||:  98%|#########8| 2400/2445 [26:16<00:29,  1.54it/s]
em: 0.3798, f1: 0.4477, loss: 3.3355 ||:  99%|#########8| 2417/2445 [26:26<00:17,  1.57it/s]
em: 0.3787, f1: 0.4469, loss: 3.3390 ||: 100%|#########9| 2434/2445 [26:37<00:07,  1.57it/s]
em: 0.3780, f1: 0.4463, loss: 3.3405 ||: : 2450it [26:48,  1.55it/s]                        
em: 0.3773, f1: 0.4458, loss: 3.3428 ||: : 2466it [26:58,  1.56it/s]
em: 0.3769, f1: 0.4455, loss: 3.3417 ||: : 2478it [27:05,  1.52it/s]

2019-05-09 22:37:18,849 - INFO - allennlp.training.trainer - Validating
  0%|          | 0/596 [00:00<?, ?it/s]
em: 0.3317, f1: 0.3477, loss: 1177898.2901 ||:   4%|4         | 26/596 [00:10<03:39,  2.59it/s]
em: 0.3034, f1: 0.3252, loss: 1225975.8243 ||:   9%|8         | 52/596 [00:24<03:59,  2.27it/s]
em: 0.3022, f1: 0.3269, loss: 1508941.8541 ||:  12%|#1        | 70/596 [00:35<04:14,  2.07it/s]
em: 0.3000, f1: 0.3255, loss: 1579683.1641 ||:  15%|#5        | 91/596 [00:45<04:05,  2.05it/s]
em: 0.2995, f1: 0.3239, loss: 1635057.8647 ||:  19%|#8        | 112/596 [00:58<04:12,  1.92it/s]
em: 0.2800, f1: 0.3060, loss: 1738152.0637 ||:  23%|##2       | 137/596 [01:08<03:43,  2.05it/s]
em: 0.2668, f1: 0.2909, loss: 1685970.4523 ||:  27%|##7       | 162/596 [01:23<03:47,  1.91it/s]
em: 0.2587, f1: 0.2834, loss: 1703310.4982 ||:  31%|###       | 182/596 [01:34<03:37,  1.90it/s]
em: 0.2522, f1: 0.2768, loss: 1705679.1373 ||:  34%|###4      | 203/596 [01:44<03:22,  1.95it/s]
em: 0.2528, f1: 0.2781, loss: 1612737.4956 ||:  38%|###7      | 224/596 [01:55<03:14,  1.92it/s]
em: 0.2538, f1: 0.2791, loss: 1624508.1574 ||:  41%|####1     | 247/596 [02:06<02:53,  2.01it/s]
em: 0.2559, f1: 0.2815, loss: 1601866.2788 ||:  45%|####5     | 270/596 [02:20<02:53,  1.88it/s]
em: 0.2582, f1: 0.2834, loss: 1566795.4875 ||:  49%|####8     | 292/596 [02:30<02:36,  1.95it/s]
em: 0.2579, f1: 0.2822, loss: 1632180.1173 ||:  53%|#####2    | 314/596 [02:41<02:23,  1.96it/s]
em: 0.2572, f1: 0.2818, loss: 1579356.1141 ||:  56%|#####6    | 334/596 [02:52<02:18,  1.89it/s]
em: 0.2565, f1: 0.2807, loss: 1612975.2775 ||:  59%|#####9    | 353/596 [03:03<02:09,  1.88it/s]
em: 0.2624, f1: 0.2869, loss: 1549074.0834 ||:  62%|######2   | 372/596 [03:14<02:01,  1.84it/s]
em: 0.2638, f1: 0.2886, loss: 1587290.9755 ||:  66%|######5   | 391/596 [03:24<01:51,  1.84it/s]
em: 0.2646, f1: 0.2886, loss: 1563275.2714 ||:  69%|######8   | 411/596 [03:34<01:38,  1.87it/s]
em: 0.2640, f1: 0.2881, loss: 1545838.6568 ||:  72%|#######2  | 431/596 [03:44<01:26,  1.91it/s]
em: 0.2623, f1: 0.2872, loss: 1510547.3186 ||:  76%|#######5  | 451/596 [03:56<01:19,  1.83it/s]
em: 0.2564, f1: 0.2813, loss: 1613387.0424 ||:  79%|#######9  | 473/596 [04:07<01:04,  1.90it/s]
em: 0.2526, f1: 0.2775, loss: 1714661.1695 ||:  83%|########3 | 495/596 [04:18<00:53,  1.90it/s]
em: 0.2498, f1: 0.2751, loss: 1824918.8634 ||:  88%|########7 | 524/596 [04:28<00:34,  2.11it/s]
em: 0.2476, f1: 0.2726, loss: 1901008.7191 ||:  93%|#########2| 553/596 [04:41<00:19,  2.18it/s]
em: 0.2411, f1: 0.2661, loss: 1955170.2112 ||:  97%|#########6| 577/596 [04:54<00:09,  2.07it/s]
em: 0.2376, f1: 0.2630, loss: 1984074.6380 ||: 100%|##########| 596/596 [05:04<00:00,  1.99it/s]
em: 0.2363, f1: 0.2617, loss: 1973317.3424 ||: : 604it [05:11,  1.94it/s]                       

2019-05-09 22:42:30,135 - INFO - allennlp.training.tensorboard_writer -                     Training |  Validation
2019-05-09 22:42:30,137 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB |  5024.000  |       N/A
2019-05-09 22:42:30,137 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB   |  6468.936  |       N/A
2019-05-09 22:42:30,137 - INFO - allennlp.training.tensorboard_writer - loss            |     3.342  |  1973317.342
2019-05-09 22:42:30,138 - INFO - allennlp.training.tensorboard_writer - em              |     0.377  |     0.236
2019-05-09 22:42:30,138 - INFO - allennlp.training.tensorboard_writer - f1              |     0.445  |     0.262
2019-05-09 22:43:12,528 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to 'out/rc_bert_simple_v2/best.th'.
2019-05-09 22:43:54,471 - INFO - allennlp.training.trainer - Epoch duration: 00:33:41
2019-05-09 22:43:54,472 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:49:26
2019-05-09 22:43:54,472 - INFO - allennlp.training.trainer - Epoch 5/9
2019-05-09 22:43:54,472 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 6468.936
2019-05-09 22:43:54,564 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 4639
2019-05-09 22:43:54,570 - INFO - allennlp.training.trainer - Training
  0%|          | 0/2445 [00:00<?, ?it/s]
em: 0.4394, f1: 0.4950, loss: 2.7121 ||:   1%|          | 17/2445 [00:10<24:55,  1.62it/s]
em: 0.4216, f1: 0.4850, loss: 2.7623 ||:   1%|1         | 34/2445 [00:21<25:06,  1.60it/s]
em: 0.3990, f1: 0.4648, loss: 3.0412 ||:   2%|2         | 50/2445 [00:31<25:05,  1.59it/s]
em: 0.3958, f1: 0.4577, loss: 3.1118 ||:   3%|2         | 67/2445 [00:42<24:47,  1.60it/s]
em: 0.3953, f1: 0.4573, loss: 3.1068 ||:   3%|3         | 84/2445 [00:53<24:59,  1.57it/s]
em: 0.4050, f1: 0.4709, loss: 3.0706 ||:   4%|4         | 101/2445 [01:03<24:30,  1.59it/s]
em: 0.4083, f1: 0.4758, loss: 3.0485 ||:   5%|4         | 118/2445 [01:14<24:42,  1.57it/s]
em: 0.4093, f1: 0.4757, loss: 3.0472 ||:   6%|5         | 135/2445 [01:25<24:22,  1.58it/s]
em: 0.4062, f1: 0.4719, loss: 3.0497 ||:   6%|6         | 152/2445 [01:36<24:07,  1.58it/s]
em: 0.4135, f1: 0.4799, loss: 3.0412 ||:   7%|6         | 168/2445 [01:46<24:05,  1.58it/s]
em: 0.4144, f1: 0.4787, loss: 3.0636 ||:   8%|7         | 185/2445 [01:56<23:43,  1.59it/s]
em: 0.4152, f1: 0.4809, loss: 3.0725 ||:   8%|8         | 202/2445 [02:08<23:49,  1.57it/s]
em: 0.4137, f1: 0.4800, loss: 3.1023 ||:   9%|8         | 218/2445 [02:18<23:46,  1.56it/s]
em: 0.4135, f1: 0.4797, loss: 3.1056 ||:  10%|9         | 234/2445 [02:28<23:37,  1.56it/s]
em: 0.4135, f1: 0.4799, loss: 3.1022 ||:  10%|#         | 250/2445 [02:39<23:28,  1.56it/s]
em: 0.4145, f1: 0.4814, loss: 3.1166 ||:  11%|#         | 266/2445 [02:49<23:20,  1.56it/s]
em: 0.4141, f1: 0.4812, loss: 3.1210 ||:  12%|#1        | 283/2445 [02:59<22:49,  1.58it/s]
em: 0.4155, f1: 0.4813, loss: 3.1079 ||:  12%|#2        | 300/2445 [03:10<22:44,  1.57it/s]
em: 0.4167, f1: 0.4816, loss: 3.1033 ||:  13%|#2        | 316/2445 [03:21<22:46,  1.56it/s]
em: 0.4159, f1: 0.4806, loss: 3.0935 ||:  14%|#3        | 333/2445 [03:31<22:25,  1.57it/s]
em: 0.4126, f1: 0.4779, loss: 3.1030 ||:  14%|#4        | 349/2445 [03:42<22:45,  1.53it/s]
em: 0.4099, f1: 0.4755, loss: 3.1172 ||:  15%|#4        | 365/2445 [03:52<22:22,  1.55it/s]
em: 0.4101, f1: 0.4767, loss: 3.1200 ||:  16%|#5        | 381/2445 [04:03<22:17,  1.54it/s]
em: 0.4104, f1: 0.4768, loss: 3.1235 ||:  16%|#6        | 398/2445 [04:13<21:50,  1.56it/s]
em: 0.4096, f1: 0.4759, loss: 3.1223 ||:  17%|#6        | 415/2445 [04:25<21:51,  1.55it/s]
em: 0.4088, f1: 0.4752, loss: 3.1320 ||:  18%|#7        | 431/2445 [04:35<21:38,  1.55it/s]
em: 0.4104, f1: 0.4768, loss: 3.1191 ||:  18%|#8        | 448/2445 [04:45<21:01,  1.58it/s]
em: 0.4077, f1: 0.4747, loss: 3.1415 ||:  19%|#9        | 465/2445 [04:56<21:00,  1.57it/s]
em: 0.4058, f1: 0.4729, loss: 3.1558 ||:  20%|#9        | 481/2445 [05:06<20:52,  1.57it/s]
em: 0.4056, f1: 0.4727, loss: 3.1646 ||:  20%|##        | 497/2445 [05:16<20:38,  1.57it/s]
em: 0.4043, f1: 0.4712, loss: 3.1728 ||:  21%|##1       | 514/2445 [05:27<20:16,  1.59it/s]
em: 0.4028, f1: 0.4699, loss: 3.1836 ||:  22%|##1       | 531/2445 [05:38<20:10,  1.58it/s]
em: 0.4019, f1: 0.4686, loss: 3.1847 ||:  22%|##2       | 547/2445 [05:48<20:06,  1.57it/s]
em: 0.4018, f1: 0.4686, loss: 3.1792 ||:  23%|##3       | 564/2445 [05:59<19:49,  1.58it/s]
em: 0.4010, f1: 0.4676, loss: 3.1850 ||:  24%|##3       | 581/2445 [06:10<19:45,  1.57it/s]
em: 0.4013, f1: 0.4678, loss: 3.1852 ||:  24%|##4       | 597/2445 [06:20<19:45,  1.56it/s]
em: 0.4008, f1: 0.4675, loss: 3.1886 ||:  25%|##5       | 613/2445 [06:30<19:36,  1.56it/s]
em: 0.4001, f1: 0.4669, loss: 3.1943 ||:  26%|##5       | 630/2445 [06:41<19:14,  1.57it/s]
em: 0.3988, f1: 0.4660, loss: 3.2022 ||:  26%|##6       | 647/2445 [06:52<18:55,  1.58it/s]
em: 0.3978, f1: 0.4653, loss: 3.2127 ||:  27%|##7       | 664/2445 [07:03<18:57,  1.57it/s]
em: 0.3975, f1: 0.4650, loss: 3.2127 ||:  28%|##7       | 680/2445 [07:13<18:43,  1.57it/s]
em: 0.3982, f1: 0.4658, loss: 3.2111 ||:  28%|##8       | 696/2445 [07:23<18:39,  1.56it/s]
em: 0.3991, f1: 0.4664, loss: 3.2022 ||:  29%|##9       | 712/2445 [07:33<18:22,  1.57it/s]
em: 0.3988, f1: 0.4662, loss: 3.1999 ||:  30%|##9       | 728/2445 [07:44<18:32,  1.54it/s]
em: 0.3988, f1: 0.4659, loss: 3.2000 ||:  30%|###       | 744/2445 [07:54<18:10,  1.56it/s]
em: 0.3989, f1: 0.4659, loss: 3.2029 ||:  31%|###1      | 760/2445 [08:04<17:56,  1.57it/s]
em: 0.3973, f1: 0.4642, loss: 3.2134 ||:  32%|###1      | 776/2445 [08:14<17:46,  1.56it/s]
em: 0.3962, f1: 0.4634, loss: 3.2187 ||:  32%|###2      | 793/2445 [08:25<17:20,  1.59it/s]
em: 0.3945, f1: 0.4623, loss: 3.2165 ||:  33%|###3      | 810/2445 [08:36<17:27,  1.56it/s]
em: 0.3936, f1: 0.4619, loss: 3.2141 ||:  34%|###3      | 826/2445 [08:46<17:11,  1.57it/s]
em: 0.3938, f1: 0.4619, loss: 3.2149 ||:  34%|###4      | 842/2445 [08:57<17:14,  1.55it/s]
em: 0.3949, f1: 0.4628, loss: 3.2076 ||:  35%|###5      | 858/2445 [09:07<17:10,  1.54it/s]
em: 0.3946, f1: 0.4624, loss: 3.2107 ||:  36%|###5      | 874/2445 [09:18<16:57,  1.54it/s]
em: 0.3941, f1: 0.4618, loss: 3.2135 ||:  36%|###6      | 890/2445 [09:28<16:48,  1.54it/s]
em: 0.3946, f1: 0.4623, loss: 3.2107 ||:  37%|###7      | 906/2445 [09:38<16:28,  1.56it/s]
em: 0.3938, f1: 0.4617, loss: 3.2109 ||:  38%|###7      | 922/2445 [09:48<16:15,  1.56it/s]
em: 0.3941, f1: 0.4616, loss: 3.2126 ||:  38%|###8      | 938/2445 [09:59<16:11,  1.55it/s]
em: 0.3931, f1: 0.4606, loss: 3.2140 ||:  39%|###9      | 954/2445 [10:09<15:58,  1.56it/s]
em: 0.3926, f1: 0.4602, loss: 3.2161 ||:  40%|###9      | 970/2445 [10:19<15:47,  1.56it/s]
em: 0.3917, f1: 0.4593, loss: 3.2250 ||:  40%|####      | 987/2445 [10:29<15:18,  1.59it/s]
em: 0.3923, f1: 0.4597, loss: 3.2211 ||:  41%|####1     | 1004/2445 [10:41<15:17,  1.57it/s]
em: 0.3928, f1: 0.4604, loss: 3.2185 ||:  42%|####1     | 1020/2445 [10:51<15:09,  1.57it/s]
em: 0.3915, f1: 0.4592, loss: 3.2214 ||:  42%|####2     | 1036/2445 [11:01<15:00,  1.57it/s]
em: 0.3915, f1: 0.4596, loss: 3.2173 ||:  43%|####3     | 1052/2445 [11:11<14:48,  1.57it/s]
em: 0.3914, f1: 0.4597, loss: 3.2154 ||:  44%|####3     | 1068/2445 [11:21<14:35,  1.57it/s]
em: 0.3912, f1: 0.4594, loss: 3.2196 ||:  44%|####4     | 1085/2445 [11:32<14:17,  1.59it/s]
em: 0.3903, f1: 0.4589, loss: 3.2216 ||:  45%|####5     | 1102/2445 [11:43<14:09,  1.58it/s]
em: 0.3905, f1: 0.4590, loss: 3.2192 ||:  46%|####5     | 1118/2445 [11:53<14:09,  1.56it/s]
em: 0.3895, f1: 0.4581, loss: 3.2190 ||:  46%|####6     | 1135/2445 [12:04<13:50,  1.58it/s]
em: 0.3889, f1: 0.4578, loss: 3.2214 ||:  47%|####7     | 1152/2445 [12:15<13:47,  1.56it/s]
em: 0.3896, f1: 0.4583, loss: 3.2125 ||:  48%|####7     | 1168/2445 [12:25<13:43,  1.55it/s]
em: 0.3895, f1: 0.4583, loss: 3.2143 ||:  48%|####8     | 1184/2445 [12:36<13:33,  1.55it/s]
em: 0.3896, f1: 0.4585, loss: 3.2133 ||:  49%|####9     | 1200/2445 [12:46<13:24,  1.55it/s]
em: 0.3893, f1: 0.4583, loss: 3.2161 ||:  50%|####9     | 1216/2445 [12:56<13:14,  1.55it/s]
em: 0.3893, f1: 0.4582, loss: 3.2197 ||:  50%|#####     | 1232/2445 [13:07<13:04,  1.55it/s]
em: 0.3893, f1: 0.4583, loss: 3.2237 ||:  51%|#####1    | 1249/2445 [13:17<12:41,  1.57it/s]
em: 0.3885, f1: 0.4575, loss: 3.2277 ||:  52%|#####1    | 1266/2445 [13:28<12:29,  1.57it/s]
em: 0.3890, f1: 0.4581, loss: 3.2239 ||:  52%|#####2    | 1282/2445 [13:38<12:23,  1.56it/s]
em: 0.3890, f1: 0.4583, loss: 3.2256 ||:  53%|#####3    | 1298/2445 [13:49<12:15,  1.56it/s]
em: 0.3891, f1: 0.4584, loss: 3.2263 ||:  54%|#####3    | 1315/2445 [13:59<11:51,  1.59it/s]
em: 0.3894, f1: 0.4586, loss: 3.2220 ||:  54%|#####4    | 1332/2445 [14:10<11:51,  1.56it/s]
em: 0.3903, f1: 0.4593, loss: 3.2206 ||:  55%|#####5    | 1349/2445 [14:20<11:26,  1.60it/s]
em: 0.3907, f1: 0.4596, loss: 3.2177 ||:  56%|#####5    | 1366/2445 [14:31<11:15,  1.60it/s]
em: 0.3903, f1: 0.4591, loss: 3.2199 ||:  57%|#####6    | 1383/2445 [14:42<11:08,  1.59it/s]
em: 0.3901, f1: 0.4587, loss: 3.2208 ||:  57%|#####7    | 1400/2445 [14:52<10:56,  1.59it/s]
em: 0.3891, f1: 0.4579, loss: 3.2230 ||:  58%|#####7    | 1417/2445 [15:04<10:55,  1.57it/s]
em: 0.3891, f1: 0.4578, loss: 3.2230 ||:  59%|#####8    | 1433/2445 [15:14<10:47,  1.56it/s]
em: 0.3892, f1: 0.4580, loss: 3.2235 ||:  59%|#####9    | 1449/2445 [15:24<10:41,  1.55it/s]
em: 0.3896, f1: 0.4585, loss: 3.2228 ||:  60%|#####9    | 1466/2445 [15:35<10:24,  1.57it/s]
em: 0.3893, f1: 0.4580, loss: 3.2234 ||:  61%|######    | 1483/2445 [15:45<10:06,  1.59it/s]
em: 0.3899, f1: 0.4584, loss: 3.2212 ||:  61%|######1   | 1500/2445 [15:56<09:48,  1.61it/s]
em: 0.3902, f1: 0.4585, loss: 3.2210 ||:  62%|######2   | 1517/2445 [16:06<09:40,  1.60it/s]
em: 0.3909, f1: 0.4591, loss: 3.2163 ||:  63%|######2   | 1533/2445 [16:17<09:38,  1.58it/s]
em: 0.3908, f1: 0.4588, loss: 3.2160 ||:  63%|######3   | 1549/2445 [16:27<09:29,  1.57it/s]
em: 0.3907, f1: 0.4585, loss: 3.2150 ||:  64%|######4   | 1565/2445 [16:38<09:24,  1.56it/s]
em: 0.3909, f1: 0.4587, loss: 3.2136 ||:  65%|######4   | 1582/2445 [16:48<09:09,  1.57it/s]
em: 0.3908, f1: 0.4584, loss: 3.2124 ||:  65%|######5   | 1599/2445 [16:59<08:56,  1.58it/s]
em: 0.3908, f1: 0.4585, loss: 3.2126 ||:  66%|######6   | 1615/2445 [17:09<08:47,  1.57it/s]
em: 0.3899, f1: 0.4577, loss: 3.2197 ||:  67%|######6   | 1631/2445 [17:19<08:38,  1.57it/s]
em: 0.3902, f1: 0.4584, loss: 3.2169 ||:  67%|######7   | 1647/2445 [17:29<08:26,  1.58it/s]
em: 0.3905, f1: 0.4589, loss: 3.2166 ||:  68%|######8   | 1663/2445 [17:40<08:19,  1.57it/s]
em: 0.3905, f1: 0.4589, loss: 3.2153 ||:  69%|######8   | 1679/2445 [17:50<08:13,  1.55it/s]
em: 0.3909, f1: 0.4594, loss: 3.2145 ||:  69%|######9   | 1695/2445 [18:01<08:03,  1.55it/s]
em: 0.3915, f1: 0.4599, loss: 3.2121 ||:  70%|#######   | 1712/2445 [18:11<07:47,  1.57it/s]
em: 0.3913, f1: 0.4599, loss: 3.2175 ||:  71%|#######   | 1729/2445 [18:22<07:32,  1.58it/s]
em: 0.3913, f1: 0.4597, loss: 3.2227 ||:  71%|#######1  | 1746/2445 [18:33<07:25,  1.57it/s]
em: 0.3914, f1: 0.4597, loss: 3.2245 ||:  72%|#######2  | 1762/2445 [18:43<07:16,  1.57it/s]
em: 0.3918, f1: 0.4602, loss: 3.2267 ||:  73%|#######2  | 1778/2445 [18:53<07:07,  1.56it/s]
em: 0.3918, f1: 0.4600, loss: 3.2279 ||:  73%|#######3  | 1794/2445 [19:04<07:00,  1.55it/s]
em: 0.3920, f1: 0.4601, loss: 3.2260 ||:  74%|#######4  | 1810/2445 [19:14<06:47,  1.56it/s]
em: 0.3920, f1: 0.4600, loss: 3.2292 ||:  75%|#######4  | 1826/2445 [19:24<06:37,  1.56it/s]
em: 0.3923, f1: 0.4602, loss: 3.2268 ||:  75%|#######5  | 1843/2445 [19:35<06:21,  1.58it/s]
em: 0.3922, f1: 0.4601, loss: 3.2264 ||:  76%|#######6  | 1860/2445 [19:46<06:13,  1.57it/s]
em: 0.3931, f1: 0.4608, loss: 3.2247 ||:  77%|#######6  | 1876/2445 [19:56<06:07,  1.55it/s]
em: 0.3933, f1: 0.4609, loss: 3.2244 ||:  77%|#######7  | 1893/2445 [20:07<05:53,  1.56it/s]
em: 0.3932, f1: 0.4607, loss: 3.2233 ||:  78%|#######8  | 1909/2445 [20:17<05:43,  1.56it/s]
em: 0.3936, f1: 0.4612, loss: 3.2215 ||:  79%|#######8  | 1926/2445 [20:28<05:27,  1.59it/s]
em: 0.3942, f1: 0.4616, loss: 3.2200 ||:  79%|#######9  | 1943/2445 [20:39<05:19,  1.57it/s]
em: 0.3938, f1: 0.4612, loss: 3.2240 ||:  80%|########  | 1959/2445 [20:49<05:10,  1.57it/s]
em: 0.3940, f1: 0.4613, loss: 3.2220 ||:  81%|########  | 1976/2445 [20:59<04:53,  1.60it/s]
em: 0.3943, f1: 0.4614, loss: 3.2208 ||:  82%|########1 | 1993/2445 [21:10<04:45,  1.59it/s]
em: 0.3938, f1: 0.4609, loss: 3.2218 ||:  82%|########2 | 2010/2445 [21:21<04:32,  1.60it/s]
em: 0.3935, f1: 0.4605, loss: 3.2250 ||:  83%|########2 | 2027/2445 [21:32<04:26,  1.57it/s]
em: 0.3936, f1: 0.4608, loss: 3.2258 ||:  84%|########3 | 2043/2445 [21:42<04:14,  1.58it/s]
em: 0.3934, f1: 0.4609, loss: 3.2259 ||:  84%|########4 | 2059/2445 [21:52<04:06,  1.57it/s]
em: 0.3941, f1: 0.4617, loss: 3.2257 ||:  85%|########4 | 2075/2445 [22:02<03:56,  1.57it/s]
em: 0.3939, f1: 0.4614, loss: 3.2270 ||:  86%|########5 | 2091/2445 [22:13<03:47,  1.56it/s]
em: 0.3942, f1: 0.4616, loss: 3.2261 ||:  86%|########6 | 2108/2445 [22:23<03:32,  1.58it/s]
em: 0.3942, f1: 0.4617, loss: 3.2274 ||:  87%|########6 | 2125/2445 [22:34<03:24,  1.57it/s]
em: 0.3939, f1: 0.4614, loss: 3.2280 ||:  88%|########7 | 2141/2445 [22:44<03:14,  1.57it/s]
em: 0.3935, f1: 0.4611, loss: 3.2306 ||:  88%|########8 | 2157/2445 [22:55<03:04,  1.56it/s]
em: 0.3937, f1: 0.4612, loss: 3.2291 ||:  89%|########8 | 2173/2445 [23:05<02:53,  1.56it/s]
em: 0.3935, f1: 0.4611, loss: 3.2288 ||:  90%|########9 | 2189/2445 [23:15<02:43,  1.57it/s]
em: 0.3935, f1: 0.4612, loss: 3.2269 ||:  90%|######### | 2206/2445 [23:26<02:31,  1.57it/s]
em: 0.3936, f1: 0.4614, loss: 3.2253 ||:  91%|######### | 2222/2445 [23:36<02:22,  1.56it/s]
em: 0.3937, f1: 0.4615, loss: 3.2261 ||:  92%|#########1| 2238/2445 [23:46<02:12,  1.56it/s]
em: 0.3939, f1: 0.4618, loss: 3.2268 ||:  92%|#########2| 2254/2445 [23:57<02:02,  1.56it/s]
em: 0.3941, f1: 0.4620, loss: 3.2248 ||:  93%|#########2| 2270/2445 [24:07<01:51,  1.57it/s]
em: 0.3936, f1: 0.4615, loss: 3.2269 ||:  93%|#########3| 2286/2445 [24:18<01:42,  1.55it/s]
em: 0.3934, f1: 0.4613, loss: 3.2276 ||:  94%|#########4| 2302/2445 [24:28<01:32,  1.55it/s]
em: 0.3933, f1: 0.4614, loss: 3.2258 ||:  95%|#########4| 2318/2445 [24:38<01:22,  1.55it/s]
em: 0.3928, f1: 0.4611, loss: 3.2256 ||:  95%|#########5| 2334/2445 [24:48<01:11,  1.56it/s]
em: 0.3921, f1: 0.4604, loss: 3.2255 ||:  96%|#########6| 2350/2445 [24:58<01:00,  1.57it/s]
em: 0.3917, f1: 0.4602, loss: 3.2261 ||:  97%|#########6| 2366/2445 [25:08<00:50,  1.58it/s]
em: 0.3910, f1: 0.4596, loss: 3.2278 ||:  97%|#########7| 2382/2445 [25:19<00:40,  1.57it/s]
em: 0.3902, f1: 0.4589, loss: 3.2301 ||:  98%|#########8| 2398/2445 [25:30<00:30,  1.54it/s]
em: 0.3896, f1: 0.4585, loss: 3.2323 ||:  99%|#########8| 2415/2445 [25:40<00:19,  1.57it/s]
em: 0.3888, f1: 0.4580, loss: 3.2321 ||:  99%|#########9| 2432/2445 [25:51<00:08,  1.57it/s]
em: 0.3881, f1: 0.4575, loss: 3.2346 ||: : 2448it [26:01,  1.56it/s]                        
em: 0.3874, f1: 0.4568, loss: 3.2360 ||: : 2465it [26:12,  1.57it/s]
em: 0.3868, f1: 0.4564, loss: 3.2359 ||: : 2478it [26:20,  1.57it/s]

2019-05-09 23:10:14,939 - INFO - allennlp.training.trainer - Validating
  0%|          | 0/596 [00:00<?, ?it/s]
em: 0.3293, f1: 0.3424, loss: 1225975.3051 ||:   4%|4         | 26/596 [00:10<03:39,  2.59it/s]
em: 0.2973, f1: 0.3188, loss: 1250014.8340 ||:   9%|8         | 52/596 [00:24<03:59,  2.27it/s]
em: 0.2995, f1: 0.3263, loss: 1526799.2192 ||:  12%|#1        | 70/596 [00:35<04:13,  2.07it/s]
em: 0.2938, f1: 0.3202, loss: 1572815.4751 ||:  15%|#5        | 91/596 [00:45<04:05,  2.06it/s]
em: 0.2973, f1: 0.3228, loss: 1640638.5163 ||:  19%|#8        | 112/596 [00:58<04:10,  1.93it/s]
em: 0.2763, f1: 0.3025, loss: 1751838.7027 ||:  23%|##2       | 137/596 [01:08<03:42,  2.06it/s]
em: 0.2648, f1: 0.2896, loss: 1712977.1533 ||:  27%|##7       | 162/596 [01:23<03:46,  1.92it/s]
em: 0.2580, f1: 0.2828, loss: 1713613.2659 ||:  31%|###       | 182/596 [01:34<03:36,  1.91it/s]
em: 0.2528, f1: 0.2779, loss: 1717994.9876 ||:  34%|###4      | 203/596 [01:44<03:21,  1.95it/s]
em: 0.2551, f1: 0.2805, loss: 1618318.4547 ||:  38%|###7      | 224/596 [01:55<03:13,  1.92it/s]
em: 0.2561, f1: 0.2817, loss: 1627039.1054 ||:  41%|####1     | 247/596 [02:05<02:53,  2.01it/s]
em: 0.2591, f1: 0.2849, loss: 1599552.0183 ||:  45%|####5     | 270/596 [02:19<02:52,  1.88it/s]
em: 0.2621, f1: 0.2876, loss: 1566796.0599 ||:  49%|####8     | 292/596 [02:30<02:35,  1.95it/s]
em: 0.2613, f1: 0.2857, loss: 1628199.7766 ||:  53%|#####2    | 314/596 [02:41<02:23,  1.97it/s]
em: 0.2610, f1: 0.2856, loss: 1572776.5152 ||:  56%|#####6    | 335/596 [02:53<02:17,  1.90it/s]
em: 0.2609, f1: 0.2848, loss: 1603888.6708 ||:  60%|#####9    | 355/596 [03:03<02:05,  1.92it/s]
em: 0.2669, f1: 0.2911, loss: 1540015.4119 ||:  63%|######2   | 375/596 [03:15<01:59,  1.84it/s]
em: 0.2683, f1: 0.2926, loss: 1585342.7689 ||:  67%|######6   | 397/596 [03:25<01:43,  1.93it/s]
em: 0.2695, f1: 0.2931, loss: 1536411.6789 ||:  70%|#######   | 419/596 [03:38<01:36,  1.83it/s]
em: 0.2692, f1: 0.2937, loss: 1524336.7756 ||:  74%|#######4  | 442/596 [03:48<01:19,  1.95it/s]
em: 0.2636, f1: 0.2882, loss: 1567219.8522 ||:  78%|#######8  | 465/596 [04:01<01:09,  1.89it/s]
em: 0.2573, f1: 0.2819, loss: 1711080.8510 ||:  82%|########1 | 488/596 [04:11<00:54,  1.99it/s]
em: 0.2547, f1: 0.2801, loss: 1818086.5813 ||:  87%|########6 | 516/596 [04:22<00:36,  2.17it/s]
em: 0.2536, f1: 0.2787, loss: 1877312.4732 ||:  91%|#########1| 544/596 [04:33<00:23,  2.23it/s]
em: 0.2475, f1: 0.2727, loss: 1924530.6302 ||:  95%|#########5| 568/596 [04:46<00:13,  2.13it/s]
em: 0.2435, f1: 0.2690, loss: 1983065.5445 ||:  99%|#########8| 590/596 [04:56<00:02,  2.13it/s]
em: 0.2406, f1: 0.2662, loss: 1972283.1370 ||: : 604it [05:06,  1.97it/s]                       

2019-05-09 23:15:21,900 - INFO - allennlp.training.tensorboard_writer -                     Training |  Validation
2019-05-09 23:15:21,901 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB |  4639.000  |       N/A
2019-05-09 23:15:21,901 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB   |  6468.936  |       N/A
2019-05-09 23:15:21,901 - INFO - allennlp.training.tensorboard_writer - loss            |     3.236  |  1972283.137
2019-05-09 23:15:21,902 - INFO - allennlp.training.tensorboard_writer - em              |     0.387  |     0.241
2019-05-09 23:15:21,902 - INFO - allennlp.training.tensorboard_writer - f1              |     0.456  |     0.266
2019-05-09 23:16:04,314 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to 'out/rc_bert_simple_v2/best.th'.
2019-05-09 23:16:46,592 - INFO - allennlp.training.trainer - Epoch duration: 00:32:52
2019-05-09 23:16:46,593 - INFO - allennlp.training.trainer - Estimated training time remaining: 2:14:52
2019-05-09 23:16:46,593 - INFO - allennlp.training.trainer - Epoch 6/9
2019-05-09 23:16:46,593 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 6469.136
2019-05-09 23:16:46,683 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 4556
2019-05-09 23:16:46,689 - INFO - allennlp.training.trainer - Training
  0%|          | 0/2445 [00:00<?, ?it/s]
em: 0.4669, f1: 0.5428, loss: 2.7468 ||:   1%|          | 17/2445 [00:10<25:16,  1.60it/s]
em: 0.4534, f1: 0.5247, loss: 2.7391 ||:   1%|1         | 34/2445 [00:20<24:54,  1.61it/s]
em: 0.4480, f1: 0.5086, loss: 2.9545 ||:   2%|2         | 51/2445 [00:32<25:05,  1.59it/s]
em: 0.4182, f1: 0.4801, loss: 3.0723 ||:   3%|2         | 67/2445 [00:42<25:02,  1.58it/s]
em: 0.4179, f1: 0.4805, loss: 3.0534 ||:   3%|3         | 84/2445 [00:52<24:47,  1.59it/s]
em: 0.4215, f1: 0.4840, loss: 3.0091 ||:   4%|4         | 101/2445 [01:03<24:31,  1.59it/s]
em: 0.4222, f1: 0.4874, loss: 2.9684 ||:   5%|4         | 118/2445 [01:14<24:43,  1.57it/s]
em: 0.4206, f1: 0.4853, loss: 2.9675 ||:   6%|5         | 135/2445 [01:25<24:16,  1.59it/s]
em: 0.4229, f1: 0.4869, loss: 2.9606 ||:   6%|6         | 152/2445 [01:35<24:05,  1.59it/s]
em: 0.4303, f1: 0.4949, loss: 2.9522 ||:   7%|6         | 168/2445 [01:46<24:02,  1.58it/s]
em: 0.4301, f1: 0.4944, loss: 2.9918 ||:   8%|7         | 185/2445 [01:56<23:39,  1.59it/s]
em: 0.4283, f1: 0.4949, loss: 3.0128 ||:   8%|8         | 202/2445 [02:07<23:45,  1.57it/s]
em: 0.4231, f1: 0.4907, loss: 3.0306 ||:   9%|8         | 218/2445 [02:17<23:41,  1.57it/s]
em: 0.4241, f1: 0.4915, loss: 3.0504 ||:  10%|9         | 234/2445 [02:28<23:42,  1.55it/s]
em: 0.4257, f1: 0.4934, loss: 3.0404 ||:  10%|#         | 250/2445 [02:38<23:30,  1.56it/s]
em: 0.4260, f1: 0.4937, loss: 3.0323 ||:  11%|#         | 266/2445 [02:48<23:16,  1.56it/s]
em: 0.4302, f1: 0.4972, loss: 3.0243 ||:  12%|#1        | 283/2445 [02:59<22:51,  1.58it/s]
em: 0.4299, f1: 0.4972, loss: 3.0164 ||:  12%|#2        | 300/2445 [03:10<22:37,  1.58it/s]
em: 0.4291, f1: 0.4964, loss: 3.0089 ||:  13%|#2        | 316/2445 [03:20<22:30,  1.58it/s]
em: 0.4275, f1: 0.4939, loss: 3.0115 ||:  14%|#3        | 332/2445 [03:30<22:35,  1.56it/s]
em: 0.4263, f1: 0.4936, loss: 3.0163 ||:  14%|#4        | 348/2445 [03:41<22:32,  1.55it/s]
em: 0.4247, f1: 0.4919, loss: 3.0182 ||:  15%|#4        | 365/2445 [03:51<22:03,  1.57it/s]
em: 0.4236, f1: 0.4918, loss: 3.0207 ||:  16%|#5        | 382/2445 [04:02<22:04,  1.56it/s]
