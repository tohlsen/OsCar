2019-05-09 19:48:18,187 - INFO - allennlp.common.params - random_seed = 13370
2019-05-09 19:48:18,187 - INFO - allennlp.common.params - numpy_seed = 1337
2019-05-09 19:48:18,187 - INFO - allennlp.common.params - pytorch_seed = 133
2019-05-09 19:48:18,212 - INFO - allennlp.common.checks - Pytorch version: 1.0.0
2019-05-09 19:48:18,223 - INFO - allennlp.common.params - evaluate_on_test = False
2019-05-09 19:48:18,223 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'instance_format': 'drop', 'passage_length_limit': 200, 'question_length_limit': 50, 'skip_when_all_empty': ['passage_span', 'question_span', 'addition_subtraction', 'counting'], 'token_indexers': {'bert': {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'type': 'bert-pretrained', 'use_starting_offsets': False}, 'token_characters': {'min_padding_length': 5, 'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}, 'type': 'drop_rc_v1'} and extras set()
2019-05-09 19:48:18,224 - INFO - allennlp.common.params - dataset_reader.type = drop_rc_v1
2019-05-09 19:48:18,224 - INFO - allennlp.common.from_params - instantiating class <class 'drop_library.dataset_readers.rc_drop_reader_v1.RCDropReaderV1'> from params {'instance_format': 'drop', 'passage_length_limit': 200, 'question_length_limit': 50, 'skip_when_all_empty': ['passage_span', 'question_span', 'addition_subtraction', 'counting'], 'token_indexers': {'bert': {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'type': 'bert-pretrained', 'use_starting_offsets': False}, 'token_characters': {'min_padding_length': 5, 'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}} and extras set()
2019-05-09 19:48:18,225 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'type': 'bert-pretrained', 'use_starting_offsets': False} and extras set()
2019-05-09 19:48:18,225 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.type = bert-pretrained
2019-05-09 19:48:18,225 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer from params {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'use_starting_offsets': False} and extras set()
2019-05-09 19:48:18,225 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.pretrained_model = bert-base-uncased
2019-05-09 19:48:18,226 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.use_starting_offsets = False
2019-05-09 19:48:18,226 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.do_lowercase = True
2019-05-09 19:48:18,226 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.never_lowercase = None
2019-05-09 19:48:18,226 - INFO - allennlp.common.params - dataset_reader.token_indexers.bert.max_pieces = 512
2019-05-09 19:48:18,537 - INFO - pytorch_pretrained_bert.tokenization - loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /homes/iws/patelr3/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
2019-05-09 19:48:18,573 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'min_padding_length': 5, 'type': 'characters'} and extras set()
2019-05-09 19:48:18,573 - INFO - allennlp.common.params - dataset_reader.token_indexers.token_characters.type = characters
2019-05-09 19:48:18,573 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer from params {'min_padding_length': 5} and extras set()
2019-05-09 19:48:18,574 - INFO - allennlp.common.params - dataset_reader.token_indexers.token_characters.namespace = token_characters
2019-05-09 19:48:18,574 - INFO - allennlp.common.params - dataset_reader.token_indexers.token_characters.start_tokens = None
2019-05-09 19:48:18,574 - INFO - allennlp.common.params - dataset_reader.token_indexers.token_characters.end_tokens = None
2019-05-09 19:48:18,574 - INFO - allennlp.common.params - dataset_reader.token_indexers.token_characters.min_padding_length = 5
2019-05-09 19:48:18,574 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'lowercase_tokens': True, 'type': 'single_id'} and extras set()
2019-05-09 19:48:18,574 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = single_id
2019-05-09 19:48:18,574 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer from params {'lowercase_tokens': True} and extras set()
2019-05-09 19:48:18,574 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tokens
2019-05-09 19:48:18,574 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.lowercase_tokens = True
2019-05-09 19:48:18,575 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.start_tokens = None
2019-05-09 19:48:18,575 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.end_tokens = None
2019-05-09 19:48:18,575 - INFO - allennlp.common.params - dataset_reader.lazy = False
2019-05-09 19:48:18,575 - INFO - allennlp.common.params - dataset_reader.passage_length_limit = 200
2019-05-09 19:48:18,575 - INFO - allennlp.common.params - dataset_reader.question_length_limit = 50
2019-05-09 19:48:18,575 - INFO - allennlp.common.params - dataset_reader.skip_when_all_empty = ['passage_span', 'question_span', 'addition_subtraction', 'counting']
2019-05-09 19:48:18,575 - INFO - allennlp.common.params - dataset_reader.instance_format = drop
2019-05-09 19:48:18,575 - INFO - allennlp.common.params - dataset_reader.relaxed_span_match_for_finding_labels = True
2019-05-09 19:48:18,893 - INFO - allennlp.training.util - Using a separate dataset reader to load validation and test data.
2019-05-09 19:48:18,894 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.dataset_readers.dataset_reader.DatasetReader'> from params {'instance_format': 'drop', 'passage_length_limit': 400, 'question_length_limit': 50, 'skip_when_all_empty': [], 'token_indexers': {'bert': {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'type': 'bert-pretrained', 'use_starting_offsets': False}, 'token_characters': {'min_padding_length': 5, 'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}, 'type': 'drop'} and extras set()
2019-05-09 19:48:18,894 - INFO - allennlp.common.params - validation_dataset_reader.type = drop
2019-05-09 19:48:18,894 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.dataset_readers.reading_comprehension.drop.DropReader'> from params {'instance_format': 'drop', 'passage_length_limit': 400, 'question_length_limit': 50, 'skip_when_all_empty': [], 'token_indexers': {'bert': {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'type': 'bert-pretrained', 'use_starting_offsets': False}, 'token_characters': {'min_padding_length': 5, 'type': 'characters'}, 'tokens': {'lowercase_tokens': True, 'type': 'single_id'}}} and extras set()
2019-05-09 19:48:18,894 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'type': 'bert-pretrained', 'use_starting_offsets': False} and extras set()
2019-05-09 19:48:18,894 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.bert.type = bert-pretrained
2019-05-09 19:48:18,894 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.wordpiece_indexer.PretrainedBertIndexer from params {'do_lowercase': True, 'pretrained_model': 'bert-base-uncased', 'use_starting_offsets': False} and extras set()
2019-05-09 19:48:18,894 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.bert.pretrained_model = bert-base-uncased
2019-05-09 19:48:18,894 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.bert.use_starting_offsets = False
2019-05-09 19:48:18,895 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.bert.do_lowercase = True
2019-05-09 19:48:18,895 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.bert.never_lowercase = None
2019-05-09 19:48:18,895 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.bert.max_pieces = 512
2019-05-09 19:48:19,192 - INFO - pytorch_pretrained_bert.tokenization - loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /homes/iws/patelr3/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
2019-05-09 19:48:19,224 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'min_padding_length': 5, 'type': 'characters'} and extras set()
2019-05-09 19:48:19,224 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.token_characters.type = characters
2019-05-09 19:48:19,224 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer from params {'min_padding_length': 5} and extras set()
2019-05-09 19:48:19,224 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.token_characters.namespace = token_characters
2019-05-09 19:48:19,224 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.token_characters.start_tokens = None
2019-05-09 19:48:19,224 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.token_characters.end_tokens = None
2019-05-09 19:48:19,225 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.token_characters.min_padding_length = 5
2019-05-09 19:48:19,225 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.token_indexer.TokenIndexer from params {'lowercase_tokens': True, 'type': 'single_id'} and extras set()
2019-05-09 19:48:19,225 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.type = single_id
2019-05-09 19:48:19,225 - INFO - allennlp.common.from_params - instantiating class allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer from params {'lowercase_tokens': True} and extras set()
2019-05-09 19:48:19,225 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.namespace = tokens
2019-05-09 19:48:19,225 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.lowercase_tokens = True
2019-05-09 19:48:19,225 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.start_tokens = None
2019-05-09 19:48:19,225 - INFO - allennlp.common.params - validation_dataset_reader.token_indexers.tokens.end_tokens = None
2019-05-09 19:48:19,225 - INFO - allennlp.common.params - validation_dataset_reader.lazy = False
2019-05-09 19:48:19,225 - INFO - allennlp.common.params - validation_dataset_reader.passage_length_limit = 400
2019-05-09 19:48:19,225 - INFO - allennlp.common.params - validation_dataset_reader.question_length_limit = 50
2019-05-09 19:48:19,226 - INFO - allennlp.common.params - validation_dataset_reader.skip_when_all_empty = []
2019-05-09 19:48:19,226 - INFO - allennlp.common.params - validation_dataset_reader.instance_format = drop
2019-05-09 19:48:19,226 - INFO - allennlp.common.params - validation_dataset_reader.relaxed_span_match_for_finding_labels = True
2019-05-09 19:48:19,226 - INFO - allennlp.common.params - train_data_path = drop_dataset/drop_dataset_train.json
2019-05-09 19:48:19,226 - INFO - allennlp.training.util - Reading training data from drop_dataset/drop_dataset_train.json
0it [00:00, ?it/s]
2019-05-09 19:48:19,227 - INFO - drop_library.dataset_readers.rc_drop_reader_v1 - Reading file at drop_dataset/drop_dataset_train.json
2019-05-09 19:48:20,116 - INFO - drop_library.dataset_readers.rc_drop_reader_v1 - Reading the dataset
3015it [00:10, 301.47it/s]
6379it [00:20, 309.31it/s]
9919it [00:30, 321.49it/s]
13530it [00:40, 332.23it/s]
17151it [00:50, 340.65it/s]
20945it [01:00, 351.39it/s]
24738it [01:11, 349.29it/s]
28464it [01:21, 349.78it/s]
32234it [01:31, 357.52it/s]
36004it [01:41, 362.64it/s]
39756it [01:52, 357.77it/s]
43780it [02:02, 370.08it/s]
47804it [02:14, 361.32it/s]
51648it [02:24, 367.86it/s]
55513it [02:34, 373.25it/s]
59378it [02:46, 360.49it/s]
62811it [02:56, 355.14it/s]
66244it [03:08, 329.87it/s]
2019-05-09 19:51:30,870 - INFO - drop_library.dataset_readers.rc_drop_reader_v1 - Skipped 10147 questions, kept 67262 questions.
67262it [03:11, 350.94it/s]

2019-05-09 19:51:30,889 - INFO - allennlp.common.params - validation_data_path = drop_dataset/drop_dataset_dev.json
2019-05-09 19:51:30,889 - INFO - allennlp.training.util - Reading validation data from drop_dataset/drop_dataset_dev.json
2019-05-09 19:51:30,891 - INFO - allennlp.data.dataset_readers.reading_comprehension.drop - Reading file at drop_dataset/drop_dataset_dev.json
2019-05-09 19:51:30,996 - INFO - allennlp.data.dataset_readers.reading_comprehension.drop - Reading the dataset
2019-05-09 19:51:48,046 - INFO - allennlp.data.dataset_readers.reading_comprehension.drop - Skipped 0 questions, kept 9536 questions.
2019-05-09 19:51:48,051 - INFO - allennlp.common.params - test_data_path = None
2019-05-09 19:51:48,052 - INFO - allennlp.training.trainer - From dataset instances, validation, train will be considered for vocabulary creation.
2019-05-09 19:51:48,052 - INFO - allennlp.common.params - vocabulary.type = None
2019-05-09 19:51:48,052 - INFO - allennlp.common.params - vocabulary.extend = False
2019-05-09 19:51:48,052 - INFO - allennlp.common.params - vocabulary.directory_path = None
2019-05-09 19:51:48,052 - INFO - allennlp.common.params - vocabulary.max_vocab_size = None
2019-05-09 19:51:48,052 - INFO - allennlp.common.params - vocabulary.non_padded_namespaces = ('*tags', '*labels')
2019-05-09 19:51:48,052 - INFO - allennlp.common.params - vocabulary.min_pretrained_embeddings = None
2019-05-09 19:51:48,052 - INFO - allennlp.common.params - vocabulary.only_include_pretrained_words = True
2019-05-09 19:51:48,052 - INFO - allennlp.common.params - vocabulary.tokens_to_add = None
2019-05-09 19:51:48,052 - INFO - allennlp.data.vocabulary - Fitting token dictionary from dataset.
0it [00:00, ?it/s]
8871it [00:10, 886.99it/s]
17768it [00:20, 887.80it/s]
26665it [00:30, 880.36it/s]
35329it [00:40, 876.11it/s]
44169it [00:50, 878.43it/s]
53008it [01:00, 876.97it/s]
61745it [01:10, 875.98it/s]
70482it [01:20, 867.96it/s]
76798it [01:28, 866.03it/s]

2019-05-09 19:53:16,732 - INFO - allennlp.data.vocabulary - Reading pretrained tokens from: https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.lower.converted.zip
0it [00:00, ?it/s]
475058it [00:10, 47505.60it/s]
957076it [00:20, 47712.08it/s]
1441352it [00:30, 47924.43it/s]
1702926it [00:35, 48139.17it/s]

2019-05-09 19:53:53,028 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.models.model.Model'> from params {'answering_abilities': ['passage_span_extraction', 'question_span_extraction', 'addition_subtraction', 'counting'], 'dropout_prob': 0.1, 'matrix_attention_layer': {'combination': 'x,y,x*y', 'tensor_1_dim': 128, 'tensor_2_dim': 128, 'type': 'linear'}, 'modeling_layer': {'attention_dropout_prob': 0, 'attention_projection_dim': 128, 'conv_kernel_size': 5, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 128, 'hidden_dim': 128, 'input_dim': 128, 'layer_dropout_undecayed_prob': 0.1, 'num_attention_heads': 8, 'num_blocks': 2, 'num_convs_per_block': 2, 'type': 'qanet_encoder'}, 'num_highway_layers': 2, 'phrase_layer': {'attention_dropout_prob': 0, 'attention_projection_dim': 128, 'conv_kernel_size': 7, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 128, 'hidden_dim': 128, 'input_dim': 128, 'layer_dropout_undecayed_prob': 0.1, 'num_attention_heads': 8, 'num_blocks': 1, 'num_convs_per_block': 4, 'type': 'qanet_encoder'}, 'regularizer': [['.*', {'alpha': 1e-07, 'type': 'l2'}]], 'text_field_embedder': {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'bert': ['bert', 'bert-offsets'], 'token_characters': ['token_characters'], 'tokens': ['tokens']}, 'token_embedders': {'bert': {'pretrained_model': 'bert-base-uncased', 'requires_grad': False, 'top_layer_only': True, 'type': 'bert-pretrained'}, 'token_characters': {'embedding': {'embedding_dim': 64}, 'encoder': {'embedding_dim': 64, 'ngram_filter_sizes': [5], 'num_filters': 200, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 300, 'pretrained_file': 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.lower.converted.zip', 'trainable': False, 'type': 'embedding'}}}, 'type': 'naqanet'} and extras {'vocab'}
2019-05-09 19:53:53,028 - INFO - allennlp.common.params - model.type = naqanet
2019-05-09 19:53:53,028 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.models.reading_comprehension.naqanet.NumericallyAugmentedQaNet'> from params {'answering_abilities': ['passage_span_extraction', 'question_span_extraction', 'addition_subtraction', 'counting'], 'dropout_prob': 0.1, 'matrix_attention_layer': {'combination': 'x,y,x*y', 'tensor_1_dim': 128, 'tensor_2_dim': 128, 'type': 'linear'}, 'modeling_layer': {'attention_dropout_prob': 0, 'attention_projection_dim': 128, 'conv_kernel_size': 5, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 128, 'hidden_dim': 128, 'input_dim': 128, 'layer_dropout_undecayed_prob': 0.1, 'num_attention_heads': 8, 'num_blocks': 2, 'num_convs_per_block': 2, 'type': 'qanet_encoder'}, 'num_highway_layers': 2, 'phrase_layer': {'attention_dropout_prob': 0, 'attention_projection_dim': 128, 'conv_kernel_size': 7, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 128, 'hidden_dim': 128, 'input_dim': 128, 'layer_dropout_undecayed_prob': 0.1, 'num_attention_heads': 8, 'num_blocks': 1, 'num_convs_per_block': 4, 'type': 'qanet_encoder'}, 'regularizer': [['.*', {'alpha': 1e-07, 'type': 'l2'}]], 'text_field_embedder': {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'bert': ['bert', 'bert-offsets'], 'token_characters': ['token_characters'], 'tokens': ['tokens']}, 'token_embedders': {'bert': {'pretrained_model': 'bert-base-uncased', 'requires_grad': False, 'top_layer_only': True, 'type': 'bert-pretrained'}, 'token_characters': {'embedding': {'embedding_dim': 64}, 'encoder': {'embedding_dim': 64, 'ngram_filter_sizes': [5], 'num_filters': 200, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 300, 'pretrained_file': 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.lower.converted.zip', 'trainable': False, 'type': 'embedding'}}}} and extras {'vocab'}
2019-05-09 19:53:53,029 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder'> from params {'allow_unmatched_keys': True, 'embedder_to_indexer_map': {'bert': ['bert', 'bert-offsets'], 'token_characters': ['token_characters'], 'tokens': ['tokens']}, 'token_embedders': {'bert': {'pretrained_model': 'bert-base-uncased', 'requires_grad': False, 'top_layer_only': True, 'type': 'bert-pretrained'}, 'token_characters': {'embedding': {'embedding_dim': 64}, 'encoder': {'embedding_dim': 64, 'ngram_filter_sizes': [5], 'num_filters': 200, 'type': 'cnn'}, 'type': 'character_encoding'}, 'tokens': {'embedding_dim': 300, 'pretrained_file': 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.lower.converted.zip', 'trainable': False, 'type': 'embedding'}}} and extras {'vocab'}
2019-05-09 19:53:53,029 - INFO - allennlp.common.params - model.text_field_embedder.type = basic
2019-05-09 19:53:53,029 - INFO - allennlp.common.params - model.text_field_embedder.allow_unmatched_keys = True
2019-05-09 19:53:53,029 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'pretrained_model': 'bert-base-uncased', 'requires_grad': False, 'top_layer_only': True, 'type': 'bert-pretrained'} and extras {'vocab'}
2019-05-09 19:53:53,029 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.type = bert-pretrained
2019-05-09 19:53:53,029 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.bert_token_embedder.PretrainedBertEmbedder'> from params {'pretrained_model': 'bert-base-uncased', 'requires_grad': False, 'top_layer_only': True} and extras {'vocab'}
2019-05-09 19:53:53,029 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.pretrained_model = bert-base-uncased
2019-05-09 19:53:53,029 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.requires_grad = False
2019-05-09 19:53:53,029 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.bert.top_layer_only = True
2019-05-09 19:53:53,380 - INFO - pytorch_pretrained_bert.modeling - loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /homes/iws/patelr3/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba
2019-05-09 19:53:53,383 - INFO - pytorch_pretrained_bert.modeling - extracting archive file /homes/iws/patelr3/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp0k4fkmvt
2019-05-09 19:53:56,712 - INFO - pytorch_pretrained_bert.modeling - Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

2019-05-09 19:54:05,199 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding': {'embedding_dim': 64}, 'encoder': {'embedding_dim': 64, 'ngram_filter_sizes': [5], 'num_filters': 200, 'type': 'cnn'}, 'type': 'character_encoding'} and extras {'vocab'}
2019-05-09 19:54:05,200 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.type = character_encoding
2019-05-09 19:54:05,200 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.embedding.num_embeddings = None
2019-05-09 19:54:05,200 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.embedding.vocab_namespace = token_characters
2019-05-09 19:54:05,200 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.embedding.embedding_dim = 64
2019-05-09 19:54:05,200 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.embedding.pretrained_file = None
2019-05-09 19:54:05,200 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.embedding.projection_dim = None
2019-05-09 19:54:05,200 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.embedding.trainable = True
2019-05-09 19:54:05,200 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.embedding.padding_index = None
2019-05-09 19:54:05,200 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.embedding.max_norm = None
2019-05-09 19:54:05,200 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.embedding.norm_type = 2.0
2019-05-09 19:54:05,201 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.embedding.scale_grad_by_freq = False
2019-05-09 19:54:05,201 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.embedding.sparse = False
2019-05-09 19:54:05,201 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder'> from params {'embedding_dim': 64, 'ngram_filter_sizes': [5], 'num_filters': 200, 'type': 'cnn'} and extras set()
2019-05-09 19:54:05,201 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.encoder.type = cnn
2019-05-09 19:54:05,201 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2vec_encoders.cnn_encoder.CnnEncoder'> from params {'embedding_dim': 64, 'ngram_filter_sizes': [5], 'num_filters': 200} and extras set()
2019-05-09 19:54:05,201 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.encoder.embedding_dim = 64
2019-05-09 19:54:05,201 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.encoder.num_filters = 200
2019-05-09 19:54:05,201 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.encoder.ngram_filter_sizes = [5]
2019-05-09 19:54:05,201 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.encoder.output_dim = None
2019-05-09 19:54:05,204 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.token_characters.dropout = 0.0
2019-05-09 19:54:05,204 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.token_embedders.token_embedder.TokenEmbedder'> from params {'embedding_dim': 300, 'pretrained_file': 'https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.lower.converted.zip', 'trainable': False, 'type': 'embedding'} and extras {'vocab'}
2019-05-09 19:54:05,204 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = embedding
2019-05-09 19:54:05,204 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.num_embeddings = None
2019-05-09 19:54:05,204 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.vocab_namespace = tokens
2019-05-09 19:54:05,204 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.embedding_dim = 300
2019-05-09 19:54:05,204 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.pretrained_file = https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.840B.300d.lower.converted.zip
2019-05-09 19:54:05,205 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.projection_dim = None
2019-05-09 19:54:05,205 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.trainable = False
2019-05-09 19:54:05,205 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.padding_index = None
2019-05-09 19:54:05,205 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_norm = None
2019-05-09 19:54:05,205 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.norm_type = 2.0
2019-05-09 19:54:05,205 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.scale_grad_by_freq = False
2019-05-09 19:54:05,205 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sparse = False
2019-05-09 19:54:05,208 - INFO - allennlp.modules.token_embedders.embedding - Reading pretrained embeddings from file
0it [00:00, ?it/s]
376087it [00:10, 37608.63it/s]
816012it [00:20, 39320.34it/s]
1280313it [00:30, 41213.58it/s]
1702926it [00:38, 43719.40it/s]

2019-05-09 19:54:44,505 - INFO - allennlp.modules.token_embedders.embedding - Initializing pre-trained embedding layer
2019-05-09 19:54:45,660 - INFO - allennlp.modules.token_embedders.embedding - Pretrained embeddings were found for 36830 out of 36832 tokens
2019-05-09 19:54:45,669 - INFO - allennlp.common.params - model.num_highway_layers = 2
2019-05-09 19:54:45,670 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'attention_dropout_prob': 0, 'attention_projection_dim': 128, 'conv_kernel_size': 7, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 128, 'hidden_dim': 128, 'input_dim': 128, 'layer_dropout_undecayed_prob': 0.1, 'num_attention_heads': 8, 'num_blocks': 1, 'num_convs_per_block': 4, 'type': 'qanet_encoder'} and extras {'vocab'}
2019-05-09 19:54:45,670 - INFO - allennlp.common.params - model.phrase_layer.type = qanet_encoder
2019-05-09 19:54:45,670 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2seq_encoders.qanet_encoder.QaNetEncoder'> from params {'attention_dropout_prob': 0, 'attention_projection_dim': 128, 'conv_kernel_size': 7, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 128, 'hidden_dim': 128, 'input_dim': 128, 'layer_dropout_undecayed_prob': 0.1, 'num_attention_heads': 8, 'num_blocks': 1, 'num_convs_per_block': 4} and extras {'vocab'}
2019-05-09 19:54:45,670 - INFO - allennlp.common.params - model.phrase_layer.input_dim = 128
2019-05-09 19:54:45,670 - INFO - allennlp.common.params - model.phrase_layer.hidden_dim = 128
2019-05-09 19:54:45,670 - INFO - allennlp.common.params - model.phrase_layer.attention_projection_dim = 128
2019-05-09 19:54:45,670 - INFO - allennlp.common.params - model.phrase_layer.feedforward_hidden_dim = 128
2019-05-09 19:54:45,670 - INFO - allennlp.common.params - model.phrase_layer.num_blocks = 1
2019-05-09 19:54:45,671 - INFO - allennlp.common.params - model.phrase_layer.num_convs_per_block = 4
2019-05-09 19:54:45,671 - INFO - allennlp.common.params - model.phrase_layer.conv_kernel_size = 7
2019-05-09 19:54:45,671 - INFO - allennlp.common.params - model.phrase_layer.num_attention_heads = 8
2019-05-09 19:54:45,671 - INFO - allennlp.common.params - model.phrase_layer.use_positional_encoding = True
2019-05-09 19:54:45,671 - INFO - allennlp.common.params - model.phrase_layer.dropout_prob = 0.1
2019-05-09 19:54:45,671 - INFO - allennlp.common.params - model.phrase_layer.layer_dropout_undecayed_prob = 0.1
2019-05-09 19:54:45,671 - INFO - allennlp.common.params - model.phrase_layer.attention_dropout_prob = 0
2019-05-09 19:54:45,676 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.matrix_attention.matrix_attention.MatrixAttention'> from params {'combination': 'x,y,x*y', 'tensor_1_dim': 128, 'tensor_2_dim': 128, 'type': 'linear'} and extras {'vocab'}
2019-05-09 19:54:45,676 - INFO - allennlp.common.params - model.matrix_attention_layer.type = linear
2019-05-09 19:54:45,676 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.matrix_attention.linear_matrix_attention.LinearMatrixAttention'> from params {'combination': 'x,y,x*y', 'tensor_1_dim': 128, 'tensor_2_dim': 128} and extras {'vocab'}
2019-05-09 19:54:45,676 - INFO - allennlp.common.params - model.matrix_attention_layer.tensor_1_dim = 128
2019-05-09 19:54:45,676 - INFO - allennlp.common.params - model.matrix_attention_layer.tensor_2_dim = 128
2019-05-09 19:54:45,676 - INFO - allennlp.common.params - model.matrix_attention_layer.combination = x,y,x*y
2019-05-09 19:54:45,676 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder'> from params {'attention_dropout_prob': 0, 'attention_projection_dim': 128, 'conv_kernel_size': 5, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 128, 'hidden_dim': 128, 'input_dim': 128, 'layer_dropout_undecayed_prob': 0.1, 'num_attention_heads': 8, 'num_blocks': 2, 'num_convs_per_block': 2, 'type': 'qanet_encoder'} and extras {'vocab'}
2019-05-09 19:54:45,677 - INFO - allennlp.common.params - model.modeling_layer.type = qanet_encoder
2019-05-09 19:54:45,677 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.modules.seq2seq_encoders.qanet_encoder.QaNetEncoder'> from params {'attention_dropout_prob': 0, 'attention_projection_dim': 128, 'conv_kernel_size': 5, 'dropout_prob': 0.1, 'feedforward_hidden_dim': 128, 'hidden_dim': 128, 'input_dim': 128, 'layer_dropout_undecayed_prob': 0.1, 'num_attention_heads': 8, 'num_blocks': 2, 'num_convs_per_block': 2} and extras {'vocab'}
2019-05-09 19:54:45,677 - INFO - allennlp.common.params - model.modeling_layer.input_dim = 128
2019-05-09 19:54:45,677 - INFO - allennlp.common.params - model.modeling_layer.hidden_dim = 128
2019-05-09 19:54:45,677 - INFO - allennlp.common.params - model.modeling_layer.attention_projection_dim = 128
2019-05-09 19:54:45,677 - INFO - allennlp.common.params - model.modeling_layer.feedforward_hidden_dim = 128
2019-05-09 19:54:45,677 - INFO - allennlp.common.params - model.modeling_layer.num_blocks = 2
2019-05-09 19:54:45,677 - INFO - allennlp.common.params - model.modeling_layer.num_convs_per_block = 2
2019-05-09 19:54:45,677 - INFO - allennlp.common.params - model.modeling_layer.conv_kernel_size = 5
2019-05-09 19:54:45,677 - INFO - allennlp.common.params - model.modeling_layer.num_attention_heads = 8
2019-05-09 19:54:45,677 - INFO - allennlp.common.params - model.modeling_layer.use_positional_encoding = True
2019-05-09 19:54:45,677 - INFO - allennlp.common.params - model.modeling_layer.dropout_prob = 0.1
2019-05-09 19:54:45,678 - INFO - allennlp.common.params - model.modeling_layer.layer_dropout_undecayed_prob = 0.1
2019-05-09 19:54:45,678 - INFO - allennlp.common.params - model.modeling_layer.attention_dropout_prob = 0
2019-05-09 19:54:45,683 - INFO - allennlp.common.params - model.dropout_prob = 0.1
2019-05-09 19:54:45,684 - INFO - allennlp.common.params - model.regularizer = [['.*', {'alpha': 1e-07, 'type': 'l2'}]]
2019-05-09 19:54:45,684 - INFO - allennlp.common.params - model.regularizer.0.1.type = l2
2019-05-09 19:54:45,684 - INFO - allennlp.common.params - model.answering_abilities = ['passage_span_extraction', 'question_span_extraction', 'addition_subtraction', 'counting']
2019-05-09 19:54:45,693 - INFO - allennlp.nn.initializers - Initializing parameters
2019-05-09 19:54:45,694 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code
2019-05-09 19:54:45,694 - INFO - allennlp.nn.initializers -    _answer_ability_predictor._linear_layers.0.bias
2019-05-09 19:54:45,694 - INFO - allennlp.nn.initializers -    _answer_ability_predictor._linear_layers.0.weight
2019-05-09 19:54:45,694 - INFO - allennlp.nn.initializers -    _answer_ability_predictor._linear_layers.1.bias
2019-05-09 19:54:45,694 - INFO - allennlp.nn.initializers -    _answer_ability_predictor._linear_layers.1.weight
2019-05-09 19:54:45,694 - INFO - allennlp.nn.initializers -    _count_number_predictor._linear_layers.0.bias
2019-05-09 19:54:45,694 - INFO - allennlp.nn.initializers -    _count_number_predictor._linear_layers.0.weight
2019-05-09 19:54:45,694 - INFO - allennlp.nn.initializers -    _count_number_predictor._linear_layers.1.bias
2019-05-09 19:54:45,694 - INFO - allennlp.nn.initializers -    _count_number_predictor._linear_layers.1.weight
2019-05-09 19:54:45,695 - INFO - allennlp.nn.initializers -    _embedding_proj_layer.bias
2019-05-09 19:54:45,695 - INFO - allennlp.nn.initializers -    _embedding_proj_layer.weight
2019-05-09 19:54:45,695 - INFO - allennlp.nn.initializers -    _encoding_proj_layer.bias
2019-05-09 19:54:45,695 - INFO - allennlp.nn.initializers -    _encoding_proj_layer.weight
2019-05-09 19:54:45,695 - INFO - allennlp.nn.initializers -    _highway_layer._layers.0.bias
2019-05-09 19:54:45,695 - INFO - allennlp.nn.initializers -    _highway_layer._layers.0.weight
2019-05-09 19:54:45,695 - INFO - allennlp.nn.initializers -    _highway_layer._layers.1.bias
2019-05-09 19:54:45,695 - INFO - allennlp.nn.initializers -    _highway_layer._layers.1.weight
2019-05-09 19:54:45,695 - INFO - allennlp.nn.initializers -    _matrix_attention._bias
2019-05-09 19:54:45,695 - INFO - allennlp.nn.initializers -    _matrix_attention._weight_vector
2019-05-09 19:54:45,695 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_layers.0.1.bias
2019-05-09 19:54:45,695 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_layers.0.1.weight
2019-05-09 19:54:45,695 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_layers.0.2.bias
2019-05-09 19:54:45,695 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_layers.0.2.weight
2019-05-09 19:54:45,695 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_layers.1.1.bias
2019-05-09 19:54:45,695 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_layers.1.1.weight
2019-05-09 19:54:45,696 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_layers.1.2.bias
2019-05-09 19:54:45,696 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_layers.1.2.weight
2019-05-09 19:54:45,696 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_norm_layers.0.bias
2019-05-09 19:54:45,696 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_norm_layers.0.weight
2019-05-09 19:54:45,696 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_norm_layers.1.bias
2019-05-09 19:54:45,696 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0._conv_norm_layers.1.weight
2019-05-09 19:54:45,696 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.attention_layer._combined_projection.bias
2019-05-09 19:54:45,696 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.attention_layer._combined_projection.weight
2019-05-09 19:54:45,696 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.attention_layer._output_projection.bias
2019-05-09 19:54:45,696 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.attention_layer._output_projection.weight
2019-05-09 19:54:45,696 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.attention_norm_layer.bias
2019-05-09 19:54:45,696 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.attention_norm_layer.weight
2019-05-09 19:54:45,696 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.feedforward._linear_layers.0.bias
2019-05-09 19:54:45,696 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.feedforward._linear_layers.0.weight
2019-05-09 19:54:45,696 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.feedforward._linear_layers.1.bias
2019-05-09 19:54:45,696 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.feedforward._linear_layers.1.weight
2019-05-09 19:54:45,697 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.feedforward_norm_layer.bias
2019-05-09 19:54:45,697 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_0.feedforward_norm_layer.weight
2019-05-09 19:54:45,697 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_layers.0.1.bias
2019-05-09 19:54:45,697 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_layers.0.1.weight
2019-05-09 19:54:45,697 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_layers.0.2.bias
2019-05-09 19:54:45,697 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_layers.0.2.weight
2019-05-09 19:54:45,697 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_layers.1.1.bias
2019-05-09 19:54:45,697 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_layers.1.1.weight
2019-05-09 19:54:45,697 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_layers.1.2.bias
2019-05-09 19:54:45,697 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_layers.1.2.weight
2019-05-09 19:54:45,697 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_norm_layers.0.bias
2019-05-09 19:54:45,697 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_norm_layers.0.weight
2019-05-09 19:54:45,697 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_norm_layers.1.bias
2019-05-09 19:54:45,697 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1._conv_norm_layers.1.weight
2019-05-09 19:54:45,697 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.attention_layer._combined_projection.bias
2019-05-09 19:54:45,697 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.attention_layer._combined_projection.weight
2019-05-09 19:54:45,697 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.attention_layer._output_projection.bias
2019-05-09 19:54:45,698 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.attention_layer._output_projection.weight
2019-05-09 19:54:45,698 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.attention_norm_layer.bias
2019-05-09 19:54:45,698 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.attention_norm_layer.weight
2019-05-09 19:54:45,698 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.feedforward._linear_layers.0.bias
2019-05-09 19:54:45,698 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.feedforward._linear_layers.0.weight
2019-05-09 19:54:45,698 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.feedforward._linear_layers.1.bias
2019-05-09 19:54:45,698 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.feedforward._linear_layers.1.weight
2019-05-09 19:54:45,698 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.feedforward_norm_layer.bias
2019-05-09 19:54:45,698 - INFO - allennlp.nn.initializers -    _modeling_layer.encoder_block_1.feedforward_norm_layer.weight
2019-05-09 19:54:45,698 - INFO - allennlp.nn.initializers -    _modeling_proj_layer.bias
2019-05-09 19:54:45,698 - INFO - allennlp.nn.initializers -    _modeling_proj_layer.weight
2019-05-09 19:54:45,698 - INFO - allennlp.nn.initializers -    _number_sign_predictor._linear_layers.0.bias
2019-05-09 19:54:45,698 - INFO - allennlp.nn.initializers -    _number_sign_predictor._linear_layers.0.weight
2019-05-09 19:54:45,698 - INFO - allennlp.nn.initializers -    _number_sign_predictor._linear_layers.1.bias
2019-05-09 19:54:45,698 - INFO - allennlp.nn.initializers -    _number_sign_predictor._linear_layers.1.weight
2019-05-09 19:54:45,698 - INFO - allennlp.nn.initializers -    _passage_span_end_predictor._linear_layers.0.bias
2019-05-09 19:54:45,699 - INFO - allennlp.nn.initializers -    _passage_span_end_predictor._linear_layers.0.weight
2019-05-09 19:54:45,699 - INFO - allennlp.nn.initializers -    _passage_span_end_predictor._linear_layers.1.bias
2019-05-09 19:54:45,699 - INFO - allennlp.nn.initializers -    _passage_span_end_predictor._linear_layers.1.weight
2019-05-09 19:54:45,699 - INFO - allennlp.nn.initializers -    _passage_span_start_predictor._linear_layers.0.bias
2019-05-09 19:54:45,699 - INFO - allennlp.nn.initializers -    _passage_span_start_predictor._linear_layers.0.weight
2019-05-09 19:54:45,699 - INFO - allennlp.nn.initializers -    _passage_span_start_predictor._linear_layers.1.bias
2019-05-09 19:54:45,699 - INFO - allennlp.nn.initializers -    _passage_span_start_predictor._linear_layers.1.weight
2019-05-09 19:54:45,699 - INFO - allennlp.nn.initializers -    _passage_weights_predictor.bias
2019-05-09 19:54:45,699 - INFO - allennlp.nn.initializers -    _passage_weights_predictor.weight
2019-05-09 19:54:45,699 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.0.1.bias
2019-05-09 19:54:45,699 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.0.1.weight
2019-05-09 19:54:45,699 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.0.2.bias
2019-05-09 19:54:45,699 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.0.2.weight
2019-05-09 19:54:45,699 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.1.1.bias
2019-05-09 19:54:45,699 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.1.1.weight
2019-05-09 19:54:45,699 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.1.2.bias
2019-05-09 19:54:45,699 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.1.2.weight
2019-05-09 19:54:45,700 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.2.1.bias
2019-05-09 19:54:45,700 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.2.1.weight
2019-05-09 19:54:45,700 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.2.2.bias
2019-05-09 19:54:45,700 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.2.2.weight
2019-05-09 19:54:45,700 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.3.1.bias
2019-05-09 19:54:45,700 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.3.1.weight
2019-05-09 19:54:45,700 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.3.2.bias
2019-05-09 19:54:45,700 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_layers.3.2.weight
2019-05-09 19:54:45,700 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_norm_layers.0.bias
2019-05-09 19:54:45,700 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_norm_layers.0.weight
2019-05-09 19:54:45,700 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_norm_layers.1.bias
2019-05-09 19:54:45,700 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_norm_layers.1.weight
2019-05-09 19:54:45,700 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_norm_layers.2.bias
2019-05-09 19:54:45,700 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_norm_layers.2.weight
2019-05-09 19:54:45,700 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_norm_layers.3.bias
2019-05-09 19:54:45,700 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0._conv_norm_layers.3.weight
2019-05-09 19:54:45,701 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.attention_layer._combined_projection.bias
2019-05-09 19:54:45,701 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.attention_layer._combined_projection.weight
2019-05-09 19:54:45,701 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.attention_layer._output_projection.bias
2019-05-09 19:54:45,701 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.attention_layer._output_projection.weight
2019-05-09 19:54:45,701 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.attention_norm_layer.bias
2019-05-09 19:54:45,701 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.attention_norm_layer.weight
2019-05-09 19:54:45,701 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.feedforward._linear_layers.0.bias
2019-05-09 19:54:45,701 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.feedforward._linear_layers.0.weight
2019-05-09 19:54:45,701 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.feedforward._linear_layers.1.bias
2019-05-09 19:54:45,701 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.feedforward._linear_layers.1.weight
2019-05-09 19:54:45,701 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.feedforward_norm_layer.bias
2019-05-09 19:54:45,701 - INFO - allennlp.nn.initializers -    _phrase_layer.encoder_block_0.feedforward_norm_layer.weight
2019-05-09 19:54:45,701 - INFO - allennlp.nn.initializers -    _question_span_end_predictor._linear_layers.0.bias
2019-05-09 19:54:45,701 - INFO - allennlp.nn.initializers -    _question_span_end_predictor._linear_layers.0.weight
2019-05-09 19:54:45,701 - INFO - allennlp.nn.initializers -    _question_span_end_predictor._linear_layers.1.bias
2019-05-09 19:54:45,701 - INFO - allennlp.nn.initializers -    _question_span_end_predictor._linear_layers.1.weight
2019-05-09 19:54:45,702 - INFO - allennlp.nn.initializers -    _question_span_start_predictor._linear_layers.0.bias
2019-05-09 19:54:45,702 - INFO - allennlp.nn.initializers -    _question_span_start_predictor._linear_layers.0.weight
2019-05-09 19:54:45,702 - INFO - allennlp.nn.initializers -    _question_span_start_predictor._linear_layers.1.bias
2019-05-09 19:54:45,702 - INFO - allennlp.nn.initializers -    _question_span_start_predictor._linear_layers.1.weight
2019-05-09 19:54:45,702 - INFO - allennlp.nn.initializers -    _question_weights_predictor.bias
2019-05-09 19:54:45,702 - INFO - allennlp.nn.initializers -    _question_weights_predictor.weight
2019-05-09 19:54:45,702 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.bias
2019-05-09 19:54:45,702 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.weight
2019-05-09 19:54:45,702 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.embeddings.position_embeddings.weight
2019-05-09 19:54:45,702 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.embeddings.token_type_embeddings.weight
2019-05-09 19:54:45,702 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.embeddings.word_embeddings.weight
2019-05-09 19:54:45,702 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.bias
2019-05-09 19:54:45,702 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.weight
2019-05-09 19:54:45,702 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.bias
2019-05-09 19:54:45,702 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.weight
2019-05-09 19:54:45,702 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.bias
2019-05-09 19:54:45,702 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.weight
2019-05-09 19:54:45,703 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.bias
2019-05-09 19:54:45,703 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.weight
2019-05-09 19:54:45,703 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.bias
2019-05-09 19:54:45,703 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.weight
2019-05-09 19:54:45,703 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.bias
2019-05-09 19:54:45,703 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.weight
2019-05-09 19:54:45,703 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.bias
2019-05-09 19:54:45,703 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.weight
2019-05-09 19:54:45,703 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.bias
2019-05-09 19:54:45,703 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.weight
2019-05-09 19:54:45,703 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.bias
2019-05-09 19:54:45,703 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.weight
2019-05-09 19:54:45,703 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.bias
2019-05-09 19:54:45,703 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.weight
2019-05-09 19:54:45,703 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.bias
2019-05-09 19:54:45,703 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.weight
2019-05-09 19:54:45,704 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.bias
2019-05-09 19:54:45,704 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.weight
2019-05-09 19:54:45,704 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.bias
2019-05-09 19:54:45,704 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.weight
2019-05-09 19:54:45,704 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.bias
2019-05-09 19:54:45,704 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.weight
2019-05-09 19:54:45,704 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.bias
2019-05-09 19:54:45,704 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.weight
2019-05-09 19:54:45,704 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.bias
2019-05-09 19:54:45,704 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.weight
2019-05-09 19:54:45,704 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.bias
2019-05-09 19:54:45,704 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.weight
2019-05-09 19:54:45,704 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.bias
2019-05-09 19:54:45,704 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.weight
2019-05-09 19:54:45,704 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.bias
2019-05-09 19:54:45,704 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.weight
2019-05-09 19:54:45,704 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.bias
2019-05-09 19:54:45,705 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.weight
2019-05-09 19:54:45,705 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.bias
2019-05-09 19:54:45,705 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.weight
2019-05-09 19:54:45,705 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.bias
2019-05-09 19:54:45,705 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.weight
2019-05-09 19:54:45,705 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.bias
2019-05-09 19:54:45,705 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.weight
2019-05-09 19:54:45,705 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.bias
2019-05-09 19:54:45,705 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.weight
2019-05-09 19:54:45,705 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.bias
2019-05-09 19:54:45,705 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.weight
2019-05-09 19:54:45,705 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.bias
2019-05-09 19:54:45,705 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.weight
2019-05-09 19:54:45,705 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.bias
2019-05-09 19:54:45,705 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.weight
2019-05-09 19:54:45,705 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.bias
2019-05-09 19:54:45,706 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.weight
2019-05-09 19:54:45,706 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.bias
2019-05-09 19:54:45,706 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.weight
2019-05-09 19:54:45,706 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.bias
2019-05-09 19:54:45,706 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.weight
2019-05-09 19:54:45,706 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.bias
2019-05-09 19:54:45,706 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.weight
2019-05-09 19:54:45,706 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.bias
2019-05-09 19:54:45,706 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.weight
2019-05-09 19:54:45,706 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.bias
2019-05-09 19:54:45,706 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.weight
2019-05-09 19:54:45,706 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.bias
2019-05-09 19:54:45,706 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.weight
2019-05-09 19:54:45,706 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.bias
2019-05-09 19:54:45,706 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.weight
2019-05-09 19:54:45,706 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.bias
2019-05-09 19:54:45,707 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.weight
2019-05-09 19:54:45,707 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.bias
2019-05-09 19:54:45,707 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.weight
2019-05-09 19:54:45,707 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.bias
2019-05-09 19:54:45,707 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.weight
2019-05-09 19:54:45,707 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.bias
2019-05-09 19:54:45,707 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.weight
2019-05-09 19:54:45,707 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.bias
2019-05-09 19:54:45,707 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.weight
2019-05-09 19:54:45,707 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.bias
2019-05-09 19:54:45,707 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.weight
2019-05-09 19:54:45,707 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.bias
2019-05-09 19:54:45,707 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.weight
2019-05-09 19:54:45,707 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.bias
2019-05-09 19:54:45,707 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.weight
2019-05-09 19:54:45,707 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.bias
2019-05-09 19:54:45,707 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.weight
2019-05-09 19:54:45,708 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.bias
2019-05-09 19:54:45,708 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.weight
2019-05-09 19:54:45,708 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.bias
2019-05-09 19:54:45,708 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.weight
2019-05-09 19:54:45,708 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.bias
2019-05-09 19:54:45,708 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.weight
2019-05-09 19:54:45,708 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.bias
2019-05-09 19:54:45,708 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.weight
2019-05-09 19:54:45,708 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.bias
2019-05-09 19:54:45,708 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.weight
2019-05-09 19:54:45,708 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.bias
2019-05-09 19:54:45,708 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.weight
2019-05-09 19:54:45,708 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.bias
2019-05-09 19:54:45,708 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.weight
2019-05-09 19:54:45,708 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.bias
2019-05-09 19:54:45,708 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.weight
2019-05-09 19:54:45,709 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.bias
2019-05-09 19:54:45,709 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.weight
2019-05-09 19:54:45,709 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.bias
2019-05-09 19:54:45,709 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.weight
2019-05-09 19:54:45,709 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.bias
2019-05-09 19:54:45,709 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.weight
2019-05-09 19:54:45,709 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.bias
2019-05-09 19:54:45,709 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.weight
2019-05-09 19:54:45,709 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.bias
2019-05-09 19:54:45,709 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.weight
2019-05-09 19:54:45,709 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.bias
2019-05-09 19:54:45,709 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.weight
2019-05-09 19:54:45,709 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.bias
2019-05-09 19:54:45,709 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.weight
2019-05-09 19:54:45,709 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.bias
2019-05-09 19:54:45,709 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.weight
2019-05-09 19:54:45,709 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.bias
2019-05-09 19:54:45,710 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.weight
2019-05-09 19:54:45,710 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.bias
2019-05-09 19:54:45,710 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.weight
2019-05-09 19:54:45,710 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.bias
2019-05-09 19:54:45,710 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.weight
2019-05-09 19:54:45,710 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.bias
2019-05-09 19:54:45,710 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.weight
2019-05-09 19:54:45,710 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.bias
2019-05-09 19:54:45,710 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.weight
2019-05-09 19:54:45,710 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.bias
2019-05-09 19:54:45,710 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.weight
2019-05-09 19:54:45,710 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.bias
2019-05-09 19:54:45,710 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.weight
2019-05-09 19:54:45,710 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.bias
2019-05-09 19:54:45,710 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.weight
2019-05-09 19:54:45,710 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.bias
2019-05-09 19:54:45,711 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.weight
2019-05-09 19:54:45,711 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.bias
2019-05-09 19:54:45,711 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.weight
2019-05-09 19:54:45,711 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.bias
2019-05-09 19:54:45,711 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.weight
2019-05-09 19:54:45,711 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.bias
2019-05-09 19:54:45,711 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.weight
2019-05-09 19:54:45,711 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.bias
2019-05-09 19:54:45,711 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.weight
2019-05-09 19:54:45,711 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.bias
2019-05-09 19:54:45,711 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.weight
2019-05-09 19:54:45,711 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.bias
2019-05-09 19:54:45,711 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.weight
2019-05-09 19:54:45,711 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.bias
2019-05-09 19:54:45,711 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.weight
2019-05-09 19:54:45,711 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.bias
2019-05-09 19:54:45,712 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.weight
2019-05-09 19:54:45,712 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.bias
2019-05-09 19:54:45,712 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.weight
2019-05-09 19:54:45,712 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.bias
2019-05-09 19:54:45,712 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.weight
2019-05-09 19:54:45,712 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.bias
2019-05-09 19:54:45,712 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.weight
2019-05-09 19:54:45,712 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.bias
2019-05-09 19:54:45,712 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.weight
2019-05-09 19:54:45,712 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.bias
2019-05-09 19:54:45,712 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.weight
2019-05-09 19:54:45,712 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.bias
2019-05-09 19:54:45,712 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.weight
2019-05-09 19:54:45,712 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.bias
2019-05-09 19:54:45,712 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.weight
2019-05-09 19:54:45,712 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.bias
2019-05-09 19:54:45,712 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.weight
2019-05-09 19:54:45,713 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.bias
2019-05-09 19:54:45,713 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.weight
2019-05-09 19:54:45,713 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.bias
2019-05-09 19:54:45,713 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.weight
2019-05-09 19:54:45,713 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.bias
2019-05-09 19:54:45,713 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.weight
2019-05-09 19:54:45,713 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.bias
2019-05-09 19:54:45,713 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.weight
2019-05-09 19:54:45,713 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.bias
2019-05-09 19:54:45,713 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.weight
2019-05-09 19:54:45,713 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.bias
2019-05-09 19:54:45,713 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.weight
2019-05-09 19:54:45,713 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.bias
2019-05-09 19:54:45,713 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.weight
2019-05-09 19:54:45,713 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.bias
2019-05-09 19:54:45,713 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.weight
2019-05-09 19:54:45,714 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.bias
2019-05-09 19:54:45,714 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.weight
2019-05-09 19:54:45,714 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.bias
2019-05-09 19:54:45,714 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.weight
2019-05-09 19:54:45,714 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.bias
2019-05-09 19:54:45,714 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.weight
2019-05-09 19:54:45,714 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.pooler.dense.bias
2019-05-09 19:54:45,714 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_bert.bert_model.pooler.dense.weight
2019-05-09 19:54:45,714 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_token_characters._embedding._module.weight
2019-05-09 19:54:45,714 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias
2019-05-09 19:54:45,714 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight
2019-05-09 19:54:45,714 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.weight
2019-05-09 19:54:45,844 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.data_iterator.DataIterator'> from params {'batch_size': 16, 'max_instances_in_memory': 600, 'sorting_keys': [['passage', 'num_tokens'], ['question', 'num_tokens']], 'type': 'bucket'} and extras set()
2019-05-09 19:54:45,844 - INFO - allennlp.common.params - iterator.type = bucket
2019-05-09 19:54:45,844 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.data.iterators.bucket_iterator.BucketIterator'> from params {'batch_size': 16, 'max_instances_in_memory': 600, 'sorting_keys': [['passage', 'num_tokens'], ['question', 'num_tokens']]} and extras set()
2019-05-09 19:54:45,845 - INFO - allennlp.common.params - iterator.sorting_keys = [['passage', 'num_tokens'], ['question', 'num_tokens']]
2019-05-09 19:54:45,845 - INFO - allennlp.common.params - iterator.padding_noise = 0.1
2019-05-09 19:54:45,845 - INFO - allennlp.common.params - iterator.biggest_batch_first = False
2019-05-09 19:54:45,845 - INFO - allennlp.common.params - iterator.batch_size = 16
2019-05-09 19:54:45,845 - INFO - allennlp.common.params - iterator.instances_per_epoch = None
2019-05-09 19:54:45,845 - INFO - allennlp.common.params - iterator.max_instances_in_memory = 600
2019-05-09 19:54:45,845 - INFO - allennlp.common.params - iterator.cache_instances = False
2019-05-09 19:54:45,845 - INFO - allennlp.common.params - iterator.track_epoch = False
2019-05-09 19:54:45,845 - INFO - allennlp.common.params - iterator.maximum_samples_per_batch = None
2019-05-09 19:54:45,845 - INFO - allennlp.common.params - validation_iterator = None
2019-05-09 19:54:45,845 - INFO - allennlp.common.params - trainer.no_grad = ()
2019-05-09 19:54:45,849 - INFO - allennlp.training.trainer - Following parameters are Frozen  (without gradient):
2019-05-09 19:54:45,854 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.embeddings.word_embeddings.weight
2019-05-09 19:54:45,854 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.embeddings.position_embeddings.weight
2019-05-09 19:54:45,854 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.embeddings.token_type_embeddings.weight
2019-05-09 19:54:45,854 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.weight
2019-05-09 19:54:45,854 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.bias
2019-05-09 19:54:45,854 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.weight
2019-05-09 19:54:45,854 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.bias
2019-05-09 19:54:45,854 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.weight
2019-05-09 19:54:45,855 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.bias
2019-05-09 19:54:45,855 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.weight
2019-05-09 19:54:45,855 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.bias
2019-05-09 19:54:45,855 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.weight
2019-05-09 19:54:45,855 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.bias
2019-05-09 19:54:45,855 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.weight
2019-05-09 19:54:45,855 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.bias
2019-05-09 19:54:45,855 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.weight
2019-05-09 19:54:45,855 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.bias
2019-05-09 19:54:45,855 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.weight
2019-05-09 19:54:45,855 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.bias
2019-05-09 19:54:45,855 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.weight
2019-05-09 19:54:45,856 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.bias
2019-05-09 19:54:45,856 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.weight
2019-05-09 19:54:45,856 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.bias
2019-05-09 19:54:45,856 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.weight
2019-05-09 19:54:45,856 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.bias
2019-05-09 19:54:45,856 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.weight
2019-05-09 19:54:45,856 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.bias
2019-05-09 19:54:45,856 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.weight
2019-05-09 19:54:45,856 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.bias
2019-05-09 19:54:45,856 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.weight
2019-05-09 19:54:45,856 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.bias
2019-05-09 19:54:45,856 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.weight
2019-05-09 19:54:45,857 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.bias
2019-05-09 19:54:45,857 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.weight
2019-05-09 19:54:45,857 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.bias
2019-05-09 19:54:45,857 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.weight
2019-05-09 19:54:45,859 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.bias
2019-05-09 19:54:45,860 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.weight
2019-05-09 19:54:45,860 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.bias
2019-05-09 19:54:45,860 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.weight
2019-05-09 19:54:45,860 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.bias
2019-05-09 19:54:45,860 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.weight
2019-05-09 19:54:45,860 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.bias
2019-05-09 19:54:45,860 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.weight
2019-05-09 19:54:45,860 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.bias
2019-05-09 19:54:45,860 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.weight
2019-05-09 19:54:45,860 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.bias
2019-05-09 19:54:45,860 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.weight
2019-05-09 19:54:45,860 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.bias
2019-05-09 19:54:45,861 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.weight
2019-05-09 19:54:45,861 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.bias
2019-05-09 19:54:45,861 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.weight
2019-05-09 19:54:45,861 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.bias
2019-05-09 19:54:45,861 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.weight
2019-05-09 19:54:45,861 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.bias
2019-05-09 19:54:45,861 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.weight
2019-05-09 19:54:45,861 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.bias
2019-05-09 19:54:45,861 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.weight
2019-05-09 19:54:45,861 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.bias
2019-05-09 19:54:45,861 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.weight
2019-05-09 19:54:45,861 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.bias
2019-05-09 19:54:45,861 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.weight
2019-05-09 19:54:45,862 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.bias
2019-05-09 19:54:45,862 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.weight
2019-05-09 19:54:45,862 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.bias
2019-05-09 19:54:45,864 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.weight
2019-05-09 19:54:45,865 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.bias
2019-05-09 19:54:45,865 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.weight
2019-05-09 19:54:45,865 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.bias
2019-05-09 19:54:45,865 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.weight
2019-05-09 19:54:45,865 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.bias
2019-05-09 19:54:45,865 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.weight
2019-05-09 19:54:45,865 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.bias
2019-05-09 19:54:45,865 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.weight
2019-05-09 19:54:45,865 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.bias
2019-05-09 19:54:45,865 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.weight
2019-05-09 19:54:45,865 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.bias
2019-05-09 19:54:45,866 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.weight
2019-05-09 19:54:45,866 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.bias
2019-05-09 19:54:45,866 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.weight
2019-05-09 19:54:45,866 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.bias
2019-05-09 19:54:45,866 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.weight
2019-05-09 19:54:45,866 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.bias
2019-05-09 19:54:45,866 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.weight
2019-05-09 19:54:45,866 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.bias
2019-05-09 19:54:45,866 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.weight
2019-05-09 19:54:45,866 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.bias
2019-05-09 19:54:45,866 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.weight
2019-05-09 19:54:45,866 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.bias
2019-05-09 19:54:45,869 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.weight
2019-05-09 19:54:45,869 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.bias
2019-05-09 19:54:45,869 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.weight
2019-05-09 19:54:45,869 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.bias
2019-05-09 19:54:45,869 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.weight
2019-05-09 19:54:45,869 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.bias
2019-05-09 19:54:45,869 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.weight
2019-05-09 19:54:45,870 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.bias
2019-05-09 19:54:45,870 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.weight
2019-05-09 19:54:45,870 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.bias
2019-05-09 19:54:45,870 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.weight
2019-05-09 19:54:45,870 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.bias
2019-05-09 19:54:45,870 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.weight
2019-05-09 19:54:45,870 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.bias
2019-05-09 19:54:45,870 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.weight
2019-05-09 19:54:45,870 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.bias
2019-05-09 19:54:45,870 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.weight
2019-05-09 19:54:45,870 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.bias
2019-05-09 19:54:45,871 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.weight
2019-05-09 19:54:45,871 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.bias
2019-05-09 19:54:45,871 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.weight
2019-05-09 19:54:45,871 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.bias
2019-05-09 19:54:45,873 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.weight
2019-05-09 19:54:45,873 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.bias
2019-05-09 19:54:45,873 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.weight
2019-05-09 19:54:45,873 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.bias
2019-05-09 19:54:45,874 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.weight
2019-05-09 19:54:45,874 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.bias
2019-05-09 19:54:45,874 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.weight
2019-05-09 19:54:45,874 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.bias
2019-05-09 19:54:45,874 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.weight
2019-05-09 19:54:45,874 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.bias
2019-05-09 19:54:45,874 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.weight
2019-05-09 19:54:45,874 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.bias
2019-05-09 19:54:45,874 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.weight
2019-05-09 19:54:45,874 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.bias
2019-05-09 19:54:45,874 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.weight
2019-05-09 19:54:45,875 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.bias
2019-05-09 19:54:45,875 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.weight
2019-05-09 19:54:45,875 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.bias
2019-05-09 19:54:45,875 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.weight
2019-05-09 19:54:45,875 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.bias
2019-05-09 19:54:45,875 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.weight
2019-05-09 19:54:45,877 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.bias
2019-05-09 19:54:45,877 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.weight
2019-05-09 19:54:45,877 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.bias
2019-05-09 19:54:45,877 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.weight
2019-05-09 19:54:45,877 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.bias
2019-05-09 19:54:45,878 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.weight
2019-05-09 19:54:45,878 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.bias
2019-05-09 19:54:45,878 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.weight
2019-05-09 19:54:45,878 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.bias
2019-05-09 19:54:45,878 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.weight
2019-05-09 19:54:45,878 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.bias
2019-05-09 19:54:45,878 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.weight
2019-05-09 19:54:45,878 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.bias
2019-05-09 19:54:45,878 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.weight
2019-05-09 19:54:45,878 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.bias
2019-05-09 19:54:45,878 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.weight
2019-05-09 19:54:45,879 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.bias
2019-05-09 19:54:45,879 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.weight
2019-05-09 19:54:45,879 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.bias
2019-05-09 19:54:45,879 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.weight
2019-05-09 19:54:45,881 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.bias
2019-05-09 19:54:45,881 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.weight
2019-05-09 19:54:45,881 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.bias
2019-05-09 19:54:45,881 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.weight
2019-05-09 19:54:45,881 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.bias
2019-05-09 19:54:45,882 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.weight
2019-05-09 19:54:45,882 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.bias
2019-05-09 19:54:45,882 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.weight
2019-05-09 19:54:45,882 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.bias
2019-05-09 19:54:45,882 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.weight
2019-05-09 19:54:45,882 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.bias
2019-05-09 19:54:45,882 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.weight
2019-05-09 19:54:45,882 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.bias
2019-05-09 19:54:45,882 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.weight
2019-05-09 19:54:45,883 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.bias
2019-05-09 19:54:45,883 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.weight
2019-05-09 19:54:45,885 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.bias
2019-05-09 19:54:45,885 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.weight
2019-05-09 19:54:45,885 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.bias
2019-05-09 19:54:45,885 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.weight
2019-05-09 19:54:45,886 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.bias
2019-05-09 19:54:45,886 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.weight
2019-05-09 19:54:45,886 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.bias
2019-05-09 19:54:45,886 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.weight
2019-05-09 19:54:45,886 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.bias
2019-05-09 19:54:45,886 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.weight
2019-05-09 19:54:45,886 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.bias
2019-05-09 19:54:45,887 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.weight
2019-05-09 19:54:45,887 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.bias
2019-05-09 19:54:45,889 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.weight
2019-05-09 19:54:45,889 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.bias
2019-05-09 19:54:45,889 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.weight
2019-05-09 19:54:45,889 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.bias
2019-05-09 19:54:45,889 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.weight
2019-05-09 19:54:45,890 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.bias
2019-05-09 19:54:45,890 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.weight
2019-05-09 19:54:45,890 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.bias
2019-05-09 19:54:45,890 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.weight
2019-05-09 19:54:45,890 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.bias
2019-05-09 19:54:45,890 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.weight
2019-05-09 19:54:45,890 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.bias
2019-05-09 19:54:45,891 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.weight
2019-05-09 19:54:45,893 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.bias
2019-05-09 19:54:45,893 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.weight
2019-05-09 19:54:45,893 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.bias
2019-05-09 19:54:45,893 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.pooler.dense.weight
2019-05-09 19:54:45,893 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_bert.bert_model.pooler.dense.bias
2019-05-09 19:54:45,893 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_tokens.weight
2019-05-09 19:54:45,894 - INFO - allennlp.training.trainer - Following parameters are Tunable (with gradient):
2019-05-09 19:54:45,894 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_token_characters._embedding._module.weight
2019-05-09 19:54:45,894 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.weight
2019-05-09 19:54:45,894 - INFO - allennlp.training.trainer - _text_field_embedder.token_embedder_token_characters._encoder._module.conv_layer_0.bias
2019-05-09 19:54:45,894 - INFO - allennlp.training.trainer - _embedding_proj_layer.weight
2019-05-09 19:54:45,894 - INFO - allennlp.training.trainer - _embedding_proj_layer.bias
2019-05-09 19:54:45,894 - INFO - allennlp.training.trainer - _highway_layer._layers.0.weight
2019-05-09 19:54:45,894 - INFO - allennlp.training.trainer - _highway_layer._layers.0.bias
2019-05-09 19:54:45,894 - INFO - allennlp.training.trainer - _highway_layer._layers.1.weight
2019-05-09 19:54:45,894 - INFO - allennlp.training.trainer - _highway_layer._layers.1.bias
2019-05-09 19:54:45,896 - INFO - allennlp.training.trainer - _encoding_proj_layer.weight
2019-05-09 19:54:45,897 - INFO - allennlp.training.trainer - _encoding_proj_layer.bias
2019-05-09 19:54:45,897 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_norm_layers.0.weight
2019-05-09 19:54:45,897 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_norm_layers.0.bias
2019-05-09 19:54:45,897 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_norm_layers.1.weight
2019-05-09 19:54:45,897 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_norm_layers.1.bias
2019-05-09 19:54:45,897 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_norm_layers.2.weight
2019-05-09 19:54:45,897 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_norm_layers.2.bias
2019-05-09 19:54:45,897 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_norm_layers.3.weight
2019-05-09 19:54:45,897 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_norm_layers.3.bias
2019-05-09 19:54:45,897 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.0.1.weight
2019-05-09 19:54:45,898 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.0.1.bias
2019-05-09 19:54:45,898 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.0.2.weight
2019-05-09 19:54:45,898 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.0.2.bias
2019-05-09 19:54:45,898 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.1.1.weight
2019-05-09 19:54:45,898 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.1.1.bias
2019-05-09 19:54:45,898 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.1.2.weight
2019-05-09 19:54:45,898 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.1.2.bias
2019-05-09 19:54:45,898 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.2.1.weight
2019-05-09 19:54:45,898 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.2.1.bias
2019-05-09 19:54:45,898 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.2.2.weight
2019-05-09 19:54:45,901 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.2.2.bias
2019-05-09 19:54:45,901 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.3.1.weight
2019-05-09 19:54:45,901 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.3.1.bias
2019-05-09 19:54:45,901 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.3.2.weight
2019-05-09 19:54:45,901 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0._conv_layers.3.2.bias
2019-05-09 19:54:45,901 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.attention_norm_layer.weight
2019-05-09 19:54:45,901 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.attention_norm_layer.bias
2019-05-09 19:54:45,901 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.attention_layer._combined_projection.weight
2019-05-09 19:54:45,901 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.attention_layer._combined_projection.bias
2019-05-09 19:54:45,901 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.attention_layer._output_projection.weight
2019-05-09 19:54:45,902 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.attention_layer._output_projection.bias
2019-05-09 19:54:45,902 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.feedforward_norm_layer.weight
2019-05-09 19:54:45,902 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.feedforward_norm_layer.bias
2019-05-09 19:54:45,902 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.feedforward._linear_layers.0.weight
2019-05-09 19:54:45,902 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.feedforward._linear_layers.0.bias
2019-05-09 19:54:45,902 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.feedforward._linear_layers.1.weight
2019-05-09 19:54:45,902 - INFO - allennlp.training.trainer - _phrase_layer.encoder_block_0.feedforward._linear_layers.1.bias
2019-05-09 19:54:45,902 - INFO - allennlp.training.trainer - _matrix_attention._weight_vector
2019-05-09 19:54:45,902 - INFO - allennlp.training.trainer - _matrix_attention._bias
2019-05-09 19:54:45,902 - INFO - allennlp.training.trainer - _modeling_proj_layer.weight
2019-05-09 19:54:45,905 - INFO - allennlp.training.trainer - _modeling_proj_layer.bias
2019-05-09 19:54:45,905 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_norm_layers.0.weight
2019-05-09 19:54:45,905 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_norm_layers.0.bias
2019-05-09 19:54:45,905 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_norm_layers.1.weight
2019-05-09 19:54:45,905 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_norm_layers.1.bias
2019-05-09 19:54:45,905 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_layers.0.1.weight
2019-05-09 19:54:45,905 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_layers.0.1.bias
2019-05-09 19:54:45,905 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_layers.0.2.weight
2019-05-09 19:54:45,905 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_layers.0.2.bias
2019-05-09 19:54:45,906 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_layers.1.1.weight
2019-05-09 19:54:45,906 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_layers.1.1.bias
2019-05-09 19:54:45,906 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_layers.1.2.weight
2019-05-09 19:54:45,906 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0._conv_layers.1.2.bias
2019-05-09 19:54:45,906 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.attention_norm_layer.weight
2019-05-09 19:54:45,906 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.attention_norm_layer.bias
2019-05-09 19:54:45,906 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.attention_layer._combined_projection.weight
2019-05-09 19:54:45,906 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.attention_layer._combined_projection.bias
2019-05-09 19:54:45,906 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.attention_layer._output_projection.weight
2019-05-09 19:54:45,906 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.attention_layer._output_projection.bias
2019-05-09 19:54:45,906 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.feedforward_norm_layer.weight
2019-05-09 19:54:45,907 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.feedforward_norm_layer.bias
2019-05-09 19:54:45,909 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.feedforward._linear_layers.0.weight
2019-05-09 19:54:45,909 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.feedforward._linear_layers.0.bias
2019-05-09 19:54:45,909 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.feedforward._linear_layers.1.weight
2019-05-09 19:54:45,909 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_0.feedforward._linear_layers.1.bias
2019-05-09 19:54:45,909 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_norm_layers.0.weight
2019-05-09 19:54:45,909 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_norm_layers.0.bias
2019-05-09 19:54:45,909 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_norm_layers.1.weight
2019-05-09 19:54:45,909 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_norm_layers.1.bias
2019-05-09 19:54:45,909 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_layers.0.1.weight
2019-05-09 19:54:45,909 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_layers.0.1.bias
2019-05-09 19:54:45,910 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_layers.0.2.weight
2019-05-09 19:54:45,910 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_layers.0.2.bias
2019-05-09 19:54:45,910 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_layers.1.1.weight
2019-05-09 19:54:45,910 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_layers.1.1.bias
2019-05-09 19:54:45,910 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_layers.1.2.weight
2019-05-09 19:54:45,910 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1._conv_layers.1.2.bias
2019-05-09 19:54:45,910 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.attention_norm_layer.weight
2019-05-09 19:54:45,910 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.attention_norm_layer.bias
2019-05-09 19:54:45,910 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.attention_layer._combined_projection.weight
2019-05-09 19:54:45,910 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.attention_layer._combined_projection.bias
2019-05-09 19:54:45,910 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.attention_layer._output_projection.weight
2019-05-09 19:54:45,913 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.attention_layer._output_projection.bias
2019-05-09 19:54:45,913 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.feedforward_norm_layer.weight
2019-05-09 19:54:45,913 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.feedforward_norm_layer.bias
2019-05-09 19:54:45,913 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.feedforward._linear_layers.0.weight
2019-05-09 19:54:45,913 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.feedforward._linear_layers.0.bias
2019-05-09 19:54:45,913 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.feedforward._linear_layers.1.weight
2019-05-09 19:54:45,913 - INFO - allennlp.training.trainer - _modeling_layer.encoder_block_1.feedforward._linear_layers.1.bias
2019-05-09 19:54:45,913 - INFO - allennlp.training.trainer - _passage_weights_predictor.weight
2019-05-09 19:54:45,913 - INFO - allennlp.training.trainer - _passage_weights_predictor.bias
2019-05-09 19:54:45,913 - INFO - allennlp.training.trainer - _question_weights_predictor.weight
2019-05-09 19:54:45,914 - INFO - allennlp.training.trainer - _question_weights_predictor.bias
2019-05-09 19:54:45,914 - INFO - allennlp.training.trainer - _answer_ability_predictor._linear_layers.0.weight
2019-05-09 19:54:45,914 - INFO - allennlp.training.trainer - _answer_ability_predictor._linear_layers.0.bias
2019-05-09 19:54:45,914 - INFO - allennlp.training.trainer - _answer_ability_predictor._linear_layers.1.weight
2019-05-09 19:54:45,914 - INFO - allennlp.training.trainer - _answer_ability_predictor._linear_layers.1.bias
2019-05-09 19:54:45,914 - INFO - allennlp.training.trainer - _passage_span_start_predictor._linear_layers.0.weight
2019-05-09 19:54:45,914 - INFO - allennlp.training.trainer - _passage_span_start_predictor._linear_layers.0.bias
2019-05-09 19:54:45,914 - INFO - allennlp.training.trainer - _passage_span_start_predictor._linear_layers.1.weight
2019-05-09 19:54:45,914 - INFO - allennlp.training.trainer - _passage_span_start_predictor._linear_layers.1.bias
2019-05-09 19:54:45,914 - INFO - allennlp.training.trainer - _passage_span_end_predictor._linear_layers.0.weight
2019-05-09 19:54:45,914 - INFO - allennlp.training.trainer - _passage_span_end_predictor._linear_layers.0.bias
2019-05-09 19:54:45,915 - INFO - allennlp.training.trainer - _passage_span_end_predictor._linear_layers.1.weight
2019-05-09 19:54:45,917 - INFO - allennlp.training.trainer - _passage_span_end_predictor._linear_layers.1.bias
2019-05-09 19:54:45,917 - INFO - allennlp.training.trainer - _question_span_start_predictor._linear_layers.0.weight
2019-05-09 19:54:45,917 - INFO - allennlp.training.trainer - _question_span_start_predictor._linear_layers.0.bias
2019-05-09 19:54:45,917 - INFO - allennlp.training.trainer - _question_span_start_predictor._linear_layers.1.weight
2019-05-09 19:54:45,917 - INFO - allennlp.training.trainer - _question_span_start_predictor._linear_layers.1.bias
2019-05-09 19:54:45,917 - INFO - allennlp.training.trainer - _question_span_end_predictor._linear_layers.0.weight
2019-05-09 19:54:45,917 - INFO - allennlp.training.trainer - _question_span_end_predictor._linear_layers.0.bias
2019-05-09 19:54:45,917 - INFO - allennlp.training.trainer - _question_span_end_predictor._linear_layers.1.weight
2019-05-09 19:54:45,918 - INFO - allennlp.training.trainer - _question_span_end_predictor._linear_layers.1.bias
2019-05-09 19:54:45,918 - INFO - allennlp.training.trainer - _number_sign_predictor._linear_layers.0.weight
2019-05-09 19:54:45,918 - INFO - allennlp.training.trainer - _number_sign_predictor._linear_layers.0.bias
2019-05-09 19:54:45,918 - INFO - allennlp.training.trainer - _number_sign_predictor._linear_layers.1.weight
2019-05-09 19:54:45,918 - INFO - allennlp.training.trainer - _number_sign_predictor._linear_layers.1.bias
2019-05-09 19:54:45,918 - INFO - allennlp.training.trainer - _count_number_predictor._linear_layers.0.weight
2019-05-09 19:54:45,918 - INFO - allennlp.training.trainer - _count_number_predictor._linear_layers.0.bias
2019-05-09 19:54:45,918 - INFO - allennlp.training.trainer - _count_number_predictor._linear_layers.1.weight
2019-05-09 19:54:45,918 - INFO - allennlp.training.trainer - _count_number_predictor._linear_layers.1.bias
2019-05-09 19:54:45,918 - INFO - allennlp.common.params - trainer.patience = 10
2019-05-09 19:54:45,919 - INFO - allennlp.common.params - trainer.validation_metric = +f1
2019-05-09 19:54:45,919 - INFO - allennlp.common.params - trainer.shuffle = True
2019-05-09 19:54:45,921 - INFO - allennlp.common.params - trainer.num_epochs = 10
2019-05-09 19:54:45,921 - INFO - allennlp.common.params - trainer.cuda_device = 0
2019-05-09 19:54:45,921 - INFO - allennlp.common.params - trainer.grad_norm = 5
2019-05-09 19:54:45,921 - INFO - allennlp.common.params - trainer.grad_clipping = None
2019-05-09 19:54:45,921 - INFO - allennlp.common.params - trainer.learning_rate_scheduler = None
2019-05-09 19:54:45,921 - INFO - allennlp.common.params - trainer.momentum_scheduler = None
2019-05-09 19:54:48,043 - INFO - allennlp.common.params - trainer.optimizer.type = adam
2019-05-09 19:54:48,043 - INFO - allennlp.common.params - trainer.optimizer.parameter_groups = None
2019-05-09 19:54:48,044 - INFO - allennlp.training.optimizers - Number of trainable parameters: 1055840
2019-05-09 19:54:48,044 - INFO - allennlp.common.params - trainer.optimizer.infer_type_and_cast = True
2019-05-09 19:54:48,044 - INFO - allennlp.common.params - Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
2019-05-09 19:54:48,044 - INFO - allennlp.common.params - CURRENTLY DEFINED PARAMETERS: 
2019-05-09 19:54:48,044 - INFO - allennlp.common.params - trainer.optimizer.betas = [0.8, 0.999]
2019-05-09 19:54:48,044 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-07
2019-05-09 19:54:48,044 - INFO - allennlp.common.params - trainer.optimizer.lr = 0.0005
2019-05-09 19:54:48,044 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.training.moving_average.MovingAverage'> from params {'decay': 0.9999, 'type': 'exponential'} and extras {'parameters'}
2019-05-09 19:54:48,044 - INFO - allennlp.common.params - trainer.moving_average.type = exponential
2019-05-09 19:54:48,044 - INFO - allennlp.common.from_params - instantiating class <class 'allennlp.training.moving_average.ExponentialMovingAverage'> from params {'decay': 0.9999} and extras {'parameters'}
2019-05-09 19:54:48,045 - INFO - allennlp.common.params - trainer.moving_average.decay = 0.9999
2019-05-09 19:54:48,045 - INFO - allennlp.common.params - trainer.moving_average.numerator = 1.0
2019-05-09 19:54:48,045 - INFO - allennlp.common.params - trainer.moving_average.denominator = 10.0
2019-05-09 19:54:48,050 - INFO - allennlp.common.params - trainer.num_serialized_models_to_keep = 1
2019-05-09 19:54:48,050 - INFO - allennlp.common.params - trainer.keep_serialized_model_every_num_seconds = None
2019-05-09 19:54:48,050 - INFO - allennlp.common.params - trainer.model_save_interval = None
2019-05-09 19:54:48,050 - INFO - allennlp.common.params - trainer.summary_interval = 100
2019-05-09 19:54:48,050 - INFO - allennlp.common.params - trainer.histogram_interval = None
2019-05-09 19:54:48,050 - INFO - allennlp.common.params - trainer.should_log_parameter_statistics = True
2019-05-09 19:54:48,051 - INFO - allennlp.common.params - trainer.should_log_learning_rate = False
2019-05-09 19:54:48,051 - INFO - allennlp.common.params - trainer.log_batch_size_period = None
2019-05-09 19:54:48,059 - INFO - allennlp.training.trainer - Beginning training.
2019-05-09 19:54:48,059 - INFO - allennlp.training.trainer - Epoch 0/9
2019-05-09 19:54:48,059 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 5960.964
2019-05-09 19:54:48,178 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 1429
2019-05-09 19:54:48,185 - INFO - allennlp.training.trainer - Training
  0%|          | 0/4204 [00:00<?, ?it/s]
em: 0.0208, f1: 0.0249, loss: 7.8721 ||:   0%|          | 12/4204 [00:10<1:00:24,  1.16it/s]
em: 0.0477, f1: 0.0509, loss: 7.2172 ||:   1%|          | 28/4204 [00:20<55:56,  1.24it/s]  
em: 0.0388, f1: 0.0479, loss: 6.8982 ||:   1%|1         | 44/4204 [00:33<55:24,  1.25it/s]
em: 0.0431, f1: 0.0572, loss: 6.5944 ||:   1%|1         | 60/4204 [00:43<51:53,  1.33it/s]
em: 0.0493, f1: 0.0664, loss: 6.3334 ||:   2%|1         | 77/4204 [00:56<51:25,  1.34it/s]
em: 0.0536, f1: 0.0738, loss: 6.2292 ||:   2%|2         | 92/4204 [01:06<50:05,  1.37it/s]
em: 0.0563, f1: 0.0794, loss: 6.1564 ||:   3%|2         | 107/4204 [01:17<49:13,  1.39it/s]
em: 0.0607, f1: 0.0856, loss: 6.0366 ||:   3%|2         | 122/4204 [01:30<51:51,  1.31it/s]
em: 0.0634, f1: 0.0872, loss: 5.9691 ||:   3%|3         | 137/4204 [01:40<50:01,  1.36it/s]
em: 0.0671, f1: 0.0904, loss: 5.9073 ||:   4%|3         | 152/4204 [01:50<48:57,  1.38it/s]
em: 0.0697, f1: 0.0955, loss: 5.8637 ||:   4%|3         | 167/4204 [02:03<51:13,  1.31it/s]
em: 0.0724, f1: 0.0985, loss: 5.7878 ||:   4%|4         | 182/4204 [02:13<49:28,  1.35it/s]
em: 0.0742, f1: 0.1002, loss: 5.7279 ||:   5%|4         | 197/4204 [02:25<50:54,  1.31it/s]
em: 0.0760, f1: 0.1023, loss: 5.6582 ||:   5%|5         | 212/4204 [02:36<48:57,  1.36it/s]
em: 0.0806, f1: 0.1083, loss: 5.5848 ||:   5%|5         | 227/4204 [02:46<47:34,  1.39it/s]
em: 0.0829, f1: 0.1118, loss: 5.5497 ||:   6%|5         | 242/4204 [02:57<48:20,  1.37it/s]
em: 0.0843, f1: 0.1142, loss: 5.5039 ||:   6%|6         | 257/4204 [03:07<47:14,  1.39it/s]
em: 0.0840, f1: 0.1140, loss: 5.4842 ||:   6%|6         | 272/4204 [03:19<48:25,  1.35it/s]
em: 0.0863, f1: 0.1176, loss: 5.4513 ||:   7%|6         | 288/4204 [03:30<46:41,  1.40it/s]
em: 0.0885, f1: 0.1199, loss: 5.4271 ||:   7%|7         | 304/4204 [03:41<46:08,  1.41it/s]
em: 0.0878, f1: 0.1185, loss: 5.4080 ||:   8%|7         | 319/4204 [03:54<48:28,  1.34it/s]
em: 0.0911, f1: 0.1225, loss: 5.3692 ||:   8%|7         | 333/4204 [04:04<48:11,  1.34it/s]
em: 0.0938, f1: 0.1256, loss: 5.3601 ||:   8%|8         | 347/4204 [04:16<50:08,  1.28it/s]
em: 0.0953, f1: 0.1272, loss: 5.3363 ||:   9%|8         | 361/4204 [04:26<48:48,  1.31it/s]
em: 0.0966, f1: 0.1284, loss: 5.3175 ||:   9%|8         | 375/4204 [04:36<48:14,  1.32it/s]
em: 0.0973, f1: 0.1288, loss: 5.3064 ||:   9%|9         | 389/4204 [04:48<49:53,  1.27it/s]
em: 0.0971, f1: 0.1284, loss: 5.2869 ||:  10%|9         | 404/4204 [04:59<47:39,  1.33it/s]
em: 0.0970, f1: 0.1282, loss: 5.2677 ||:  10%|9         | 419/4204 [05:11<48:46,  1.29it/s]
em: 0.0984, f1: 0.1294, loss: 5.2507 ||:  10%|#         | 434/4204 [05:21<47:03,  1.34it/s]
em: 0.0999, f1: 0.1308, loss: 5.2193 ||:  11%|#         | 449/4204 [05:32<46:14,  1.35it/s]
em: 0.1002, f1: 0.1314, loss: 5.2176 ||:  11%|#1        | 463/4204 [05:47<51:51,  1.20it/s]
em: 0.1001, f1: 0.1320, loss: 5.2085 ||:  11%|#1        | 479/4204 [05:57<48:08,  1.29it/s]
em: 0.1009, f1: 0.1330, loss: 5.1819 ||:  12%|#1        | 495/4204 [06:10<48:50,  1.27it/s]
em: 0.1032, f1: 0.1355, loss: 5.1585 ||:  12%|#2        | 510/4204 [06:21<47:03,  1.31it/s]
em: 0.1040, f1: 0.1363, loss: 5.1396 ||:  12%|#2        | 525/4204 [06:32<46:03,  1.33it/s]
em: 0.1054, f1: 0.1378, loss: 5.1204 ||:  13%|#2        | 539/4204 [06:43<46:40,  1.31it/s]
em: 0.1068, f1: 0.1390, loss: 5.0971 ||:  13%|#3        | 554/4204 [06:53<45:32,  1.34it/s]
em: 0.1074, f1: 0.1395, loss: 5.0761 ||:  14%|#3        | 569/4204 [07:04<44:23,  1.36it/s]
em: 0.1084, f1: 0.1404, loss: 5.0587 ||:  14%|#3        | 584/4204 [07:16<45:47,  1.32it/s]
em: 0.1092, f1: 0.1413, loss: 5.0567 ||:  14%|#4        | 598/4204 [07:26<44:52,  1.34it/s]
em: 0.1101, f1: 0.1420, loss: 5.0394 ||:  15%|#4        | 612/4204 [07:38<46:12,  1.30it/s]
em: 0.1118, f1: 0.1435, loss: 5.0217 ||:  15%|#4        | 626/4204 [07:48<45:09,  1.32it/s]
em: 0.1117, f1: 0.1439, loss: 5.0110 ||:  15%|#5        | 641/4204 [07:58<43:41,  1.36it/s]
em: 0.1115, f1: 0.1436, loss: 4.9970 ||:  16%|#5        | 656/4204 [08:10<44:38,  1.32it/s]
em: 0.1118, f1: 0.1438, loss: 4.9832 ||:  16%|#5        | 670/4204 [08:20<43:55,  1.34it/s]
em: 0.1126, f1: 0.1447, loss: 4.9708 ||:  16%|#6        | 685/4204 [08:32<44:48,  1.31it/s]
em: 0.1130, f1: 0.1450, loss: 4.9582 ||:  17%|#6        | 700/4204 [08:43<43:05,  1.36it/s]
em: 0.1140, f1: 0.1461, loss: 4.9415 ||:  17%|#7        | 717/4204 [08:53<40:38,  1.43it/s]
em: 0.1152, f1: 0.1474, loss: 4.9315 ||:  17%|#7        | 734/4204 [09:06<41:55,  1.38it/s]
em: 0.1159, f1: 0.1484, loss: 4.9233 ||:  18%|#7        | 749/4204 [09:17<41:08,  1.40it/s]
em: 0.1168, f1: 0.1496, loss: 4.9100 ||:  18%|#8        | 764/4204 [09:29<42:59,  1.33it/s]
em: 0.1174, f1: 0.1501, loss: 4.8951 ||:  19%|#8        | 779/4204 [09:40<41:53,  1.36it/s]
em: 0.1187, f1: 0.1514, loss: 4.8813 ||:  19%|#8        | 794/4204 [09:50<40:45,  1.39it/s]
em: 0.1191, f1: 0.1515, loss: 4.8771 ||:  19%|#9        | 809/4204 [10:02<42:31,  1.33it/s]
em: 0.1188, f1: 0.1513, loss: 4.8682 ||:  20%|#9        | 824/4204 [10:13<41:30,  1.36it/s]
em: 0.1190, f1: 0.1515, loss: 4.8578 ||:  20%|#9        | 839/4204 [10:25<42:10,  1.33it/s]
em: 0.1199, f1: 0.1528, loss: 4.8540 ||:  20%|##        | 855/4204 [10:35<40:33,  1.38it/s]
em: 0.1203, f1: 0.1535, loss: 4.8503 ||:  21%|##        | 871/4204 [10:46<39:07,  1.42it/s]
em: 0.1197, f1: 0.1527, loss: 4.8464 ||:  21%|##1       | 887/4204 [10:59<41:06,  1.34it/s]
em: 0.1204, f1: 0.1533, loss: 4.8419 ||:  21%|##1       | 901/4204 [11:09<40:36,  1.36it/s]
em: 0.1211, f1: 0.1540, loss: 4.8306 ||:  22%|##1       | 915/4204 [11:21<42:06,  1.30it/s]
em: 0.1222, f1: 0.1551, loss: 4.8198 ||:  22%|##2       | 930/4204 [11:31<40:31,  1.35it/s]
em: 0.1230, f1: 0.1562, loss: 4.8066 ||:  22%|##2       | 945/4204 [11:41<39:24,  1.38it/s]
em: 0.1230, f1: 0.1562, loss: 4.8017 ||:  23%|##2       | 960/4204 [11:53<40:08,  1.35it/s]
em: 0.1231, f1: 0.1565, loss: 4.7875 ||:  23%|##3       | 976/4204 [12:04<38:45,  1.39it/s]
em: 0.1241, f1: 0.1575, loss: 4.7762 ||:  24%|##3       | 991/4204 [12:16<40:16,  1.33it/s]
em: 0.1248, f1: 0.1581, loss: 4.7700 ||:  24%|##3       | 1005/4204 [12:26<39:40,  1.34it/s]
em: 0.1262, f1: 0.1593, loss: 4.7627 ||:  24%|##4       | 1020/4204 [12:37<38:36,  1.37it/s]
em: 0.1270, f1: 0.1600, loss: 4.7576 ||:  25%|##4       | 1035/4204 [12:48<39:12,  1.35it/s]
em: 0.1274, f1: 0.1604, loss: 4.7573 ||:  25%|##4       | 1050/4204 [12:58<37:50,  1.39it/s]
em: 0.1281, f1: 0.1613, loss: 4.7524 ||:  25%|##5       | 1065/4204 [13:11<39:13,  1.33it/s]
em: 0.1281, f1: 0.1615, loss: 4.7471 ||:  26%|##5       | 1079/4204 [13:21<38:37,  1.35it/s]
em: 0.1286, f1: 0.1620, loss: 4.7405 ||:  26%|##6       | 1094/4204 [13:31<37:33,  1.38it/s]
em: 0.1289, f1: 0.1622, loss: 4.7367 ||:  26%|##6       | 1109/4204 [13:44<39:37,  1.30it/s]
em: 0.1287, f1: 0.1618, loss: 4.7367 ||:  27%|##6       | 1124/4204 [13:55<38:16,  1.34it/s]
em: 0.1286, f1: 0.1619, loss: 4.7348 ||:  27%|##7       | 1139/4204 [14:05<37:16,  1.37it/s]
em: 0.1291, f1: 0.1625, loss: 4.7300 ||:  27%|##7       | 1154/4204 [14:17<38:04,  1.34it/s]
em: 0.1296, f1: 0.1629, loss: 4.7276 ||:  28%|##7       | 1168/4204 [14:27<37:39,  1.34it/s]
em: 0.1301, f1: 0.1634, loss: 4.7194 ||:  28%|##8       | 1182/4204 [14:39<39:04,  1.29it/s]
em: 0.1303, f1: 0.1636, loss: 4.7138 ||:  28%|##8       | 1197/4204 [14:49<37:15,  1.35it/s]
em: 0.1309, f1: 0.1641, loss: 4.7045 ||:  29%|##8       | 1212/4204 [15:00<36:45,  1.36it/s]
em: 0.1315, f1: 0.1646, loss: 4.6993 ||:  29%|##9       | 1226/4204 [15:12<38:19,  1.29it/s]
em: 0.1316, f1: 0.1648, loss: 4.6955 ||:  30%|##9       | 1241/4204 [15:23<37:16,  1.32it/s]
em: 0.1320, f1: 0.1651, loss: 4.6885 ||:  30%|##9       | 1256/4204 [15:35<37:59,  1.29it/s]
em: 0.1320, f1: 0.1652, loss: 4.6862 ||:  30%|###       | 1270/4204 [15:45<37:10,  1.32it/s]
em: 0.1324, f1: 0.1656, loss: 4.6741 ||:  31%|###       | 1286/4204 [15:56<35:35,  1.37it/s]
em: 0.1321, f1: 0.1653, loss: 4.6707 ||:  31%|###       | 1302/4204 [16:12<39:55,  1.21it/s]
em: 0.1324, f1: 0.1657, loss: 4.6650 ||:  31%|###1      | 1316/4204 [16:22<38:10,  1.26it/s]
em: 0.1319, f1: 0.1652, loss: 4.6652 ||:  32%|###1      | 1331/4204 [16:35<38:11,  1.25it/s]
em: 0.1322, f1: 0.1657, loss: 4.6620 ||:  32%|###2      | 1347/4204 [16:45<36:04,  1.32it/s]
em: 0.1326, f1: 0.1663, loss: 4.6568 ||:  32%|###2      | 1363/4204 [16:56<34:32,  1.37it/s]
em: 0.1326, f1: 0.1664, loss: 4.6512 ||:  33%|###2      | 1379/4204 [17:09<35:38,  1.32it/s]
em: 0.1330, f1: 0.1669, loss: 4.6445 ||:  33%|###3      | 1393/4204 [17:19<34:58,  1.34it/s]
em: 0.1337, f1: 0.1675, loss: 4.6400 ||:  33%|###3      | 1407/4204 [17:31<36:22,  1.28it/s]
em: 0.1339, f1: 0.1677, loss: 4.6338 ||:  34%|###3      | 1421/4204 [17:42<35:56,  1.29it/s]
em: 0.1341, f1: 0.1679, loss: 4.6296 ||:  34%|###4      | 1436/4204 [17:52<34:41,  1.33it/s]
em: 0.1344, f1: 0.1681, loss: 4.6236 ||:  35%|###4      | 1451/4204 [18:05<35:45,  1.28it/s]
em: 0.1343, f1: 0.1680, loss: 4.6185 ||:  35%|###4      | 1465/4204 [18:15<34:53,  1.31it/s]
em: 0.1344, f1: 0.1681, loss: 4.6140 ||:  35%|###5      | 1480/4204 [18:25<33:47,  1.34it/s]
em: 0.1345, f1: 0.1682, loss: 4.6075 ||:  36%|###5      | 1495/4204 [18:38<34:45,  1.30it/s]
em: 0.1344, f1: 0.1681, loss: 4.6033 ||:  36%|###5      | 1509/4204 [18:48<34:03,  1.32it/s]
em: 0.1346, f1: 0.1683, loss: 4.5992 ||:  36%|###6      | 1523/4204 [19:00<34:57,  1.28it/s]
em: 0.1350, f1: 0.1686, loss: 4.5967 ||:  37%|###6      | 1538/4204 [19:10<33:33,  1.32it/s]
em: 0.1356, f1: 0.1691, loss: 4.5922 ||:  37%|###6      | 1553/4204 [19:20<32:22,  1.37it/s]
em: 0.1356, f1: 0.1689, loss: 4.5892 ||:  37%|###7      | 1568/4204 [19:32<32:58,  1.33it/s]
em: 0.1356, f1: 0.1690, loss: 4.5843 ||:  38%|###7      | 1583/4204 [19:43<32:18,  1.35it/s]
em: 0.1357, f1: 0.1691, loss: 4.5782 ||:  38%|###8      | 1598/4204 [19:55<33:08,  1.31it/s]
em: 0.1359, f1: 0.1692, loss: 4.5750 ||:  38%|###8      | 1612/4204 [20:06<32:41,  1.32it/s]
em: 0.1360, f1: 0.1693, loss: 4.5699 ||:  39%|###8      | 1628/4204 [20:16<31:16,  1.37it/s]
em: 0.1364, f1: 0.1697, loss: 4.5682 ||:  39%|###9      | 1644/4204 [20:29<32:00,  1.33it/s]
em: 0.1362, f1: 0.1695, loss: 4.5649 ||:  39%|###9      | 1658/4204 [20:39<31:22,  1.35it/s]
em: 0.1366, f1: 0.1699, loss: 4.5635 ||:  40%|###9      | 1673/4204 [20:51<31:58,  1.32it/s]
em: 0.1370, f1: 0.1703, loss: 4.5567 ||:  40%|####      | 1688/4204 [21:01<30:49,  1.36it/s]
em: 0.1375, f1: 0.1707, loss: 4.5524 ||:  41%|####      | 1703/4204 [21:13<30:43,  1.36it/s]
em: 0.1374, f1: 0.1708, loss: 4.5491 ||:  41%|####      | 1717/4204 [21:23<31:03,  1.33it/s]
em: 0.1377, f1: 0.1710, loss: 4.5473 ||:  41%|####1     | 1732/4204 [21:34<30:17,  1.36it/s]
em: 0.1376, f1: 0.1710, loss: 4.5447 ||:  42%|####1     | 1748/4204 [21:44<29:01,  1.41it/s]
em: 0.1377, f1: 0.1710, loss: 4.5418 ||:  42%|####1     | 1764/4204 [21:57<30:12,  1.35it/s]
em: 0.1378, f1: 0.1710, loss: 4.5364 ||:  42%|####2     | 1779/4204 [22:08<29:38,  1.36it/s]
em: 0.1379, f1: 0.1711, loss: 4.5334 ||:  43%|####2     | 1794/4204 [22:20<30:04,  1.34it/s]
em: 0.1383, f1: 0.1715, loss: 4.5278 ||:  43%|####3     | 1809/4204 [22:31<29:23,  1.36it/s]
em: 0.1384, f1: 0.1717, loss: 4.5238 ||:  43%|####3     | 1824/4204 [22:41<29:00,  1.37it/s]
em: 0.1383, f1: 0.1717, loss: 4.5214 ||:  44%|####3     | 1838/4204 [22:53<29:54,  1.32it/s]
em: 0.1382, f1: 0.1718, loss: 4.5205 ||:  44%|####4     | 1853/4204 [23:03<29:03,  1.35it/s]
em: 0.1387, f1: 0.1723, loss: 4.5167 ||:  44%|####4     | 1868/4204 [23:16<30:03,  1.30it/s]
em: 0.1392, f1: 0.1728, loss: 4.5158 ||:  45%|####4     | 1883/4204 [23:26<28:46,  1.34it/s]
em: 0.1397, f1: 0.1733, loss: 4.5114 ||:  45%|####5     | 1898/4204 [23:37<28:05,  1.37it/s]
em: 0.1395, f1: 0.1731, loss: 4.5079 ||:  46%|####5     | 1913/4204 [23:49<29:14,  1.31it/s]
em: 0.1396, f1: 0.1732, loss: 4.5055 ||:  46%|####5     | 1928/4204 [24:00<28:00,  1.35it/s]
em: 0.1392, f1: 0.1728, loss: 4.5024 ||:  46%|####6     | 1943/4204 [24:11<28:18,  1.33it/s]
em: 0.1390, f1: 0.1727, loss: 4.5009 ||:  47%|####6     | 1957/4204 [24:21<27:44,  1.35it/s]
em: 0.1390, f1: 0.1726, loss: 4.4963 ||:  47%|####6     | 1972/4204 [24:32<27:08,  1.37it/s]
em: 0.1393, f1: 0.1730, loss: 4.4920 ||:  47%|####7     | 1987/4204 [24:44<28:07,  1.31it/s]
em: 0.1394, f1: 0.1730, loss: 4.4905 ||:  48%|####7     | 2002/4204 [24:55<27:24,  1.34it/s]
em: 0.1393, f1: 0.1730, loss: 4.4890 ||:  48%|####7     | 2017/4204 [25:08<28:18,  1.29it/s]
em: 0.1396, f1: 0.1734, loss: 4.4850 ||:  48%|####8     | 2031/4204 [25:18<27:32,  1.31it/s]
em: 0.1400, f1: 0.1737, loss: 4.4820 ||:  49%|####8     | 2045/4204 [25:28<26:56,  1.34it/s]
em: 0.1401, f1: 0.1739, loss: 4.4773 ||:  49%|####8     | 2059/4204 [25:40<27:42,  1.29it/s]
em: 0.1403, f1: 0.1742, loss: 4.4751 ||:  49%|####9     | 2074/4204 [25:50<26:48,  1.32it/s]
em: 0.1403, f1: 0.1742, loss: 4.4732 ||:  50%|####9     | 2089/4204 [26:01<25:55,  1.36it/s]
em: 0.1404, f1: 0.1743, loss: 4.4709 ||:  50%|#####     | 2104/4204 [26:13<26:27,  1.32it/s]
em: 0.1409, f1: 0.1746, loss: 4.4677 ||:  50%|#####     | 2118/4204 [26:23<25:57,  1.34it/s]
em: 0.1409, f1: 0.1746, loss: 4.4668 ||:  51%|#####     | 2132/4204 [26:35<26:43,  1.29it/s]
em: 0.1412, f1: 0.1749, loss: 4.4657 ||:  51%|#####1    | 2146/4204 [26:45<26:00,  1.32it/s]
em: 0.1416, f1: 0.1753, loss: 4.4643 ||:  51%|#####1    | 2161/4204 [26:55<25:07,  1.36it/s]
em: 0.1418, f1: 0.1754, loss: 4.4603 ||:  52%|#####1    | 2176/4204 [27:07<25:53,  1.31it/s]
em: 0.1420, f1: 0.1756, loss: 4.4559 ||:  52%|#####2    | 2190/4204 [27:18<25:15,  1.33it/s]
em: 0.1422, f1: 0.1759, loss: 4.4520 ||:  52%|#####2    | 2205/4204 [27:30<25:41,  1.30it/s]
em: 0.1423, f1: 0.1760, loss: 4.4488 ||:  53%|#####2    | 2221/4204 [27:40<24:09,  1.37it/s]
em: 0.1425, f1: 0.1764, loss: 4.4455 ||:  53%|#####3    | 2237/4204 [27:51<23:40,  1.38it/s]
em: 0.1425, f1: 0.1764, loss: 4.4404 ||:  54%|#####3    | 2252/4204 [28:04<24:32,  1.33it/s]
em: 0.1424, f1: 0.1763, loss: 4.4373 ||:  54%|#####3    | 2267/4204 [28:14<23:37,  1.37it/s]
em: 0.1423, f1: 0.1762, loss: 4.4340 ||:  54%|#####4    | 2282/4204 [28:26<24:03,  1.33it/s]
em: 0.1427, f1: 0.1766, loss: 4.4296 ||:  55%|#####4    | 2298/4204 [28:36<22:53,  1.39it/s]
em: 0.1430, f1: 0.1768, loss: 4.4279 ||:  55%|#####5    | 2314/4204 [28:48<22:36,  1.39it/s]
em: 0.1430, f1: 0.1768, loss: 4.4250 ||:  55%|#####5    | 2329/4204 [29:03<25:33,  1.22it/s]
em: 0.1432, f1: 0.1770, loss: 4.4234 ||:  56%|#####5    | 2344/4204 [29:14<24:14,  1.28it/s]
em: 0.1434, f1: 0.1771, loss: 4.4212 ||:  56%|#####6    | 2359/4204 [29:26<24:22,  1.26it/s]
em: 0.1434, f1: 0.1772, loss: 4.4171 ||:  56%|#####6    | 2374/4204 [29:36<23:12,  1.31it/s]
em: 0.1433, f1: 0.1769, loss: 4.4153 ||:  57%|#####6    | 2389/4204 [29:47<22:23,  1.35it/s]
em: 0.1433, f1: 0.1771, loss: 4.4123 ||:  57%|#####7    | 2404/4204 [29:59<22:41,  1.32it/s]
em: 0.1435, f1: 0.1773, loss: 4.4099 ||:  58%|#####7    | 2419/4204 [30:09<21:59,  1.35it/s]
em: 0.1438, f1: 0.1776, loss: 4.4070 ||:  58%|#####7    | 2434/4204 [30:22<22:48,  1.29it/s]
em: 0.1439, f1: 0.1777, loss: 4.4069 ||:  58%|#####8    | 2449/4204 [30:33<22:05,  1.32it/s]
em: 0.1443, f1: 0.1780, loss: 4.4044 ||:  59%|#####8    | 2464/4204 [30:44<21:47,  1.33it/s]
em: 0.1443, f1: 0.1781, loss: 4.4032 ||:  59%|#####8    | 2478/4204 [30:55<21:49,  1.32it/s]
em: 0.1442, f1: 0.1781, loss: 4.4015 ||:  59%|#####9    | 2493/4204 [31:05<21:05,  1.35it/s]
em: 0.1443, f1: 0.1781, loss: 4.3986 ||:  60%|#####9    | 2509/4204 [31:17<21:03,  1.34it/s]
em: 0.1445, f1: 0.1783, loss: 4.3966 ||:  60%|######    | 2523/4204 [31:27<20:39,  1.36it/s]
em: 0.1446, f1: 0.1783, loss: 4.3943 ||:  60%|######    | 2538/4204 [31:37<19:55,  1.39it/s]
em: 0.1447, f1: 0.1784, loss: 4.3943 ||:  61%|######    | 2553/4204 [31:49<20:06,  1.37it/s]
em: 0.1445, f1: 0.1784, loss: 4.3939 ||:  61%|######1   | 2569/4204 [31:59<19:11,  1.42it/s]
em: 0.1448, f1: 0.1786, loss: 4.3912 ||:  61%|######1   | 2585/4204 [32:11<19:35,  1.38it/s]
em: 0.1451, f1: 0.1787, loss: 4.3871 ||:  62%|######1   | 2600/4204 [32:22<19:00,  1.41it/s]
em: 0.1454, f1: 0.1791, loss: 4.3837 ||:  62%|######2   | 2615/4204 [32:32<18:55,  1.40it/s]
em: 0.1455, f1: 0.1791, loss: 4.3821 ||:  63%|######2   | 2629/4204 [32:44<19:41,  1.33it/s]
em: 0.1457, f1: 0.1793, loss: 4.3796 ||:  63%|######2   | 2643/4204 [32:54<19:22,  1.34it/s]
em: 0.1462, f1: 0.1797, loss: 4.3779 ||:  63%|######3   | 2658/4204 [33:05<18:49,  1.37it/s]
em: 0.1465, f1: 0.1800, loss: 4.3752 ||:  64%|######3   | 2673/4204 [33:17<19:03,  1.34it/s]
em: 0.1469, f1: 0.1804, loss: 4.3720 ||:  64%|######3   | 2688/4204 [33:27<18:22,  1.37it/s]
em: 0.1475, f1: 0.1809, loss: 4.3674 ||:  64%|######4   | 2703/4204 [33:39<18:46,  1.33it/s]
em: 0.1478, f1: 0.1813, loss: 4.3652 ||:  65%|######4   | 2719/4204 [33:49<17:44,  1.39it/s]
em: 0.1477, f1: 0.1813, loss: 4.3626 ||:  65%|######5   | 2735/4204 [34:00<17:17,  1.42it/s]
em: 0.1477, f1: 0.1814, loss: 4.3605 ||:  65%|######5   | 2750/4204 [34:13<18:09,  1.33it/s]
em: 0.1475, f1: 0.1812, loss: 4.3608 ||:  66%|######5   | 2764/4204 [34:23<17:57,  1.34it/s]
em: 0.1475, f1: 0.1812, loss: 4.3591 ||:  66%|######6   | 2778/4204 [34:35<18:33,  1.28it/s]
em: 0.1477, f1: 0.1814, loss: 4.3580 ||:  66%|######6   | 2793/4204 [34:46<17:45,  1.32it/s]
em: 0.1478, f1: 0.1814, loss: 4.3561 ||:  67%|######6   | 2808/4204 [34:56<17:19,  1.34it/s]
em: 0.1476, f1: 0.1813, loss: 4.3553 ||:  67%|######7   | 2822/4204 [35:08<17:46,  1.30it/s]
em: 0.1477, f1: 0.1813, loss: 4.3538 ||:  67%|######7   | 2836/4204 [35:18<17:17,  1.32it/s]
em: 0.1478, f1: 0.1815, loss: 4.3525 ||:  68%|######7   | 2850/4204 [35:29<16:56,  1.33it/s]
em: 0.1482, f1: 0.1819, loss: 4.3499 ||:  68%|######8   | 2864/4204 [35:39<16:51,  1.32it/s]
em: 0.1481, f1: 0.1818, loss: 4.3484 ||:  68%|######8   | 2879/4204 [35:49<16:08,  1.37it/s]
em: 0.1487, f1: 0.1824, loss: 4.3435 ||:  69%|######8   | 2894/4204 [36:02<16:29,  1.32it/s]
em: 0.1490, f1: 0.1828, loss: 4.3415 ||:  69%|######9   | 2909/4204 [36:12<15:51,  1.36it/s]
em: 0.1493, f1: 0.1832, loss: 4.3384 ||:  70%|######9   | 2924/4204 [36:23<15:32,  1.37it/s]
em: 0.1493, f1: 0.1832, loss: 4.3372 ||:  70%|######9   | 2939/4204 [36:35<16:03,  1.31it/s]
em: 0.1494, f1: 0.1832, loss: 4.3387 ||:  70%|#######   | 2954/4204 [36:45<15:22,  1.35it/s]
em: 0.1494, f1: 0.1834, loss: 4.3395 ||:  71%|#######   | 2969/4204 [36:58<15:44,  1.31it/s]
em: 0.1498, f1: 0.1838, loss: 4.3384 ||:  71%|#######   | 2983/4204 [37:08<15:26,  1.32it/s]
em: 0.1501, f1: 0.1841, loss: 4.3372 ||:  71%|#######1  | 2997/4204 [37:18<15:03,  1.34it/s]
em: 0.1504, f1: 0.1844, loss: 4.3370 ||:  72%|#######1  | 3011/4204 [37:30<15:30,  1.28it/s]
em: 0.1503, f1: 0.1843, loss: 4.3366 ||:  72%|#######1  | 3025/4204 [37:40<14:56,  1.32it/s]
em: 0.1505, f1: 0.1844, loss: 4.3345 ||:  72%|#######2  | 3040/4204 [37:51<14:29,  1.34it/s]
em: 0.1506, f1: 0.1844, loss: 4.3332 ||:  73%|#######2  | 3054/4204 [38:03<14:48,  1.29it/s]
em: 0.1508, f1: 0.1846, loss: 4.3329 ||:  73%|#######3  | 3069/4204 [38:13<14:12,  1.33it/s]
em: 0.1510, f1: 0.1848, loss: 4.3302 ||:  73%|#######3  | 3084/4204 [38:25<14:18,  1.30it/s]
em: 0.1511, f1: 0.1849, loss: 4.3268 ||:  74%|#######3  | 3099/4204 [38:36<13:42,  1.34it/s]
em: 0.1513, f1: 0.1850, loss: 4.3246 ||:  74%|#######4  | 3114/4204 [38:46<13:20,  1.36it/s]
em: 0.1515, f1: 0.1853, loss: 4.3208 ||:  74%|#######4  | 3129/4204 [38:59<13:35,  1.32it/s]
em: 0.1518, f1: 0.1856, loss: 4.3179 ||:  75%|#######4  | 3143/4204 [39:09<13:11,  1.34it/s]
em: 0.1519, f1: 0.1857, loss: 4.3148 ||:  75%|#######5  | 3157/4204 [39:20<13:24,  1.30it/s]
em: 0.1521, f1: 0.1860, loss: 4.3130 ||:  75%|#######5  | 3172/4204 [39:30<12:44,  1.35it/s]
em: 0.1522, f1: 0.1860, loss: 4.3114 ||:  76%|#######5  | 3187/4204 [39:41<12:24,  1.37it/s]
em: 0.1523, f1: 0.1861, loss: 4.3100 ||:  76%|#######6  | 3202/4204 [39:54<12:50,  1.30it/s]
em: 0.1524, f1: 0.1862, loss: 4.3089 ||:  77%|#######6  | 3217/4204 [40:04<12:14,  1.34it/s]
em: 0.1527, f1: 0.1865, loss: 4.3053 ||:  77%|#######6  | 3232/4204 [40:17<12:35,  1.29it/s]
em: 0.1527, f1: 0.1865, loss: 4.3036 ||:  77%|#######7  | 3247/4204 [40:27<11:58,  1.33it/s]
em: 0.1530, f1: 0.1868, loss: 4.3014 ||:  78%|#######7  | 3262/4204 [40:38<11:33,  1.36it/s]
em: 0.1530, f1: 0.1868, loss: 4.2992 ||:  78%|#######7  | 3277/4204 [40:50<11:43,  1.32it/s]
em: 0.1531, f1: 0.1868, loss: 4.2964 ||:  78%|#######8  | 3293/4204 [41:00<11:03,  1.37it/s]
em: 0.1533, f1: 0.1870, loss: 4.2936 ||:  79%|#######8  | 3309/4204 [41:12<10:56,  1.36it/s]
em: 0.1533, f1: 0.1870, loss: 4.2938 ||:  79%|#######9  | 3324/4204 [41:22<10:28,  1.40it/s]
em: 0.1536, f1: 0.1874, loss: 4.2904 ||:  79%|#######9  | 3339/4204 [41:33<10:17,  1.40it/s]
em: 0.1537, f1: 0.1874, loss: 4.2874 ||:  80%|#######9  | 3354/4204 [41:45<10:36,  1.34it/s]
em: 0.1537, f1: 0.1874, loss: 4.2864 ||:  80%|########  | 3369/4204 [41:56<10:06,  1.38it/s]
em: 0.1539, f1: 0.1876, loss: 4.2828 ||:  80%|########  | 3384/4204 [42:07<10:00,  1.37it/s]
em: 0.1542, f1: 0.1878, loss: 4.2794 ||:  81%|########  | 3398/4204 [42:17<09:48,  1.37it/s]
em: 0.1545, f1: 0.1882, loss: 4.2756 ||:  81%|########1 | 3413/4204 [42:28<09:32,  1.38it/s]
em: 0.1546, f1: 0.1883, loss: 4.2740 ||:  82%|########1 | 3428/4204 [42:39<09:36,  1.35it/s]
em: 0.1549, f1: 0.1885, loss: 4.2725 ||:  82%|########1 | 3443/4204 [42:50<09:20,  1.36it/s]
em: 0.1550, f1: 0.1888, loss: 4.2697 ||:  82%|########2 | 3457/4204 [43:00<09:06,  1.37it/s]
em: 0.1552, f1: 0.1889, loss: 4.2688 ||:  83%|########2 | 3471/4204 [43:12<09:23,  1.30it/s]
em: 0.1555, f1: 0.1893, loss: 4.2667 ||:  83%|########2 | 3485/4204 [43:22<09:03,  1.32it/s]
em: 0.1557, f1: 0.1896, loss: 4.2656 ||:  83%|########3 | 3499/4204 [43:34<09:05,  1.29it/s]
em: 0.1558, f1: 0.1896, loss: 4.2650 ||:  84%|########3 | 3513/4204 [43:44<08:49,  1.30it/s]
em: 0.1559, f1: 0.1898, loss: 4.2623 ||:  84%|########3 | 3528/4204 [43:55<08:24,  1.34it/s]
em: 0.1561, f1: 0.1900, loss: 4.2619 ||:  84%|########4 | 3543/4204 [44:07<08:29,  1.30it/s]
em: 0.1562, f1: 0.1902, loss: 4.2612 ||:  85%|########4 | 3557/4204 [44:18<08:12,  1.31it/s]
em: 0.1566, f1: 0.1906, loss: 4.2586 ||:  85%|########4 | 3572/4204 [44:28<07:46,  1.36it/s]
em: 0.1570, f1: 0.1909, loss: 4.2562 ||:  85%|########5 | 3587/4204 [44:40<07:43,  1.33it/s]
em: 0.1570, f1: 0.1910, loss: 4.2540 ||:  86%|########5 | 3602/4204 [44:50<07:21,  1.36it/s]
em: 0.1571, f1: 0.1910, loss: 4.2521 ||:  86%|########6 | 3617/4204 [45:07<08:16,  1.18it/s]
em: 0.1573, f1: 0.1913, loss: 4.2508 ||:  86%|########6 | 3632/4204 [45:17<07:39,  1.24it/s]
em: 0.1574, f1: 0.1914, loss: 4.2486 ||:  87%|########6 | 3647/4204 [45:28<07:14,  1.28it/s]
em: 0.1574, f1: 0.1914, loss: 4.2483 ||:  87%|########7 | 3661/4204 [45:39<07:09,  1.27it/s]
em: 0.1575, f1: 0.1915, loss: 4.2494 ||:  87%|########7 | 3676/4204 [45:50<06:42,  1.31it/s]
em: 0.1579, f1: 0.1918, loss: 4.2479 ||:  88%|########7 | 3691/4204 [46:02<06:39,  1.28it/s]
em: 0.1580, f1: 0.1919, loss: 4.2460 ||:  88%|########8 | 3705/4204 [46:12<06:19,  1.32it/s]
em: 0.1581, f1: 0.1920, loss: 4.2433 ||:  88%|########8 | 3720/4204 [46:23<06:00,  1.34it/s]
em: 0.1581, f1: 0.1921, loss: 4.2406 ||:  89%|########8 | 3735/4204 [46:35<06:01,  1.30it/s]
em: 0.1584, f1: 0.1923, loss: 4.2378 ||:  89%|########9 | 3749/4204 [46:45<05:43,  1.32it/s]
em: 0.1584, f1: 0.1924, loss: 4.2347 ||:  90%|########9 | 3763/4204 [46:56<05:36,  1.31it/s]
em: 0.1584, f1: 0.1923, loss: 4.2334 ||:  90%|########9 | 3777/4204 [47:07<05:21,  1.33it/s]
em: 0.1584, f1: 0.1923, loss: 4.2319 ||:  90%|######### | 3792/4204 [47:17<05:03,  1.36it/s]
em: 0.1584, f1: 0.1924, loss: 4.2314 ||:  91%|######### | 3807/4204 [47:30<05:04,  1.30it/s]
em: 0.1588, f1: 0.1927, loss: 4.2298 ||:  91%|######### | 3822/4204 [47:40<04:46,  1.34it/s]
em: 0.1593, f1: 0.1933, loss: 4.2267 ||:  91%|#########1| 3837/4204 [47:51<04:32,  1.35it/s]
em: 0.1595, f1: 0.1935, loss: 4.2256 ||:  92%|#########1| 3851/4204 [48:03<04:35,  1.28it/s]
em: 0.1595, f1: 0.1935, loss: 4.2248 ||:  92%|#########1| 3865/4204 [48:13<04:18,  1.31it/s]
em: 0.1594, f1: 0.1934, loss: 4.2237 ||:  92%|#########2| 3879/4204 [48:25<04:15,  1.27it/s]
em: 0.1596, f1: 0.1936, loss: 4.2228 ||:  93%|#########2| 3893/4204 [48:35<03:58,  1.31it/s]
em: 0.1597, f1: 0.1938, loss: 4.2218 ||:  93%|#########2| 3907/4204 [48:45<03:43,  1.33it/s]
em: 0.1597, f1: 0.1938, loss: 4.2206 ||:  93%|#########3| 3921/4204 [48:57<03:40,  1.28it/s]
em: 0.1596, f1: 0.1938, loss: 4.2225 ||:  94%|#########3| 3935/4204 [49:07<03:24,  1.31it/s]
em: 0.1594, f1: 0.1937, loss: 4.2234 ||:  94%|#########3| 3949/4204 [49:17<03:11,  1.33it/s]
em: 0.1594, f1: 0.1937, loss: 4.2244 ||:  94%|#########4| 3963/4204 [49:29<03:05,  1.30it/s]
em: 0.1594, f1: 0.1938, loss: 4.2247 ||:  95%|#########4| 3978/4204 [49:39<02:47,  1.35it/s]
em: 0.1593, f1: 0.1937, loss: 4.2262 ||:  95%|#########4| 3993/4204 [49:50<02:38,  1.33it/s]
em: 0.1592, f1: 0.1937, loss: 4.2279 ||:  95%|#########5| 4007/4204 [50:01<02:27,  1.34it/s]
em: 0.1591, f1: 0.1936, loss: 4.2287 ||:  96%|#########5| 4021/4204 [50:11<02:17,  1.34it/s]
em: 0.1589, f1: 0.1934, loss: 4.2310 ||:  96%|#########5| 4035/4204 [50:22<02:08,  1.32it/s]
em: 0.1588, f1: 0.1933, loss: 4.2326 ||:  96%|#########6| 4049/4204 [50:33<01:56,  1.33it/s]
em: 0.1589, f1: 0.1934, loss: 4.2343 ||:  97%|#########6| 4063/4204 [50:43<01:45,  1.34it/s]
em: 0.1588, f1: 0.1934, loss: 4.2345 ||:  97%|#########6| 4077/4204 [50:54<01:37,  1.31it/s]
em: 0.1586, f1: 0.1933, loss: 4.2366 ||:  97%|#########7| 4092/4204 [51:05<01:23,  1.34it/s]
em: 0.1586, f1: 0.1932, loss: 4.2374 ||:  98%|#########7| 4107/4204 [51:18<01:15,  1.28it/s]
em: 0.1584, f1: 0.1931, loss: 4.2396 ||:  98%|#########8| 4121/4204 [51:28<01:03,  1.30it/s]
em: 0.1583, f1: 0.1930, loss: 4.2410 ||:  98%|#########8| 4137/4204 [51:39<00:49,  1.36it/s]
em: 0.1582, f1: 0.1928, loss: 4.2424 ||:  99%|#########8| 4153/4204 [51:51<00:37,  1.34it/s]
em: 0.1581, f1: 0.1927, loss: 4.2443 ||:  99%|#########9| 4167/4204 [52:01<00:27,  1.34it/s]
em: 0.1581, f1: 0.1928, loss: 4.2451 ||:  99%|#########9| 4181/4204 [52:12<00:17,  1.33it/s]
em: 0.1581, f1: 0.1928, loss: 4.2459 ||: 100%|#########9| 4195/4204 [52:22<00:06,  1.35it/s]
em: 0.1580, f1: 0.1928, loss: 4.2472 ||: : 4209it [52:32,  1.35it/s]                        
em: 0.1578, f1: 0.1927, loss: 4.2488 ||: : 4223it [52:44,  1.30it/s]
em: 0.1577, f1: 0.1927, loss: 4.2490 ||: : 4238it [52:54,  1.35it/s]
em: 0.1576, f1: 0.1925, loss: 4.2505 ||: : 4253it [53:05,  1.36it/s]
em: 0.1575, f1: 0.1925, loss: 4.2509 ||: : 4260it [53:10,  1.34it/s]

2019-05-09 20:47:58,908 - INFO - allennlp.training.trainer - Validating
  0%|          | 0/596 [00:00<?, ?it/s]
em: 0.2898, f1: 0.3195, loss: 1278411.6066 ||:   4%|3         | 22/596 [00:10<04:34,  2.09it/s]
em: 0.2809, f1: 0.3037, loss: 1061049.2504 ||:   7%|7         | 43/596 [00:26<05:13,  1.76it/s]
em: 0.2693, f1: 0.2957, loss: 1411292.9121 ||:  10%|#         | 62/596 [00:36<04:57,  1.80it/s]
em: 0.2531, f1: 0.2809, loss: 1589508.7953 ||:  14%|#3        | 81/596 [00:51<05:21,  1.60it/s]
em: 0.2556, f1: 0.2829, loss: 1627477.7849 ||:  17%|#6        | 101/596 [01:01<04:51,  1.70it/s]
em: 0.2615, f1: 0.2894, loss: 1704548.0136 ||:  20%|##        | 121/596 [01:15<04:55,  1.61it/s]
em: 0.2576, f1: 0.2861, loss: 1702130.2525 ||:  24%|##3       | 141/596 [01:26<04:28,  1.69it/s]
em: 0.2551, f1: 0.2825, loss: 1704195.1813 ||:  27%|##7       | 161/596 [01:41<04:40,  1.55it/s]
em: 0.2489, f1: 0.2755, loss: 1724862.9298 ||:  30%|###       | 179/596 [01:51<04:18,  1.61it/s]
em: 0.2474, f1: 0.2727, loss: 1703682.8484 ||:  33%|###3      | 197/596 [02:04<04:17,  1.55it/s]
em: 0.2456, f1: 0.2711, loss: 1664749.1731 ||:  36%|###6      | 217/596 [02:14<03:49,  1.65it/s]
em: 0.2420, f1: 0.2685, loss: 1656120.8173 ||:  40%|###9      | 237/596 [02:27<03:42,  1.61it/s]
em: 0.2431, f1: 0.2710, loss: 1598041.9279 ||:  43%|####2     | 255/596 [02:38<03:26,  1.65it/s]
em: 0.2435, f1: 0.2711, loss: 1634618.1169 ||:  46%|####5     | 273/596 [02:51<03:27,  1.55it/s]
em: 0.2455, f1: 0.2732, loss: 1562502.7462 ||:  49%|####8     | 292/596 [03:01<03:07,  1.62it/s]
em: 0.2451, f1: 0.2717, loss: 1585613.6713 ||:  52%|#####2    | 311/596 [03:14<03:02,  1.56it/s]
em: 0.2425, f1: 0.2688, loss: 1589017.9122 ||:  55%|#####5    | 330/596 [03:24<02:41,  1.65it/s]
em: 0.2424, f1: 0.2686, loss: 1615332.2701 ||:  59%|#####8    | 349/596 [03:40<02:43,  1.51it/s]
em: 0.2449, f1: 0.2711, loss: 1558268.3471 ||:  62%|######1   | 369/596 [03:50<02:21,  1.60it/s]
em: 0.2458, f1: 0.2719, loss: 1586665.1457 ||:  65%|######5   | 388/596 [04:04<02:16,  1.53it/s]
em: 0.2432, f1: 0.2686, loss: 1570948.7221 ||:  68%|######8   | 407/596 [04:14<01:56,  1.62it/s]
em: 0.2438, f1: 0.2696, loss: 1553699.9799 ||:  71%|#######1  | 426/596 [04:28<01:50,  1.54it/s]
em: 0.2440, f1: 0.2708, loss: 1515452.2732 ||:  75%|#######4  | 445/596 [04:38<01:33,  1.62it/s]
em: 0.2398, f1: 0.2663, loss: 1565196.7957 ||:  78%|#######7  | 464/596 [04:52<01:25,  1.55it/s]
em: 0.2336, f1: 0.2602, loss: 1689435.7515 ||:  81%|########1 | 485/596 [05:02<01:06,  1.68it/s]
em: 0.2324, f1: 0.2587, loss: 1784834.7224 ||:  85%|########4 | 506/596 [05:12<00:50,  1.77it/s]
em: 0.2294, f1: 0.2559, loss: 1843042.4472 ||:  89%|########8 | 528/596 [05:22<00:36,  1.88it/s]
em: 0.2281, f1: 0.2542, loss: 1897729.9399 ||:  92%|#########2| 550/596 [05:33<00:24,  1.91it/s]
em: 0.2240, f1: 0.2501, loss: 1929827.2351 ||:  96%|#########5| 570/596 [05:46<00:14,  1.80it/s]
em: 0.2196, f1: 0.2461, loss: 1983421.0084 ||:  99%|#########8| 588/596 [05:56<00:04,  1.79it/s]
em: 0.2176, f1: 0.2442, loss: 1972270.8759 ||: : 604it [06:09,  1.64it/s]                       

2019-05-09 20:54:08,189 - INFO - allennlp.training.tensorboard_writer -                     Training |  Validation
2019-05-09 20:54:08,190 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB   |  5960.964  |       N/A
2019-05-09 20:54:08,190 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB |  1429.000  |       N/A
2019-05-09 20:54:08,190 - INFO - allennlp.training.tensorboard_writer - loss            |     4.251  |  1972270.876
2019-05-09 20:54:08,190 - INFO - allennlp.training.tensorboard_writer - em              |     0.157  |     0.218
2019-05-09 20:54:08,191 - INFO - allennlp.training.tensorboard_writer - f1              |     0.192  |     0.244
2019-05-09 20:54:12,942 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to 'out/rc_bert_simple/best.th'.
2019-05-09 20:54:17,525 - INFO - allennlp.training.trainer - Epoch duration: 00:59:29
2019-05-09 20:54:17,526 - INFO - allennlp.training.trainer - Estimated training time remaining: 8:55:25
2019-05-09 20:54:17,526 - INFO - allennlp.training.trainer - Epoch 1/9
2019-05-09 20:54:17,526 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 8320.872
2019-05-09 20:54:17,808 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 4339
2019-05-09 20:54:17,815 - INFO - allennlp.training.trainer - Training
  0%|          | 0/4204 [00:00<?, ?it/s]
em: 0.2321, f1: 0.2657, loss: 3.4606 ||:   0%|          | 14/4204 [00:10<50:54,  1.37it/s]
em: 0.2303, f1: 0.2618, loss: 3.4113 ||:   1%|          | 29/4204 [00:20<50:23,  1.38it/s]
em: 0.2299, f1: 0.2581, loss: 3.4967 ||:   1%|1         | 44/4204 [00:31<49:44,  1.39it/s]
em: 0.2158, f1: 0.2431, loss: 3.5809 ||:   1%|1         | 59/4204 [00:42<49:21,  1.40it/s]
em: 0.2200, f1: 0.2512, loss: 3.4828 ||:   2%|1         | 76/4204 [00:52<46:40,  1.47it/s]
em: 0.2194, f1: 0.2491, loss: 3.5590 ||:   2%|2         | 93/4204 [01:03<46:50,  1.46it/s]
em: 0.2207, f1: 0.2495, loss: 3.6201 ||:   3%|2         | 108/4204 [01:15<47:53,  1.43it/s]
em: 0.2151, f1: 0.2443, loss: 3.6447 ||:   3%|2         | 122/4204 [01:25<48:26,  1.40it/s]
em: 0.2097, f1: 0.2407, loss: 3.6601 ||:   3%|3         | 137/4204 [01:35<47:44,  1.42it/s]
em: 0.2100, f1: 0.2416, loss: 3.6470 ||:   4%|3         | 152/4204 [01:46<48:13,  1.40it/s]
em: 0.2110, f1: 0.2436, loss: 3.6506 ||:   4%|3         | 166/4204 [01:57<48:33,  1.39it/s]
em: 0.2125, f1: 0.2459, loss: 3.6306 ||:   4%|4         | 181/4204 [02:07<47:42,  1.41it/s]
em: 0.2098, f1: 0.2451, loss: 3.6521 ||:   5%|4         | 196/4204 [02:18<47:31,  1.41it/s]
em: 0.2121, f1: 0.2480, loss: 3.6379 ||:   5%|5         | 211/4204 [02:28<47:11,  1.41it/s]
em: 0.2108, f1: 0.2468, loss: 3.6313 ||:   5%|5         | 226/4204 [02:38<46:25,  1.43it/s]
em: 0.2097, f1: 0.2463, loss: 3.6214 ||:   6%|5         | 241/4204 [02:49<46:19,  1.43it/s]
em: 0.2077, f1: 0.2448, loss: 3.6135 ||:   6%|6         | 256/4204 [03:00<46:14,  1.42it/s]
em: 0.2107, f1: 0.2467, loss: 3.6294 ||:   6%|6         | 271/4204 [03:10<45:47,  1.43it/s]
em: 0.2110, f1: 0.2474, loss: 3.6335 ||:   7%|6         | 288/4204 [03:20<44:08,  1.48it/s]
em: 0.2093, f1: 0.2460, loss: 3.6366 ||:   7%|7         | 305/4204 [03:33<45:08,  1.44it/s]
em: 0.2113, f1: 0.2484, loss: 3.6312 ||:   8%|7         | 319/4204 [03:44<46:08,  1.40it/s]
em: 0.2125, f1: 0.2492, loss: 3.6250 ||:   8%|7         | 333/4204 [03:54<46:26,  1.39it/s]
em: 0.2126, f1: 0.2497, loss: 3.6404 ||:   8%|8         | 347/4204 [04:05<47:07,  1.36it/s]
em: 0.2118, f1: 0.2490, loss: 3.6514 ||:   9%|8         | 362/4204 [04:15<46:22,  1.38it/s]
em: 0.2120, f1: 0.2490, loss: 3.6553 ||:   9%|8         | 377/4204 [04:26<46:45,  1.36it/s]
em: 0.2109, f1: 0.2479, loss: 3.6648 ||:   9%|9         | 392/4204 [04:37<46:10,  1.38it/s]
em: 0.2101, f1: 0.2467, loss: 3.6733 ||:  10%|9         | 407/4204 [04:47<44:58,  1.41it/s]
em: 0.2104, f1: 0.2473, loss: 3.6643 ||:  10%|#         | 422/4204 [04:58<45:10,  1.40it/s]
em: 0.2107, f1: 0.2475, loss: 3.6623 ||:  10%|#         | 436/4204 [05:08<45:10,  1.39it/s]
em: 0.2111, f1: 0.2477, loss: 3.6573 ||:  11%|#         | 450/4204 [05:19<45:09,  1.39it/s]
em: 0.2096, f1: 0.2462, loss: 3.6640 ||:  11%|#1        | 465/4204 [05:29<44:03,  1.41it/s]
em: 0.2108, f1: 0.2471, loss: 3.6628 ||:  11%|#1        | 480/4204 [05:40<44:27,  1.40it/s]
em: 0.2101, f1: 0.2465, loss: 3.6645 ||:  12%|#1        | 495/4204 [05:50<43:33,  1.42it/s]
em: 0.2103, f1: 0.2463, loss: 3.6589 ||:  12%|#2        | 510/4204 [06:01<44:06,  1.40it/s]
em: 0.2099, f1: 0.2458, loss: 3.6557 ||:  12%|#2        | 525/4204 [06:11<43:17,  1.42it/s]
em: 0.2107, f1: 0.2466, loss: 3.6510 ||:  13%|#2        | 540/4204 [06:22<43:28,  1.40it/s]
em: 0.2102, f1: 0.2459, loss: 3.6461 ||:  13%|#3        | 556/4204 [06:33<42:23,  1.43it/s]
em: 0.2115, f1: 0.2468, loss: 3.6359 ||:  14%|#3        | 572/4204 [06:45<43:05,  1.40it/s]
em: 0.2121, f1: 0.2476, loss: 3.6372 ||:  14%|#3        | 587/4204 [06:55<42:53,  1.41it/s]
em: 0.2126, f1: 0.2480, loss: 3.6284 ||:  14%|#4        | 602/4204 [07:06<42:30,  1.41it/s]
em: 0.2111, f1: 0.2469, loss: 3.6356 ||:  15%|#4        | 617/4204 [07:16<42:10,  1.42it/s]
em: 0.2117, f1: 0.2473, loss: 3.6345 ||:  15%|#5        | 632/4204 [07:27<42:11,  1.41it/s]
em: 0.2130, f1: 0.2488, loss: 3.6313 ||:  15%|#5        | 646/4204 [07:37<42:26,  1.40it/s]
em: 0.2135, f1: 0.2492, loss: 3.6256 ||:  16%|#5        | 660/4204 [07:48<42:39,  1.38it/s]
em: 0.2133, f1: 0.2488, loss: 3.6220 ||:  16%|#6        | 675/4204 [07:58<42:18,  1.39it/s]
em: 0.2123, f1: 0.2478, loss: 3.6223 ||:  16%|#6        | 691/4204 [08:08<40:28,  1.45it/s]
em: 0.2124, f1: 0.2478, loss: 3.6163 ||:  17%|#6        | 707/4204 [08:19<39:51,  1.46it/s]
em: 0.2125, f1: 0.2478, loss: 3.6153 ||:  17%|#7        | 723/4204 [08:30<39:48,  1.46it/s]
em: 0.2124, f1: 0.2481, loss: 3.6111 ||:  18%|#7        | 738/4204 [08:41<39:56,  1.45it/s]
em: 0.2130, f1: 0.2486, loss: 3.6088 ||:  18%|#7        | 753/4204 [08:51<39:53,  1.44it/s]
em: 0.2136, f1: 0.2492, loss: 3.6106 ||:  18%|#8        | 768/4204 [09:02<40:26,  1.42it/s]
em: 0.2132, f1: 0.2487, loss: 3.6109 ||:  19%|#8        | 783/4204 [09:13<40:12,  1.42it/s]
em: 0.2136, f1: 0.2492, loss: 3.6063 ||:  19%|#8        | 798/4204 [09:23<40:06,  1.42it/s]
em: 0.2133, f1: 0.2488, loss: 3.6115 ||:  19%|#9        | 813/4204 [09:33<39:16,  1.44it/s]
em: 0.2134, f1: 0.2489, loss: 3.6129 ||:  20%|#9        | 828/4204 [09:44<39:17,  1.43it/s]
em: 0.2145, f1: 0.2501, loss: 3.6154 ||:  20%|##        | 843/4204 [09:55<39:51,  1.41it/s]
em: 0.2146, f1: 0.2501, loss: 3.6191 ||:  20%|##        | 860/4204 [10:06<38:19,  1.45it/s]
em: 0.2143, f1: 0.2499, loss: 3.6250 ||:  21%|##        | 876/4204 [10:17<38:49,  1.43it/s]
em: 0.2147, f1: 0.2502, loss: 3.6240 ||:  21%|##1       | 890/4204 [10:28<39:09,  1.41it/s]
em: 0.2146, f1: 0.2500, loss: 3.6259 ||:  22%|##1       | 904/4204 [10:38<39:31,  1.39it/s]
em: 0.2156, f1: 0.2511, loss: 3.6239 ||:  22%|##1       | 918/4204 [10:48<39:28,  1.39it/s]
em: 0.2157, f1: 0.2513, loss: 3.6246 ||:  22%|##2       | 933/4204 [10:59<39:01,  1.40it/s]
em: 0.2166, f1: 0.2523, loss: 3.6180 ||:  23%|##2       | 949/4204 [11:09<38:04,  1.43it/s]
em: 0.2170, f1: 0.2528, loss: 3.6153 ||:  23%|##2       | 964/4204 [11:20<37:37,  1.44it/s]
em: 0.2169, f1: 0.2524, loss: 3.6129 ||:  23%|##3       | 979/4204 [11:30<37:25,  1.44it/s]
em: 0.2167, f1: 0.2522, loss: 3.6098 ||:  24%|##3       | 994/4204 [11:41<37:53,  1.41it/s]
em: 0.2173, f1: 0.2527, loss: 3.6098 ||:  24%|##3       | 1008/4204 [11:51<37:51,  1.41it/s]
em: 0.2180, f1: 0.2535, loss: 3.6076 ||:  24%|##4       | 1023/4204 [12:02<37:18,  1.42it/s]
em: 0.2183, f1: 0.2536, loss: 3.6089 ||:  25%|##4       | 1038/4204 [12:12<37:11,  1.42it/s]
em: 0.2181, f1: 0.2533, loss: 3.6123 ||:  25%|##5       | 1053/4204 [12:22<36:33,  1.44it/s]
em: 0.2179, f1: 0.2529, loss: 3.6170 ||:  25%|##5       | 1068/4204 [12:33<36:43,  1.42it/s]
em: 0.2178, f1: 0.2527, loss: 3.6170 ||:  26%|##5       | 1082/4204 [12:43<36:50,  1.41it/s]
em: 0.2177, f1: 0.2527, loss: 3.6189 ||:  26%|##6       | 1096/4204 [12:53<37:00,  1.40it/s]
em: 0.2175, f1: 0.2526, loss: 3.6216 ||:  26%|##6       | 1110/4204 [13:04<36:58,  1.39it/s]
em: 0.2175, f1: 0.2528, loss: 3.6257 ||:  27%|##6       | 1125/4204 [13:14<36:37,  1.40it/s]
em: 0.2180, f1: 0.2532, loss: 3.6260 ||:  27%|##7       | 1140/4204 [13:25<36:51,  1.39it/s]
em: 0.2174, f1: 0.2527, loss: 3.6253 ||:  27%|##7       | 1154/4204 [13:35<36:49,  1.38it/s]
em: 0.2175, f1: 0.2528, loss: 3.6245 ||:  28%|##7       | 1168/4204 [13:45<36:33,  1.38it/s]
em: 0.2177, f1: 0.2529, loss: 3.6251 ||:  28%|##8       | 1182/4204 [13:56<36:32,  1.38it/s]
em: 0.2180, f1: 0.2530, loss: 3.6235 ||:  28%|##8       | 1196/4204 [14:06<36:34,  1.37it/s]
em: 0.2186, f1: 0.2536, loss: 3.6164 ||:  29%|##8       | 1211/4204 [14:16<35:34,  1.40it/s]
em: 0.2182, f1: 0.2533, loss: 3.6167 ||:  29%|##9       | 1226/4204 [14:27<35:22,  1.40it/s]
em: 0.2179, f1: 0.2528, loss: 3.6191 ||:  30%|##9       | 1241/4204 [14:38<35:40,  1.38it/s]
em: 0.2178, f1: 0.2526, loss: 3.6165 ||:  30%|##9       | 1255/4204 [14:48<35:24,  1.39it/s]
em: 0.2181, f1: 0.2529, loss: 3.6139 ||:  30%|###       | 1270/4204 [14:58<34:42,  1.41it/s]
em: 0.2184, f1: 0.2532, loss: 3.6126 ||:  31%|###       | 1285/4204 [15:09<34:53,  1.39it/s]
em: 0.2182, f1: 0.2530, loss: 3.6110 ||:  31%|###       | 1300/4204 [15:20<34:38,  1.40it/s]
em: 0.2176, f1: 0.2524, loss: 3.6127 ||:  31%|###1      | 1315/4204 [15:30<34:06,  1.41it/s]
em: 0.2173, f1: 0.2521, loss: 3.6166 ||:  32%|###1      | 1330/4204 [15:41<34:02,  1.41it/s]
em: 0.2174, f1: 0.2525, loss: 3.6147 ||:  32%|###1      | 1345/4204 [15:52<33:46,  1.41it/s]
em: 0.2176, f1: 0.2528, loss: 3.6148 ||:  32%|###2      | 1360/4204 [16:02<33:29,  1.42it/s]
em: 0.2185, f1: 0.2537, loss: 3.6099 ||:  33%|###2      | 1375/4204 [16:13<33:03,  1.43it/s]
em: 0.2183, f1: 0.2536, loss: 3.6107 ||:  33%|###3      | 1390/4204 [16:23<33:09,  1.41it/s]
em: 0.2185, f1: 0.2538, loss: 3.6091 ||:  33%|###3      | 1404/4204 [16:34<33:28,  1.39it/s]
em: 0.2188, f1: 0.2542, loss: 3.6078 ||:  34%|###3      | 1418/4204 [16:44<33:49,  1.37it/s]
em: 0.2191, f1: 0.2545, loss: 3.6071 ||:  34%|###4      | 1432/4204 [16:55<33:44,  1.37it/s]
em: 0.2190, f1: 0.2544, loss: 3.6038 ||:  34%|###4      | 1446/4204 [17:05<34:01,  1.35it/s]
em: 0.2186, f1: 0.2540, loss: 3.6042 ||:  35%|###4      | 1461/4204 [17:16<33:25,  1.37it/s]
em: 0.2190, f1: 0.2544, loss: 3.6022 ||:  35%|###5      | 1476/4204 [17:27<33:09,  1.37it/s]
em: 0.2188, f1: 0.2542, loss: 3.6005 ||:  35%|###5      | 1490/4204 [17:37<33:21,  1.36it/s]
em: 0.2185, f1: 0.2540, loss: 3.6000 ||:  36%|###5      | 1504/4204 [17:47<32:52,  1.37it/s]
em: 0.2186, f1: 0.2542, loss: 3.6004 ||:  36%|###6      | 1519/4204 [17:58<32:27,  1.38it/s]
em: 0.2188, f1: 0.2545, loss: 3.5989 ||:  36%|###6      | 1534/4204 [18:09<32:06,  1.39it/s]
em: 0.2190, f1: 0.2546, loss: 3.6012 ||:  37%|###6      | 1549/4204 [18:19<31:26,  1.41it/s]
em: 0.2189, f1: 0.2545, loss: 3.6029 ||:  37%|###7      | 1564/4204 [18:30<31:33,  1.39it/s]
em: 0.2189, f1: 0.2545, loss: 3.6017 ||:  38%|###7      | 1578/4204 [18:40<31:25,  1.39it/s]
em: 0.2191, f1: 0.2548, loss: 3.5962 ||:  38%|###7      | 1593/4204 [18:50<30:47,  1.41it/s]
em: 0.2196, f1: 0.2552, loss: 3.5928 ||:  38%|###8      | 1608/4204 [19:01<30:47,  1.41it/s]
em: 0.2199, f1: 0.2556, loss: 3.5908 ||:  39%|###8      | 1623/4204 [19:12<30:16,  1.42it/s]
em: 0.2195, f1: 0.2553, loss: 3.5946 ||:  39%|###8      | 1638/4204 [19:22<29:40,  1.44it/s]
em: 0.2195, f1: 0.2553, loss: 3.5974 ||:  39%|###9      | 1653/4204 [19:32<29:45,  1.43it/s]
em: 0.2198, f1: 0.2554, loss: 3.5954 ||:  40%|###9      | 1668/4204 [19:43<29:42,  1.42it/s]
em: 0.2195, f1: 0.2552, loss: 3.5963 ||:  40%|####      | 1683/4204 [19:54<29:51,  1.41it/s]
em: 0.2195, f1: 0.2552, loss: 3.5956 ||:  40%|####      | 1698/4204 [20:04<29:31,  1.41it/s]
em: 0.2195, f1: 0.2554, loss: 3.5928 ||:  41%|####      | 1713/4204 [20:15<29:08,  1.42it/s]
em: 0.2194, f1: 0.2553, loss: 3.5926 ||:  41%|####1     | 1728/4204 [20:25<28:40,  1.44it/s]
em: 0.2191, f1: 0.2549, loss: 3.5942 ||:  41%|####1     | 1743/4204 [20:36<28:59,  1.41it/s]
em: 0.2192, f1: 0.2550, loss: 3.5937 ||:  42%|####1     | 1758/4204 [20:46<28:22,  1.44it/s]
em: 0.2188, f1: 0.2545, loss: 3.5924 ||:  42%|####2     | 1773/4204 [20:57<28:49,  1.41it/s]
em: 0.2189, f1: 0.2546, loss: 3.5888 ||:  43%|####2     | 1788/4204 [21:08<28:30,  1.41it/s]
em: 0.2189, f1: 0.2547, loss: 3.5888 ||:  43%|####2     | 1803/4204 [21:18<28:11,  1.42it/s]
em: 0.2189, f1: 0.2547, loss: 3.5863 ||:  43%|####3     | 1818/4204 [21:29<28:15,  1.41it/s]
em: 0.2189, f1: 0.2547, loss: 3.5887 ||:  44%|####3     | 1833/4204 [21:39<27:54,  1.42it/s]
em: 0.2190, f1: 0.2548, loss: 3.5889 ||:  44%|####3     | 1848/4204 [21:50<27:45,  1.41it/s]
em: 0.2192, f1: 0.2550, loss: 3.5892 ||:  44%|####4     | 1863/4204 [22:01<27:42,  1.41it/s]
em: 0.2194, f1: 0.2553, loss: 3.5894 ||:  45%|####4     | 1878/4204 [22:11<27:31,  1.41it/s]
em: 0.2196, f1: 0.2554, loss: 3.5890 ||:  45%|####5     | 1893/4204 [22:22<27:11,  1.42it/s]
em: 0.2194, f1: 0.2552, loss: 3.5887 ||:  45%|####5     | 1908/4204 [22:32<26:39,  1.44it/s]
em: 0.2191, f1: 0.2549, loss: 3.5874 ||:  46%|####5     | 1923/4204 [22:43<26:50,  1.42it/s]
em: 0.2189, f1: 0.2548, loss: 3.5868 ||:  46%|####6     | 1938/4204 [22:53<26:16,  1.44it/s]
em: 0.2189, f1: 0.2548, loss: 3.5860 ||:  46%|####6     | 1953/4204 [23:04<26:10,  1.43it/s]
em: 0.2190, f1: 0.2550, loss: 3.5822 ||:  47%|####6     | 1968/4204 [23:15<26:28,  1.41it/s]
em: 0.2188, f1: 0.2548, loss: 3.5837 ||:  47%|####7     | 1982/4204 [23:25<26:24,  1.40it/s]
em: 0.2186, f1: 0.2547, loss: 3.5839 ||:  47%|####7     | 1996/4204 [23:35<26:18,  1.40it/s]
em: 0.2189, f1: 0.2549, loss: 3.5835 ||:  48%|####7     | 2011/4204 [23:45<26:07,  1.40it/s]
em: 0.2188, f1: 0.2548, loss: 3.5816 ||:  48%|####8     | 2025/4204 [23:56<26:13,  1.38it/s]
em: 0.2186, f1: 0.2547, loss: 3.5808 ||:  49%|####8     | 2039/4204 [24:06<26:00,  1.39it/s]
em: 0.2188, f1: 0.2551, loss: 3.5805 ||:  49%|####8     | 2053/4204 [24:17<26:17,  1.36it/s]
em: 0.2189, f1: 0.2552, loss: 3.5797 ||:  49%|####9     | 2068/4204 [24:27<25:26,  1.40it/s]
em: 0.2189, f1: 0.2553, loss: 3.5794 ||:  50%|####9     | 2083/4204 [24:38<25:26,  1.39it/s]
em: 0.2191, f1: 0.2555, loss: 3.5777 ||:  50%|####9     | 2097/4204 [24:48<25:20,  1.39it/s]
em: 0.2191, f1: 0.2555, loss: 3.5765 ||:  50%|#####     | 2112/4204 [24:58<24:56,  1.40it/s]
em: 0.2193, f1: 0.2556, loss: 3.5776 ||:  51%|#####     | 2127/4204 [25:09<24:33,  1.41it/s]
em: 0.2195, f1: 0.2558, loss: 3.5776 ||:  51%|#####     | 2142/4204 [25:20<24:45,  1.39it/s]
em: 0.2196, f1: 0.2559, loss: 3.5794 ||:  51%|#####1    | 2157/4204 [25:30<24:21,  1.40it/s]
em: 0.2197, f1: 0.2560, loss: 3.5793 ||:  52%|#####1    | 2172/4204 [25:42<24:33,  1.38it/s]
em: 0.2203, f1: 0.2565, loss: 3.5758 ||:  52%|#####2    | 2187/4204 [25:52<24:10,  1.39it/s]
em: 0.2203, f1: 0.2566, loss: 3.5747 ||:  52%|#####2    | 2202/4204 [26:03<23:53,  1.40it/s]
em: 0.2203, f1: 0.2567, loss: 3.5747 ||:  53%|#####2    | 2217/4204 [26:13<23:35,  1.40it/s]
em: 0.2204, f1: 0.2569, loss: 3.5727 ||:  53%|#####3    | 2232/4204 [26:23<22:59,  1.43it/s]
em: 0.2208, f1: 0.2573, loss: 3.5692 ||:  53%|#####3    | 2247/4204 [26:34<22:58,  1.42it/s]
em: 0.2208, f1: 0.2574, loss: 3.5683 ||:  54%|#####3    | 2262/4204 [26:45<22:42,  1.42it/s]
em: 0.2209, f1: 0.2574, loss: 3.5677 ||:  54%|#####4    | 2277/4204 [26:55<22:34,  1.42it/s]
em: 0.2208, f1: 0.2572, loss: 3.5679 ||:  55%|#####4    | 2292/4204 [27:06<22:20,  1.43it/s]
em: 0.2208, f1: 0.2572, loss: 3.5675 ||:  55%|#####4    | 2307/4204 [27:16<21:54,  1.44it/s]
em: 0.2213, f1: 0.2576, loss: 3.5657 ||:  55%|#####5    | 2323/4204 [27:26<21:29,  1.46it/s]
em: 0.2214, f1: 0.2577, loss: 3.5661 ||:  56%|#####5    | 2338/4204 [27:37<21:17,  1.46it/s]
em: 0.2213, f1: 0.2577, loss: 3.5667 ||:  56%|#####5    | 2353/4204 [27:48<21:26,  1.44it/s]
em: 0.2210, f1: 0.2573, loss: 3.5672 ||:  56%|#####6    | 2367/4204 [27:58<21:37,  1.42it/s]
em: 0.2210, f1: 0.2573, loss: 3.5656 ||:  57%|#####6    | 2382/4204 [28:08<21:15,  1.43it/s]
em: 0.2211, f1: 0.2574, loss: 3.5637 ||:  57%|#####7    | 2397/4204 [28:19<21:07,  1.43it/s]
em: 0.2212, f1: 0.2575, loss: 3.5621 ||:  57%|#####7    | 2412/4204 [28:29<21:02,  1.42it/s]
em: 0.2213, f1: 0.2577, loss: 3.5639 ||:  58%|#####7    | 2427/4204 [28:40<20:52,  1.42it/s]
em: 0.2211, f1: 0.2576, loss: 3.5660 ||:  58%|#####8    | 2442/4204 [28:51<20:59,  1.40it/s]
em: 0.2210, f1: 0.2574, loss: 3.5673 ||:  58%|#####8    | 2456/4204 [29:02<21:10,  1.38it/s]
em: 0.2213, f1: 0.2577, loss: 3.5668 ||:  59%|#####8    | 2471/4204 [29:12<21:00,  1.37it/s]
em: 0.2211, f1: 0.2574, loss: 3.5677 ||:  59%|#####9    | 2487/4204 [29:23<20:06,  1.42it/s]
em: 0.2211, f1: 0.2575, loss: 3.5682 ||:  60%|#####9    | 2503/4204 [29:34<19:45,  1.43it/s]
em: 0.2211, f1: 0.2574, loss: 3.5681 ||:  60%|#####9    | 2519/4204 [29:44<19:11,  1.46it/s]
em: 0.2213, f1: 0.2577, loss: 3.5675 ||:  60%|######    | 2535/4204 [29:55<19:00,  1.46it/s]
em: 0.2213, f1: 0.2576, loss: 3.5671 ||:  61%|######    | 2550/4204 [30:05<18:56,  1.46it/s]
em: 0.2215, f1: 0.2578, loss: 3.5681 ||:  61%|######1   | 2566/4204 [30:16<18:21,  1.49it/s]
em: 0.2218, f1: 0.2580, loss: 3.5669 ||:  61%|######1   | 2582/4204 [30:27<18:17,  1.48it/s]
em: 0.2222, f1: 0.2585, loss: 3.5640 ||:  62%|######1   | 2597/4204 [30:38<18:31,  1.45it/s]
em: 0.2226, f1: 0.2588, loss: 3.5613 ||:  62%|######2   | 2612/4204 [30:48<18:26,  1.44it/s]
em: 0.2225, f1: 0.2587, loss: 3.5617 ||:  62%|######2   | 2627/4204 [30:59<18:27,  1.42it/s]
em: 0.2227, f1: 0.2589, loss: 3.5616 ||:  63%|######2   | 2641/4204 [31:09<18:36,  1.40it/s]
em: 0.2230, f1: 0.2592, loss: 3.5615 ||:  63%|######3   | 2656/4204 [31:20<18:09,  1.42it/s]
em: 0.2233, f1: 0.2594, loss: 3.5585 ||:  64%|######3   | 2671/4204 [31:30<17:42,  1.44it/s]
em: 0.2235, f1: 0.2597, loss: 3.5575 ||:  64%|######3   | 2687/4204 [31:40<17:19,  1.46it/s]
em: 0.2235, f1: 0.2596, loss: 3.5562 ||:  64%|######4   | 2703/4204 [31:52<17:26,  1.43it/s]
em: 0.2236, f1: 0.2596, loss: 3.5563 ||:  65%|######4   | 2718/4204 [32:02<17:15,  1.44it/s]
em: 0.2235, f1: 0.2596, loss: 3.5546 ||:  65%|######5   | 2734/4204 [32:13<16:40,  1.47it/s]
em: 0.2235, f1: 0.2596, loss: 3.5534 ||:  65%|######5   | 2750/4204 [32:25<17:03,  1.42it/s]
em: 0.2233, f1: 0.2593, loss: 3.5555 ||:  66%|######5   | 2764/4204 [32:35<17:10,  1.40it/s]
em: 0.2231, f1: 0.2591, loss: 3.5569 ||:  66%|######6   | 2779/4204 [32:46<16:55,  1.40it/s]
em: 0.2230, f1: 0.2590, loss: 3.5565 ||:  66%|######6   | 2794/4204 [32:56<16:39,  1.41it/s]
em: 0.2229, f1: 0.2589, loss: 3.5568 ||:  67%|######6   | 2809/4204 [33:07<16:36,  1.40it/s]
em: 0.2227, f1: 0.2587, loss: 3.5571 ||:  67%|######7   | 2823/4204 [33:18<16:42,  1.38it/s]
em: 0.2228, f1: 0.2588, loss: 3.5569 ||:  67%|######7   | 2837/4204 [33:28<16:33,  1.38it/s]
em: 0.2229, f1: 0.2589, loss: 3.5565 ||:  68%|######7   | 2851/4204 [33:38<16:30,  1.37it/s]
em: 0.2231, f1: 0.2591, loss: 3.5552 ||:  68%|######8   | 2866/4204 [33:48<15:55,  1.40it/s]
em: 0.2232, f1: 0.2593, loss: 3.5549 ||:  69%|######8   | 2881/4204 [33:59<15:34,  1.42it/s]
em: 0.2234, f1: 0.2595, loss: 3.5540 ||:  69%|######8   | 2897/4204 [34:09<14:58,  1.46it/s]
em: 0.2238, f1: 0.2598, loss: 3.5519 ||:  69%|######9   | 2913/4204 [34:20<14:58,  1.44it/s]
em: 0.2240, f1: 0.2600, loss: 3.5514 ||:  70%|######9   | 2927/4204 [34:30<14:57,  1.42it/s]
em: 0.2241, f1: 0.2601, loss: 3.5526 ||:  70%|######9   | 2942/4204 [34:41<14:41,  1.43it/s]
em: 0.2240, f1: 0.2600, loss: 3.5549 ||:  70%|#######   | 2957/4204 [34:52<14:41,  1.41it/s]
em: 0.2241, f1: 0.2601, loss: 3.5554 ||:  71%|#######   | 2971/4204 [35:02<14:44,  1.39it/s]
em: 0.2243, f1: 0.2603, loss: 3.5571 ||:  71%|#######1  | 2985/4204 [35:12<14:34,  1.39it/s]
em: 0.2245, f1: 0.2605, loss: 3.5572 ||:  71%|#######1  | 3000/4204 [35:23<14:18,  1.40it/s]
em: 0.2248, f1: 0.2610, loss: 3.5581 ||:  72%|#######1  | 3015/4204 [35:34<14:22,  1.38it/s]
em: 0.2248, f1: 0.2610, loss: 3.5579 ||:  72%|#######2  | 3029/4204 [35:44<14:08,  1.38it/s]
em: 0.2247, f1: 0.2609, loss: 3.5574 ||:  72%|#######2  | 3043/4204 [35:54<13:58,  1.38it/s]
em: 0.2247, f1: 0.2608, loss: 3.5585 ||:  73%|#######2  | 3058/4204 [36:04<13:34,  1.41it/s]
em: 0.2246, f1: 0.2608, loss: 3.5591 ||:  73%|#######3  | 3073/4204 [36:15<13:23,  1.41it/s]
em: 0.2249, f1: 0.2610, loss: 3.5574 ||:  73%|#######3  | 3088/4204 [36:26<13:14,  1.41it/s]
em: 0.2247, f1: 0.2608, loss: 3.5560 ||:  74%|#######3  | 3103/4204 [36:36<13:00,  1.41it/s]
em: 0.2248, f1: 0.2609, loss: 3.5555 ||:  74%|#######4  | 3118/4204 [36:47<12:42,  1.42it/s]
em: 0.2251, f1: 0.2613, loss: 3.5533 ||:  75%|#######4  | 3133/4204 [36:58<12:42,  1.41it/s]
em: 0.2253, f1: 0.2614, loss: 3.5505 ||:  75%|#######4  | 3148/4204 [37:08<12:27,  1.41it/s]
em: 0.2254, f1: 0.2615, loss: 3.5494 ||:  75%|#######5  | 3163/4204 [37:19<12:16,  1.41it/s]
em: 0.2253, f1: 0.2613, loss: 3.5494 ||:  76%|#######5  | 3178/4204 [37:30<12:15,  1.39it/s]
em: 0.2254, f1: 0.2614, loss: 3.5498 ||:  76%|#######5  | 3193/4204 [37:40<11:57,  1.41it/s]
em: 0.2257, f1: 0.2617, loss: 3.5479 ||:  76%|#######6  | 3208/4204 [37:51<11:49,  1.40it/s]
em: 0.2258, f1: 0.2619, loss: 3.5471 ||:  77%|#######6  | 3223/4204 [38:01<11:31,  1.42it/s]
em: 0.2260, f1: 0.2620, loss: 3.5458 ||:  77%|#######7  | 3238/4204 [38:12<11:28,  1.40it/s]
em: 0.2259, f1: 0.2619, loss: 3.5463 ||:  77%|#######7  | 3253/4204 [38:23<11:11,  1.42it/s]
em: 0.2261, f1: 0.2620, loss: 3.5452 ||:  78%|#######7  | 3268/4204 [38:33<11:02,  1.41it/s]
em: 0.2262, f1: 0.2621, loss: 3.5445 ||:  78%|#######8  | 3284/4204 [38:44<10:36,  1.44it/s]
em: 0.2263, f1: 0.2622, loss: 3.5426 ||:  78%|#######8  | 3300/4204 [38:54<10:15,  1.47it/s]
em: 0.2263, f1: 0.2621, loss: 3.5432 ||:  79%|#######8  | 3316/4204 [39:06<10:13,  1.45it/s]
em: 0.2263, f1: 0.2621, loss: 3.5430 ||:  79%|#######9  | 3331/4204 [39:16<10:08,  1.44it/s]
em: 0.2262, f1: 0.2619, loss: 3.5427 ||:  80%|#######9  | 3346/4204 [39:27<09:56,  1.44it/s]
em: 0.2260, f1: 0.2618, loss: 3.5419 ||:  80%|#######9  | 3362/4204 [39:37<09:38,  1.46it/s]
em: 0.2261, f1: 0.2619, loss: 3.5403 ||:  80%|########  | 3377/4204 [39:48<09:33,  1.44it/s]
em: 0.2264, f1: 0.2622, loss: 3.5376 ||:  81%|########  | 3392/4204 [39:58<09:23,  1.44it/s]
em: 0.2270, f1: 0.2628, loss: 3.5348 ||:  81%|########1 | 3407/4204 [40:09<09:17,  1.43it/s]
em: 0.2274, f1: 0.2631, loss: 3.5335 ||:  81%|########1 | 3422/4204 [40:19<09:05,  1.43it/s]
em: 0.2275, f1: 0.2633, loss: 3.5328 ||:  82%|########1 | 3437/4204 [40:30<08:55,  1.43it/s]
em: 0.2275, f1: 0.2632, loss: 3.5325 ||:  82%|########2 | 3452/4204 [40:41<08:46,  1.43it/s]
em: 0.2275, f1: 0.2633, loss: 3.5332 ||:  82%|########2 | 3467/4204 [40:52<08:46,  1.40it/s]
em: 0.2278, f1: 0.2636, loss: 3.5336 ||:  83%|########2 | 3482/4204 [41:02<08:33,  1.41it/s]
em: 0.2280, f1: 0.2638, loss: 3.5330 ||:  83%|########3 | 3497/4204 [41:13<08:29,  1.39it/s]
em: 0.2281, f1: 0.2640, loss: 3.5328 ||:  84%|########3 | 3511/4204 [41:24<08:22,  1.38it/s]
em: 0.2280, f1: 0.2639, loss: 3.5328 ||:  84%|########3 | 3525/4204 [41:34<08:16,  1.37it/s]
em: 0.2280, f1: 0.2638, loss: 3.5338 ||:  84%|########4 | 3539/4204 [41:44<08:04,  1.37it/s]
em: 0.2283, f1: 0.2641, loss: 3.5327 ||:  85%|########4 | 3554/4204 [41:55<07:45,  1.40it/s]
em: 0.2284, f1: 0.2642, loss: 3.5329 ||:  85%|########4 | 3569/4204 [42:05<07:36,  1.39it/s]
em: 0.2286, f1: 0.2644, loss: 3.5318 ||:  85%|########5 | 3583/4204 [42:15<07:26,  1.39it/s]
em: 0.2289, f1: 0.2648, loss: 3.5312 ||:  86%|########5 | 3599/4204 [42:26<07:02,  1.43it/s]
em: 0.2289, f1: 0.2648, loss: 3.5301 ||:  86%|########5 | 3615/4204 [42:38<06:56,  1.41it/s]
em: 0.2290, f1: 0.2649, loss: 3.5293 ||:  86%|########6 | 3630/4204 [42:48<06:45,  1.42it/s]
em: 0.2293, f1: 0.2652, loss: 3.5288 ||:  87%|########6 | 3645/4204 [42:59<06:40,  1.40it/s]
em: 0.2295, f1: 0.2654, loss: 3.5304 ||:  87%|########7 | 3661/4204 [43:10<06:21,  1.42it/s]
em: 0.2293, f1: 0.2652, loss: 3.5317 ||:  87%|########7 | 3676/4204 [43:21<06:11,  1.42it/s]
em: 0.2295, f1: 0.2653, loss: 3.5316 ||:  88%|########7 | 3691/4204 [43:31<06:01,  1.42it/s]
em: 0.2294, f1: 0.2653, loss: 3.5308 ||:  88%|########8 | 3706/4204 [43:41<05:48,  1.43it/s]
em: 0.2294, f1: 0.2653, loss: 3.5290 ||:  89%|########8 | 3721/4204 [43:53<05:43,  1.40it/s]
em: 0.2296, f1: 0.2655, loss: 3.5276 ||:  89%|########8 | 3735/4204 [44:03<05:37,  1.39it/s]
em: 0.2298, f1: 0.2656, loss: 3.5265 ||:  89%|########9 | 3750/4204 [44:13<05:23,  1.41it/s]
em: 0.2299, f1: 0.2658, loss: 3.5245 ||:  90%|########9 | 3765/4204 [44:24<05:12,  1.41it/s]
em: 0.2298, f1: 0.2657, loss: 3.5247 ||:  90%|########9 | 3780/4204 [44:35<05:01,  1.41it/s]
em: 0.2296, f1: 0.2655, loss: 3.5248 ||:  90%|######### | 3795/4204 [44:45<04:52,  1.40it/s]
em: 0.2297, f1: 0.2656, loss: 3.5243 ||:  91%|######### | 3809/4204 [44:55<04:42,  1.40it/s]
em: 0.2300, f1: 0.2659, loss: 3.5233 ||:  91%|######### | 3823/4204 [45:06<04:34,  1.39it/s]
em: 0.2303, f1: 0.2662, loss: 3.5228 ||:  91%|#########1| 3838/4204 [45:16<04:18,  1.42it/s]
em: 0.2302, f1: 0.2660, loss: 3.5217 ||:  92%|#########1| 3853/4204 [45:27<04:11,  1.39it/s]
em: 0.2303, f1: 0.2661, loss: 3.5218 ||:  92%|#########1| 3867/4204 [45:37<04:03,  1.38it/s]
em: 0.2302, f1: 0.2661, loss: 3.5223 ||:  92%|#########2| 3881/4204 [45:47<03:53,  1.38it/s]
em: 0.2303, f1: 0.2663, loss: 3.5222 ||:  93%|#########2| 3896/4204 [45:58<03:40,  1.40it/s]
em: 0.2302, f1: 0.2661, loss: 3.5236 ||:  93%|#########3| 3911/4204 [46:09<03:29,  1.40it/s]
em: 0.2299, f1: 0.2659, loss: 3.5263 ||:  93%|#########3| 3926/4204 [46:20<03:21,  1.38it/s]
em: 0.2298, f1: 0.2658, loss: 3.5285 ||:  94%|#########3| 3940/4204 [46:30<03:14,  1.36it/s]
em: 0.2295, f1: 0.2657, loss: 3.5300 ||:  94%|#########4| 3954/4204 [46:41<03:03,  1.37it/s]
em: 0.2295, f1: 0.2657, loss: 3.5320 ||:  94%|#########4| 3970/4204 [46:51<02:45,  1.42it/s]
em: 0.2294, f1: 0.2655, loss: 3.5338 ||:  95%|#########4| 3986/4204 [47:02<02:33,  1.42it/s]
em: 0.2290, f1: 0.2653, loss: 3.5374 ||:  95%|#########5| 4001/4204 [47:13<02:23,  1.42it/s]
em: 0.2286, f1: 0.2649, loss: 3.5406 ||:  96%|#########5| 4016/4204 [47:23<02:12,  1.42it/s]
em: 0.2283, f1: 0.2646, loss: 3.5442 ||:  96%|#########5| 4031/4204 [47:35<02:05,  1.38it/s]
em: 0.2281, f1: 0.2644, loss: 3.5474 ||:  96%|#########6| 4045/4204 [47:45<01:54,  1.38it/s]
em: 0.2278, f1: 0.2641, loss: 3.5506 ||:  97%|#########6| 4060/4204 [47:55<01:43,  1.39it/s]
em: 0.2275, f1: 0.2638, loss: 3.5532 ||:  97%|#########6| 4075/4204 [48:06<01:32,  1.39it/s]
em: 0.2271, f1: 0.2635, loss: 3.5565 ||:  97%|#########7| 4090/4204 [48:16<01:20,  1.42it/s]
em: 0.2268, f1: 0.2631, loss: 3.5595 ||:  98%|#########7| 4105/4204 [48:27<01:10,  1.40it/s]
em: 0.2266, f1: 0.2630, loss: 3.5616 ||:  98%|#########8| 4120/4204 [48:37<00:58,  1.43it/s]
em: 0.2262, f1: 0.2626, loss: 3.5653 ||:  98%|#########8| 4135/4204 [48:48<00:48,  1.41it/s]
em: 0.2258, f1: 0.2623, loss: 3.5680 ||:  99%|#########8| 4150/4204 [48:59<00:38,  1.41it/s]
em: 0.2254, f1: 0.2619, loss: 3.5712 ||:  99%|#########9| 4165/4204 [49:10<00:27,  1.41it/s]
em: 0.2252, f1: 0.2618, loss: 3.5734 ||:  99%|#########9| 4180/4204 [49:20<00:16,  1.43it/s]
em: 0.2249, f1: 0.2615, loss: 3.5763 ||: 100%|#########9| 4195/4204 [49:31<00:06,  1.41it/s]
em: 0.2246, f1: 0.2613, loss: 3.5789 ||: : 4209it [49:41,  1.40it/s]                        
em: 0.2244, f1: 0.2611, loss: 3.5811 ||: : 4224it [49:52,  1.40it/s]
em: 0.2241, f1: 0.2609, loss: 3.5832 ||: : 4240it [50:02,  1.42it/s]
em: 0.2240, f1: 0.2608, loss: 3.5853 ||: : 4255it [50:13,  1.40it/s]
em: 0.2238, f1: 0.2606, loss: 3.5862 ||: : 4260it [50:17,  1.41it/s]

2019-05-09 21:44:35,337 - INFO - allennlp.training.trainer - Validating
  0%|          | 0/596 [00:00<?, ?it/s]
em: 0.3594, f1: 0.3746, loss: 1250002.1905 ||:   4%|4         | 24/596 [00:10<03:58,  2.40it/s]
em: 0.3211, f1: 0.3392, loss: 1171877.3558 ||:   8%|8         | 48/596 [00:25<04:26,  2.06it/s]
em: 0.3082, f1: 0.3325, loss: 1382578.1275 ||:  11%|#1        | 66/596 [00:35<04:32,  1.94it/s]
em: 0.2967, f1: 0.3238, loss: 1651788.0216 ||:  14%|#4        | 84/596 [00:47<04:40,  1.82it/s]
em: 0.3064, f1: 0.3316, loss: 1644419.6993 ||:  17%|#7        | 103/596 [00:57<04:29,  1.83it/s]
em: 0.3086, f1: 0.3346, loss: 1700821.9453 ||:  20%|##        | 122/596 [01:08<04:24,  1.79it/s]
em: 0.3073, f1: 0.3332, loss: 1706562.5719 ||:  24%|##3       | 141/596 [01:18<04:09,  1.82it/s]
em: 0.3014, f1: 0.3266, loss: 1691408.6039 ||:  27%|##6       | 160/596 [01:31<04:17,  1.69it/s]
em: 0.2973, f1: 0.3208, loss: 1731845.8865 ||:  30%|###       | 179/596 [01:42<04:02,  1.72it/s]
em: 0.2976, f1: 0.3209, loss: 1698234.6949 ||:  33%|###3      | 198/596 [01:53<03:51,  1.72it/s]
em: 0.3028, f1: 0.3268, loss: 1659979.4095 ||:  37%|###6      | 218/596 [02:04<03:33,  1.77it/s]
em: 0.3011, f1: 0.3251, loss: 1646536.0031 ||:  40%|###9      | 238/596 [02:15<03:20,  1.78it/s]
em: 0.3024, f1: 0.3274, loss: 1585605.5483 ||:  43%|####3     | 257/596 [02:26<03:13,  1.75it/s]
em: 0.3001, f1: 0.3249, loss: 1626371.0589 ||:  46%|####5     | 274/596 [02:36<03:07,  1.72it/s]
em: 0.3037, f1: 0.3285, loss: 1562502.4539 ||:  49%|####8     | 292/596 [02:46<02:54,  1.74it/s]
em: 0.3027, f1: 0.3264, loss: 1576615.3590 ||:  52%|#####2    | 310/596 [02:57<02:47,  1.71it/s]
em: 0.2991, f1: 0.3232, loss: 1589017.6224 ||:  55%|#####5    | 330/596 [03:07<02:29,  1.78it/s]
em: 0.2983, f1: 0.3222, loss: 1612502.4714 ||:  59%|#####8    | 350/596 [03:21<02:25,  1.69it/s]
em: 0.3030, f1: 0.3270, loss: 1558268.0519 ||:  62%|######1   | 369/596 [03:31<02:11,  1.73it/s]
em: 0.3037, f1: 0.3284, loss: 1585054.0193 ||:  65%|######5   | 388/596 [03:43<02:04,  1.68it/s]
em: 0.3010, f1: 0.3250, loss: 1568629.9334 ||:  68%|######8   | 408/596 [03:54<01:48,  1.73it/s]
em: 0.3001, f1: 0.3246, loss: 1550061.0545 ||:  72%|#######1  | 427/596 [04:05<01:38,  1.71it/s]
em: 0.3009, f1: 0.3255, loss: 1514047.4727 ||:  75%|#######4  | 445/596 [04:15<01:27,  1.73it/s]
em: 0.2961, f1: 0.3205, loss: 1546978.7796 ||:  78%|#######7  | 463/596 [04:27<01:19,  1.67it/s]
em: 0.2898, f1: 0.3142, loss: 1682992.1622 ||:  81%|########1 | 485/596 [04:37<01:02,  1.79it/s]
em: 0.2866, f1: 0.3111, loss: 1790277.4713 ||:  85%|########5 | 509/596 [04:47<00:44,  1.94it/s]
em: 0.2845, f1: 0.3092, loss: 1835133.7341 ||:  89%|########9 | 533/596 [05:00<00:32,  1.93it/s]
em: 0.2796, f1: 0.3039, loss: 1911964.7521 ||:  94%|#########3| 558/596 [05:10<00:18,  2.06it/s]
em: 0.2715, f1: 0.2957, loss: 1978990.3618 ||:  98%|#########7| 583/596 [05:24<00:06,  1.96it/s]
em: 0.2674, f1: 0.2920, loss: 1981075.5878 ||: : 601it [05:37,  1.75it/s]                       
em: 0.2669, f1: 0.2915, loss: 1973305.3675 ||: : 604it [05:40,  1.78it/s]

2019-05-09 21:50:15,590 - INFO - allennlp.training.tensorboard_writer -                     Training |  Validation
2019-05-09 21:50:15,591 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB   |  8320.872  |       N/A
2019-05-09 21:50:15,592 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB |  4339.000  |       N/A
2019-05-09 21:50:15,592 - INFO - allennlp.training.tensorboard_writer - loss            |     3.586  |  1973305.367
2019-05-09 21:50:15,592 - INFO - allennlp.training.tensorboard_writer - em              |     0.224  |     0.267
2019-05-09 21:50:15,593 - INFO - allennlp.training.tensorboard_writer - f1              |     0.261  |     0.292
2019-05-09 21:50:20,554 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to 'out/rc_bert_simple/best.th'.
2019-05-09 21:50:25,479 - INFO - allennlp.training.trainer - Epoch duration: 00:56:07
2019-05-09 21:50:25,480 - INFO - allennlp.training.trainer - Estimated training time remaining: 7:42:29
2019-05-09 21:50:25,480 - INFO - allennlp.training.trainer - Epoch 2/9
2019-05-09 21:50:25,480 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 8328.524
2019-05-09 21:50:25,577 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 4339
2019-05-09 21:50:25,584 - INFO - allennlp.training.trainer - Training
  0%|          | 0/4204 [00:00<?, ?it/s]
em: 0.2672, f1: 0.3017, loss: 3.1922 ||:   0%|          | 15/4204 [00:10<48:50,  1.43it/s]
em: 0.2818, f1: 0.3203, loss: 3.1757 ||:   1%|          | 30/4204 [00:21<49:17,  1.41it/s]
em: 0.2716, f1: 0.3054, loss: 3.2155 ||:   1%|1         | 44/4204 [00:31<49:17,  1.41it/s]
em: 0.2831, f1: 0.3117, loss: 3.1723 ||:   1%|1         | 61/4204 [00:41<47:09,  1.46it/s]
em: 0.2711, f1: 0.3002, loss: 3.2199 ||:   2%|1         | 78/4204 [00:53<47:03,  1.46it/s]
em: 0.2623, f1: 0.2937, loss: 3.2773 ||:   2%|2         | 93/4204 [01:04<47:26,  1.44it/s]
em: 0.2635, f1: 0.2942, loss: 3.3347 ||:   3%|2         | 108/4204 [01:15<48:01,  1.42it/s]
em: 0.2682, f1: 0.2990, loss: 3.3344 ||:   3%|2         | 122/4204 [01:25<48:21,  1.41it/s]
em: 0.2676, f1: 0.2995, loss: 3.3448 ||:   3%|3         | 137/4204 [01:35<47:21,  1.43it/s]
em: 0.2683, f1: 0.2994, loss: 3.3203 ||:   4%|3         | 152/4204 [01:46<47:36,  1.42it/s]
em: 0.2698, f1: 0.3031, loss: 3.3157 ||:   4%|3         | 166/4204 [01:56<47:54,  1.40it/s]
em: 0.2725, f1: 0.3070, loss: 3.3048 ||:   4%|4         | 180/4204 [02:06<48:03,  1.40it/s]
em: 0.2766, f1: 0.3110, loss: 3.3011 ||:   5%|4         | 195/4204 [02:17<47:29,  1.41it/s]
em: 0.2778, f1: 0.3113, loss: 3.2899 ||:   5%|4         | 210/4204 [02:27<46:38,  1.43it/s]
em: 0.2773, f1: 0.3107, loss: 3.2905 ||:   5%|5         | 225/4204 [02:37<46:27,  1.43it/s]
em: 0.2793, f1: 0.3132, loss: 3.2809 ||:   6%|5         | 240/4204 [02:48<46:21,  1.43it/s]
em: 0.2824, f1: 0.3169, loss: 3.2764 ||:   6%|6         | 256/4204 [02:58<45:11,  1.46it/s]
em: 0.2796, f1: 0.3139, loss: 3.2854 ||:   6%|6         | 272/4204 [03:10<45:34,  1.44it/s]
em: 0.2765, f1: 0.3114, loss: 3.2934 ||:   7%|6         | 287/4204 [03:20<44:57,  1.45it/s]
em: 0.2778, f1: 0.3119, loss: 3.2965 ||:   7%|7         | 303/4204 [03:30<44:16,  1.47it/s]
em: 0.2754, f1: 0.3090, loss: 3.3086 ||:   8%|7         | 319/4204 [03:43<45:39,  1.42it/s]
em: 0.2732, f1: 0.3071, loss: 3.3177 ||:   8%|7         | 333/4204 [03:53<45:47,  1.41it/s]
em: 0.2745, f1: 0.3090, loss: 3.3090 ||:   8%|8         | 347/4204 [04:04<46:50,  1.37it/s]
em: 0.2724, f1: 0.3072, loss: 3.3311 ||:   9%|8         | 361/4204 [04:14<46:43,  1.37it/s]
em: 0.2721, f1: 0.3068, loss: 3.3354 ||:   9%|8         | 375/4204 [04:24<46:51,  1.36it/s]
em: 0.2687, f1: 0.3032, loss: 3.3376 ||:   9%|9         | 390/4204 [04:34<45:36,  1.39it/s]
em: 0.2667, f1: 0.3008, loss: 3.3492 ||:  10%|9         | 405/4204 [04:45<45:27,  1.39it/s]
em: 0.2663, f1: 0.3005, loss: 3.3562 ||:  10%|9         | 420/4204 [04:56<44:58,  1.40it/s]
em: 0.2647, f1: 0.2989, loss: 3.3661 ||:  10%|#         | 435/4204 [05:07<44:58,  1.40it/s]
em: 0.2663, f1: 0.3006, loss: 3.3521 ||:  11%|#         | 450/4204 [05:17<44:28,  1.41it/s]
em: 0.2665, f1: 0.3008, loss: 3.3488 ||:  11%|#1        | 465/4204 [05:28<44:14,  1.41it/s]
em: 0.2674, f1: 0.3017, loss: 3.3416 ||:  11%|#1        | 480/4204 [05:38<43:20,  1.43it/s]
em: 0.2670, f1: 0.3016, loss: 3.3498 ||:  12%|#1        | 495/4204 [05:48<43:20,  1.43it/s]
em: 0.2656, f1: 0.2998, loss: 3.3501 ||:  12%|#2        | 510/4204 [05:59<43:57,  1.40it/s]
em: 0.2659, f1: 0.3003, loss: 3.3463 ||:  12%|#2        | 525/4204 [06:10<43:28,  1.41it/s]
em: 0.2663, f1: 0.3006, loss: 3.3358 ||:  13%|#2        | 540/4204 [06:20<43:10,  1.41it/s]
em: 0.2685, f1: 0.3027, loss: 3.3249 ||:  13%|#3        | 555/4204 [06:31<43:03,  1.41it/s]
em: 0.2693, f1: 0.3035, loss: 3.3202 ||:  14%|#3        | 571/4204 [06:42<42:26,  1.43it/s]
em: 0.2691, f1: 0.3036, loss: 3.3210 ||:  14%|#3        | 586/4204 [06:53<42:44,  1.41it/s]
em: 0.2677, f1: 0.3024, loss: 3.3215 ||:  14%|#4        | 601/4204 [07:03<42:18,  1.42it/s]
em: 0.2678, f1: 0.3025, loss: 3.3221 ||:  15%|#4        | 616/4204 [07:15<43:01,  1.39it/s]
em: 0.2682, f1: 0.3033, loss: 3.3254 ||:  15%|#5        | 631/4204 [07:25<42:37,  1.40it/s]
em: 0.2692, f1: 0.3044, loss: 3.3227 ||:  15%|#5        | 646/4204 [07:36<41:55,  1.41it/s]
em: 0.2690, f1: 0.3042, loss: 3.3148 ||:  16%|#5        | 661/4204 [07:46<41:45,  1.41it/s]
em: 0.2686, f1: 0.3040, loss: 3.3128 ||:  16%|#6        | 676/4204 [07:57<41:44,  1.41it/s]
em: 0.2684, f1: 0.3038, loss: 3.3190 ||:  16%|#6        | 692/4204 [08:08<40:59,  1.43it/s]
em: 0.2692, f1: 0.3045, loss: 3.3111 ||:  17%|#6        | 708/4204 [08:18<39:50,  1.46it/s]
em: 0.2685, f1: 0.3040, loss: 3.3124 ||:  17%|#7        | 724/4204 [08:29<39:50,  1.46it/s]
em: 0.2675, f1: 0.3031, loss: 3.3111 ||:  18%|#7        | 739/4204 [08:40<40:08,  1.44it/s]
em: 0.2683, f1: 0.3040, loss: 3.3073 ||:  18%|#7        | 755/4204 [08:51<39:26,  1.46it/s]
em: 0.2679, f1: 0.3039, loss: 3.3124 ||:  18%|#8        | 771/4204 [09:02<40:10,  1.42it/s]
em: 0.2689, f1: 0.3048, loss: 3.3073 ||:  19%|#8        | 786/4204 [09:13<39:39,  1.44it/s]
em: 0.2698, f1: 0.3055, loss: 3.3053 ||:  19%|#9        | 801/4204 [09:24<39:57,  1.42it/s]
em: 0.2685, f1: 0.3045, loss: 3.3099 ||:  19%|#9        | 816/4204 [09:34<39:16,  1.44it/s]
em: 0.2680, f1: 0.3038, loss: 3.3146 ||:  20%|#9        | 831/4204 [09:44<39:10,  1.44it/s]
em: 0.2683, f1: 0.3040, loss: 3.3159 ||:  20%|##        | 846/4204 [09:55<38:53,  1.44it/s]
em: 0.2683, f1: 0.3041, loss: 3.3220 ||:  20%|##        | 861/4204 [10:05<38:17,  1.46it/s]
em: 0.2679, f1: 0.3040, loss: 3.3253 ||:  21%|##        | 876/4204 [10:15<38:19,  1.45it/s]
em: 0.2676, f1: 0.3037, loss: 3.3291 ||:  21%|##1       | 891/4204 [10:26<39:02,  1.41it/s]
em: 0.2670, f1: 0.3032, loss: 3.3338 ||:  22%|##1       | 906/4204 [10:37<38:44,  1.42it/s]
em: 0.2676, f1: 0.3038, loss: 3.3324 ||:  22%|##1       | 921/4204 [10:48<39:07,  1.40it/s]
em: 0.2679, f1: 0.3041, loss: 3.3302 ||:  22%|##2       | 936/4204 [10:58<38:17,  1.42it/s]
em: 0.2686, f1: 0.3048, loss: 3.3231 ||:  23%|##2       | 951/4204 [11:08<37:56,  1.43it/s]
em: 0.2684, f1: 0.3046, loss: 3.3211 ||:  23%|##2       | 966/4204 [11:19<37:42,  1.43it/s]
em: 0.2686, f1: 0.3046, loss: 3.3211 ||:  23%|##3       | 981/4204 [11:29<37:28,  1.43it/s]
em: 0.2686, f1: 0.3046, loss: 3.3221 ||:  24%|##3       | 996/4204 [11:40<37:24,  1.43it/s]
em: 0.2693, f1: 0.3054, loss: 3.3196 ||:  24%|##4       | 1011/4204 [11:50<37:07,  1.43it/s]
em: 0.2698, f1: 0.3056, loss: 3.3188 ||:  24%|##4       | 1026/4204 [12:01<37:25,  1.42it/s]
em: 0.2698, f1: 0.3058, loss: 3.3216 ||:  25%|##4       | 1041/4204 [12:12<37:05,  1.42it/s]
em: 0.2703, f1: 0.3064, loss: 3.3275 ||:  25%|##5       | 1056/4204 [12:22<36:34,  1.43it/s]
em: 0.2704, f1: 0.3066, loss: 3.3258 ||:  25%|##5       | 1071/4204 [12:32<36:20,  1.44it/s]
em: 0.2705, f1: 0.3067, loss: 3.3240 ||:  26%|##5       | 1086/4204 [12:43<36:26,  1.43it/s]
em: 0.2696, f1: 0.3058, loss: 3.3277 ||:  26%|##6       | 1101/4204 [12:54<36:46,  1.41it/s]
em: 0.2690, f1: 0.3051, loss: 3.3331 ||:  27%|##6       | 1116/4204 [13:04<36:31,  1.41it/s]
em: 0.2694, f1: 0.3055, loss: 3.3321 ||:  27%|##6       | 1131/4204 [13:15<36:24,  1.41it/s]
em: 0.2692, f1: 0.3053, loss: 3.3330 ||:  27%|##7       | 1146/4204 [13:27<36:57,  1.38it/s]
em: 0.2696, f1: 0.3057, loss: 3.3338 ||:  28%|##7       | 1160/4204 [13:37<37:00,  1.37it/s]
em: 0.2691, f1: 0.3053, loss: 3.3353 ||:  28%|##7       | 1176/4204 [13:48<35:53,  1.41it/s]
em: 0.2689, f1: 0.3050, loss: 3.3342 ||:  28%|##8       | 1191/4204 [13:58<35:53,  1.40it/s]
em: 0.2694, f1: 0.3055, loss: 3.3275 ||:  29%|##8       | 1205/4204 [14:09<36:00,  1.39it/s]
em: 0.2704, f1: 0.3064, loss: 3.3276 ||:  29%|##9       | 1220/4204 [14:19<35:25,  1.40it/s]
em: 0.2700, f1: 0.3058, loss: 3.3276 ||:  29%|##9       | 1235/4204 [14:30<35:17,  1.40it/s]
em: 0.2701, f1: 0.3060, loss: 3.3276 ||:  30%|##9       | 1250/4204 [14:40<34:54,  1.41it/s]
em: 0.2699, f1: 0.3057, loss: 3.3282 ||:  30%|###       | 1265/4204 [14:51<35:07,  1.39it/s]
em: 0.2697, f1: 0.3054, loss: 3.3285 ||:  30%|###       | 1280/4204 [15:02<34:51,  1.40it/s]
em: 0.2703, f1: 0.3061, loss: 3.3247 ||:  31%|###       | 1295/4204 [15:13<34:59,  1.39it/s]
em: 0.2704, f1: 0.3061, loss: 3.3238 ||:  31%|###1      | 1309/4204 [15:23<34:43,  1.39it/s]
em: 0.2705, f1: 0.3062, loss: 3.3269 ||:  32%|###1      | 1325/4204 [15:34<33:46,  1.42it/s]
em: 0.2699, f1: 0.3058, loss: 3.3294 ||:  32%|###1      | 1340/4204 [15:45<33:51,  1.41it/s]
em: 0.2698, f1: 0.3058, loss: 3.3283 ||:  32%|###2      | 1356/4204 [15:55<32:56,  1.44it/s]
em: 0.2700, f1: 0.3063, loss: 3.3251 ||:  33%|###2      | 1372/4204 [16:06<33:00,  1.43it/s]
em: 0.2699, f1: 0.3062, loss: 3.3248 ||:  33%|###2      | 1387/4204 [16:18<33:28,  1.40it/s]
em: 0.2700, f1: 0.3064, loss: 3.3255 ||:  33%|###3      | 1401/4204 [16:28<33:30,  1.39it/s]
em: 0.2704, f1: 0.3068, loss: 3.3241 ||:  34%|###3      | 1415/4204 [16:38<33:35,  1.38it/s]
em: 0.2706, f1: 0.3070, loss: 3.3234 ||:  34%|###3      | 1429/4204 [16:48<33:25,  1.38it/s]
em: 0.2705, f1: 0.3069, loss: 3.3214 ||:  34%|###4      | 1443/4204 [16:59<33:22,  1.38it/s]
em: 0.2705, f1: 0.3068, loss: 3.3209 ||:  35%|###4      | 1457/4204 [17:09<33:37,  1.36it/s]
em: 0.2700, f1: 0.3063, loss: 3.3207 ||:  35%|###4      | 1471/4204 [17:19<33:26,  1.36it/s]
em: 0.2705, f1: 0.3070, loss: 3.3212 ||:  35%|###5      | 1486/4204 [17:30<32:57,  1.37it/s]
em: 0.2703, f1: 0.3068, loss: 3.3184 ||:  36%|###5      | 1501/4204 [17:41<32:52,  1.37it/s]
em: 0.2699, f1: 0.3065, loss: 3.3182 ||:  36%|###6      | 1515/4204 [17:51<32:32,  1.38it/s]
em: 0.2702, f1: 0.3069, loss: 3.3167 ||:  36%|###6      | 1530/4204 [18:01<31:37,  1.41it/s]
em: 0.2703, f1: 0.3071, loss: 3.3194 ||:  37%|###6      | 1545/4204 [18:12<31:21,  1.41it/s]
em: 0.2704, f1: 0.3071, loss: 3.3192 ||:  37%|###7      | 1560/4204 [18:22<31:14,  1.41it/s]
em: 0.2706, f1: 0.3073, loss: 3.3175 ||:  37%|###7      | 1575/4204 [18:34<31:46,  1.38it/s]
em: 0.2710, f1: 0.3076, loss: 3.3158 ||:  38%|###7      | 1590/4204 [18:44<31:06,  1.40it/s]
em: 0.2705, f1: 0.3070, loss: 3.3160 ||:  38%|###8      | 1605/4204 [18:54<30:31,  1.42it/s]
em: 0.2701, f1: 0.3065, loss: 3.3178 ||:  39%|###8      | 1620/4204 [19:05<29:59,  1.44it/s]
em: 0.2702, f1: 0.3067, loss: 3.3167 ||:  39%|###8      | 1635/4204 [19:15<30:08,  1.42it/s]
em: 0.2703, f1: 0.3066, loss: 3.3179 ||:  39%|###9      | 1650/4204 [19:26<29:35,  1.44it/s]
em: 0.2700, f1: 0.3062, loss: 3.3191 ||:  40%|###9      | 1665/4204 [19:36<29:34,  1.43it/s]
em: 0.2701, f1: 0.3063, loss: 3.3186 ||:  40%|###9      | 1680/4204 [19:47<29:42,  1.42it/s]
em: 0.2702, f1: 0.3063, loss: 3.3179 ||:  40%|####      | 1694/4204 [19:57<29:45,  1.41it/s]
em: 0.2700, f1: 0.3062, loss: 3.3183 ||:  41%|####      | 1709/4204 [20:07<29:12,  1.42it/s]
em: 0.2699, f1: 0.3062, loss: 3.3181 ||:  41%|####1     | 1724/4204 [20:18<28:50,  1.43it/s]
em: 0.2701, f1: 0.3062, loss: 3.3184 ||:  41%|####1     | 1740/4204 [20:28<28:10,  1.46it/s]
em: 0.2703, f1: 0.3064, loss: 3.3181 ||:  42%|####1     | 1756/4204 [20:40<28:19,  1.44it/s]
em: 0.2699, f1: 0.3060, loss: 3.3174 ||:  42%|####2     | 1771/4204 [20:50<28:26,  1.43it/s]
em: 0.2696, f1: 0.3057, loss: 3.3171 ||:  42%|####2     | 1785/4204 [21:01<28:34,  1.41it/s]
em: 0.2697, f1: 0.3058, loss: 3.3150 ||:  43%|####2     | 1800/4204 [21:11<28:00,  1.43it/s]
em: 0.2693, f1: 0.3053, loss: 3.3144 ||:  43%|####3     | 1815/4204 [21:21<28:04,  1.42it/s]
em: 0.2693, f1: 0.3053, loss: 3.3160 ||:  44%|####3     | 1829/4204 [21:32<28:18,  1.40it/s]
em: 0.2697, f1: 0.3058, loss: 3.3163 ||:  44%|####3     | 1844/4204 [21:42<27:50,  1.41it/s]
em: 0.2694, f1: 0.3053, loss: 3.3174 ||:  44%|####4     | 1859/4204 [21:53<27:47,  1.41it/s]
em: 0.2691, f1: 0.3051, loss: 3.3193 ||:  45%|####4     | 1873/4204 [22:03<27:49,  1.40it/s]
em: 0.2690, f1: 0.3049, loss: 3.3186 ||:  45%|####4     | 1888/4204 [22:14<27:33,  1.40it/s]
em: 0.2698, f1: 0.3056, loss: 3.3172 ||:  45%|####5     | 1903/4204 [22:24<26:59,  1.42it/s]
em: 0.2697, f1: 0.3056, loss: 3.3150 ||:  46%|####5     | 1918/4204 [22:34<26:37,  1.43it/s]
em: 0.2696, f1: 0.3054, loss: 3.3134 ||:  46%|####5     | 1933/4204 [22:45<26:30,  1.43it/s]
em: 0.2690, f1: 0.3048, loss: 3.3148 ||:  46%|####6     | 1948/4204 [22:56<26:33,  1.42it/s]
em: 0.2690, f1: 0.3048, loss: 3.3138 ||:  47%|####6     | 1962/4204 [23:06<26:43,  1.40it/s]
em: 0.2693, f1: 0.3049, loss: 3.3120 ||:  47%|####7     | 1977/4204 [23:17<26:26,  1.40it/s]
em: 0.2691, f1: 0.3048, loss: 3.3122 ||:  47%|####7     | 1992/4204 [23:28<26:47,  1.38it/s]
em: 0.2688, f1: 0.3045, loss: 3.3150 ||:  48%|####7     | 2008/4204 [23:39<25:53,  1.41it/s]
em: 0.2685, f1: 0.3042, loss: 3.3162 ||:  48%|####8     | 2024/4204 [23:50<25:58,  1.40it/s]
em: 0.2685, f1: 0.3044, loss: 3.3152 ||:  48%|####8     | 2038/4204 [24:01<26:05,  1.38it/s]
em: 0.2689, f1: 0.3048, loss: 3.3111 ||:  49%|####8     | 2053/4204 [24:11<25:56,  1.38it/s]
em: 0.2687, f1: 0.3046, loss: 3.3128 ||:  49%|####9     | 2068/4204 [24:22<25:40,  1.39it/s]
em: 0.2687, f1: 0.3047, loss: 3.3123 ||:  50%|####9     | 2083/4204 [24:33<25:14,  1.40it/s]
em: 0.2686, f1: 0.3045, loss: 3.3113 ||:  50%|####9     | 2098/4204 [24:44<25:10,  1.39it/s]
em: 0.2685, f1: 0.3045, loss: 3.3133 ||:  50%|#####     | 2113/4204 [24:54<24:52,  1.40it/s]
em: 0.2686, f1: 0.3045, loss: 3.3122 ||:  51%|#####     | 2128/4204 [25:04<24:25,  1.42it/s]
em: 0.2688, f1: 0.3047, loss: 3.3147 ||:  51%|#####     | 2143/4204 [25:16<24:38,  1.39it/s]
em: 0.2688, f1: 0.3048, loss: 3.3142 ||:  51%|#####1    | 2158/4204 [25:26<24:18,  1.40it/s]
em: 0.2687, f1: 0.3047, loss: 3.3145 ||:  52%|#####1    | 2173/4204 [25:37<24:13,  1.40it/s]
em: 0.2687, f1: 0.3049, loss: 3.3133 ||:  52%|#####2    | 2188/4204 [25:48<24:00,  1.40it/s]
em: 0.2690, f1: 0.3052, loss: 3.3118 ||:  52%|#####2    | 2203/4204 [25:59<23:56,  1.39it/s]
em: 0.2692, f1: 0.3053, loss: 3.3109 ||:  53%|#####2    | 2218/4204 [26:09<23:41,  1.40it/s]
em: 0.2690, f1: 0.3052, loss: 3.3107 ||:  53%|#####3    | 2233/4204 [26:19<23:02,  1.43it/s]
em: 0.2695, f1: 0.3058, loss: 3.3096 ||:  53%|#####3    | 2248/4204 [26:30<22:44,  1.43it/s]
em: 0.2696, f1: 0.3059, loss: 3.3084 ||:  54%|#####3    | 2263/4204 [26:40<22:17,  1.45it/s]
em: 0.2697, f1: 0.3059, loss: 3.3070 ||:  54%|#####4    | 2278/4204 [26:50<22:26,  1.43it/s]
em: 0.2697, f1: 0.3059, loss: 3.3066 ||:  55%|#####4    | 2293/4204 [27:01<22:08,  1.44it/s]
em: 0.2695, f1: 0.3058, loss: 3.3073 ||:  55%|#####4    | 2308/4204 [27:11<21:59,  1.44it/s]
em: 0.2694, f1: 0.3057, loss: 3.3060 ||:  55%|#####5    | 2323/4204 [27:22<21:45,  1.44it/s]
em: 0.2693, f1: 0.3056, loss: 3.3067 ||:  56%|#####5    | 2338/4204 [27:32<21:24,  1.45it/s]
em: 0.2694, f1: 0.3056, loss: 3.3072 ||:  56%|#####5    | 2353/4204 [27:42<21:20,  1.45it/s]
em: 0.2694, f1: 0.3056, loss: 3.3051 ||:  56%|#####6    | 2368/4204 [27:53<21:10,  1.45it/s]
em: 0.2690, f1: 0.3053, loss: 3.3038 ||:  57%|#####6    | 2383/4204 [28:04<21:23,  1.42it/s]
em: 0.2689, f1: 0.3052, loss: 3.3046 ||:  57%|#####7    | 2398/4204 [28:14<21:16,  1.41it/s]
em: 0.2695, f1: 0.3057, loss: 3.3041 ||:  57%|#####7    | 2413/4204 [28:25<21:11,  1.41it/s]
em: 0.2693, f1: 0.3056, loss: 3.3037 ||:  58%|#####7    | 2428/4204 [28:35<20:38,  1.43it/s]
em: 0.2692, f1: 0.3055, loss: 3.3067 ||:  58%|#####8    | 2443/4204 [28:46<21:02,  1.39it/s]
em: 0.2688, f1: 0.3053, loss: 3.3087 ||:  58%|#####8    | 2458/4204 [28:57<20:38,  1.41it/s]
em: 0.2689, f1: 0.3053, loss: 3.3082 ||:  59%|#####8    | 2473/4204 [29:08<20:50,  1.38it/s]
em: 0.2688, f1: 0.3052, loss: 3.3072 ||:  59%|#####9    | 2489/4204 [29:19<20:10,  1.42it/s]
em: 0.2687, f1: 0.3051, loss: 3.3092 ||:  60%|#####9    | 2505/4204 [29:29<19:35,  1.44it/s]
em: 0.2684, f1: 0.3047, loss: 3.3110 ||:  60%|#####9    | 2521/4204 [29:41<19:32,  1.44it/s]
em: 0.2685, f1: 0.3048, loss: 3.3107 ||:  60%|######    | 2536/4204 [29:51<19:09,  1.45it/s]
em: 0.2685, f1: 0.3049, loss: 3.3111 ||:  61%|######    | 2551/4204 [30:01<18:57,  1.45it/s]
em: 0.2688, f1: 0.3051, loss: 3.3101 ||:  61%|######1   | 2567/4204 [30:12<18:31,  1.47it/s]
em: 0.2688, f1: 0.3050, loss: 3.3081 ||:  61%|######1   | 2583/4204 [30:22<18:20,  1.47it/s]
em: 0.2689, f1: 0.3052, loss: 3.3071 ||:  62%|######1   | 2598/4204 [30:33<18:23,  1.46it/s]
em: 0.2690, f1: 0.3052, loss: 3.3075 ||:  62%|######2   | 2613/4204 [30:44<18:32,  1.43it/s]
em: 0.2693, f1: 0.3055, loss: 3.3056 ||:  63%|######2   | 2628/4204 [30:55<18:26,  1.42it/s]
em: 0.2694, f1: 0.3056, loss: 3.3043 ||:  63%|######2   | 2643/4204 [31:05<18:16,  1.42it/s]
em: 0.2693, f1: 0.3055, loss: 3.3047 ||:  63%|######3   | 2658/4204 [31:16<18:16,  1.41it/s]
em: 0.2695, f1: 0.3056, loss: 3.3037 ||:  64%|######3   | 2674/4204 [31:27<17:46,  1.43it/s]
em: 0.2699, f1: 0.3060, loss: 3.3018 ||:  64%|######3   | 2689/4204 [31:37<17:33,  1.44it/s]
em: 0.2700, f1: 0.3061, loss: 3.3004 ||:  64%|######4   | 2704/4204 [31:47<17:19,  1.44it/s]
em: 0.2702, f1: 0.3064, loss: 3.2986 ||:  65%|######4   | 2719/4204 [31:58<17:08,  1.44it/s]
em: 0.2699, f1: 0.3061, loss: 3.2987 ||:  65%|######5   | 2734/4204 [32:08<16:49,  1.46it/s]
em: 0.2700, f1: 0.3062, loss: 3.2997 ||:  65%|######5   | 2749/4204 [32:19<16:56,  1.43it/s]
em: 0.2698, f1: 0.3059, loss: 3.3015 ||:  66%|######5   | 2764/4204 [32:29<16:53,  1.42it/s]
em: 0.2694, f1: 0.3055, loss: 3.3022 ||:  66%|######6   | 2778/4204 [32:40<17:13,  1.38it/s]
em: 0.2692, f1: 0.3053, loss: 3.3019 ||:  66%|######6   | 2792/4204 [32:51<17:08,  1.37it/s]
em: 0.2689, f1: 0.3051, loss: 3.3023 ||:  67%|######6   | 2807/4204 [33:01<16:32,  1.41it/s]
em: 0.2689, f1: 0.3052, loss: 3.3037 ||:  67%|######7   | 2822/4204 [33:12<16:33,  1.39it/s]
em: 0.2690, f1: 0.3053, loss: 3.3031 ||:  67%|######7   | 2836/4204 [33:22<16:31,  1.38it/s]
em: 0.2687, f1: 0.3050, loss: 3.3032 ||:  68%|######7   | 2850/4204 [33:32<16:18,  1.38it/s]
em: 0.2687, f1: 0.3050, loss: 3.3039 ||:  68%|######8   | 2865/4204 [33:42<15:52,  1.41it/s]
em: 0.2686, f1: 0.3050, loss: 3.3021 ||:  69%|######8   | 2881/4204 [33:53<15:20,  1.44it/s]
em: 0.2688, f1: 0.3053, loss: 3.3006 ||:  69%|######8   | 2897/4204 [34:04<15:10,  1.44it/s]
em: 0.2690, f1: 0.3054, loss: 3.2992 ||:  69%|######9   | 2912/4204 [34:15<15:05,  1.43it/s]
em: 0.2690, f1: 0.3055, loss: 3.2992 ||:  70%|######9   | 2927/4204 [34:26<15:05,  1.41it/s]
em: 0.2689, f1: 0.3054, loss: 3.3025 ||:  70%|######9   | 2942/4204 [34:36<14:45,  1.43it/s]
em: 0.2689, f1: 0.3054, loss: 3.3039 ||:  70%|#######   | 2957/4204 [34:47<14:43,  1.41it/s]
em: 0.2689, f1: 0.3054, loss: 3.3046 ||:  71%|#######   | 2971/4204 [34:57<14:41,  1.40it/s]
em: 0.2691, f1: 0.3056, loss: 3.3052 ||:  71%|#######1  | 2985/4204 [35:07<14:41,  1.38it/s]
em: 0.2693, f1: 0.3059, loss: 3.3059 ||:  71%|#######1  | 3000/4204 [35:18<14:25,  1.39it/s]
em: 0.2693, f1: 0.3060, loss: 3.3050 ||:  72%|#######1  | 3015/4204 [35:29<14:20,  1.38it/s]
em: 0.2696, f1: 0.3063, loss: 3.3059 ||:  72%|#######2  | 3030/4204 [35:40<14:02,  1.39it/s]
em: 0.2694, f1: 0.3061, loss: 3.3068 ||:  72%|#######2  | 3045/4204 [35:51<13:54,  1.39it/s]
em: 0.2697, f1: 0.3064, loss: 3.3073 ||:  73%|#######2  | 3060/4204 [36:01<13:37,  1.40it/s]
em: 0.2698, f1: 0.3065, loss: 3.3068 ||:  73%|#######3  | 3075/4204 [36:11<13:15,  1.42it/s]
em: 0.2700, f1: 0.3067, loss: 3.3065 ||:  74%|#######3  | 3090/4204 [36:22<13:11,  1.41it/s]
em: 0.2699, f1: 0.3066, loss: 3.3064 ||:  74%|#######3  | 3105/4204 [36:33<12:55,  1.42it/s]
em: 0.2702, f1: 0.3069, loss: 3.3039 ||:  74%|#######4  | 3120/4204 [36:43<12:44,  1.42it/s]
em: 0.2703, f1: 0.3071, loss: 3.3020 ||:  75%|#######4  | 3135/4204 [36:53<12:27,  1.43it/s]
em: 0.2703, f1: 0.3071, loss: 3.3001 ||:  75%|#######4  | 3150/4204 [37:04<12:18,  1.43it/s]
em: 0.2706, f1: 0.3073, loss: 3.3009 ||:  75%|#######5  | 3165/4204 [37:15<12:17,  1.41it/s]
em: 0.2708, f1: 0.3075, loss: 3.3000 ||:  76%|#######5  | 3180/4204 [37:26<12:11,  1.40it/s]
em: 0.2709, f1: 0.3076, loss: 3.2993 ||:  76%|#######5  | 3195/4204 [37:36<11:57,  1.41it/s]
em: 0.2709, f1: 0.3076, loss: 3.2988 ||:  76%|#######6  | 3210/4204 [37:47<11:44,  1.41it/s]
em: 0.2712, f1: 0.3079, loss: 3.2978 ||:  77%|#######6  | 3225/4204 [37:58<11:35,  1.41it/s]
em: 0.2712, f1: 0.3079, loss: 3.2980 ||:  77%|#######7  | 3240/4204 [38:08<11:26,  1.40it/s]
em: 0.2714, f1: 0.3081, loss: 3.2975 ||:  77%|#######7  | 3255/4204 [38:19<11:08,  1.42it/s]
em: 0.2714, f1: 0.3081, loss: 3.2965 ||:  78%|#######7  | 3270/4204 [38:29<10:58,  1.42it/s]
em: 0.2715, f1: 0.3081, loss: 3.2962 ||:  78%|#######8  | 3285/4204 [38:40<10:49,  1.41it/s]
em: 0.2715, f1: 0.3081, loss: 3.2962 ||:  79%|#######8  | 3302/4204 [38:50<10:11,  1.48it/s]
em: 0.2715, f1: 0.3081, loss: 3.2958 ||:  79%|#######8  | 3319/4204 [39:02<10:04,  1.47it/s]
em: 0.2715, f1: 0.3081, loss: 3.2959 ||:  79%|#######9  | 3334/4204 [39:12<09:50,  1.47it/s]
em: 0.2715, f1: 0.3082, loss: 3.2961 ||:  80%|#######9  | 3349/4204 [39:23<09:55,  1.44it/s]
em: 0.2717, f1: 0.3083, loss: 3.2947 ||:  80%|########  | 3365/4204 [39:34<09:36,  1.46it/s]
em: 0.2717, f1: 0.3083, loss: 3.2937 ||:  80%|########  | 3381/4204 [39:45<09:29,  1.45it/s]
em: 0.2719, f1: 0.3085, loss: 3.2930 ||:  81%|########  | 3396/4204 [39:56<09:20,  1.44it/s]
em: 0.2723, f1: 0.3088, loss: 3.2901 ||:  81%|########1 | 3411/4204 [40:06<09:12,  1.43it/s]
em: 0.2724, f1: 0.3088, loss: 3.2899 ||:  81%|########1 | 3426/4204 [40:16<08:56,  1.45it/s]
em: 0.2725, f1: 0.3089, loss: 3.2898 ||:  82%|########1 | 3441/4204 [40:27<08:53,  1.43it/s]
em: 0.2725, f1: 0.3090, loss: 3.2888 ||:  82%|########2 | 3455/4204 [40:37<08:49,  1.42it/s]
em: 0.2725, f1: 0.3090, loss: 3.2894 ||:  83%|########2 | 3469/4204 [40:47<08:45,  1.40it/s]
em: 0.2727, f1: 0.3092, loss: 3.2891 ||:  83%|########2 | 3483/4204 [40:58<08:43,  1.38it/s]
em: 0.2726, f1: 0.3093, loss: 3.2893 ||:  83%|########3 | 3497/4204 [41:08<08:31,  1.38it/s]
em: 0.2725, f1: 0.3091, loss: 3.2886 ||:  84%|########3 | 3511/4204 [41:18<08:24,  1.37it/s]
em: 0.2725, f1: 0.3091, loss: 3.2908 ||:  84%|########3 | 3525/4204 [41:29<08:13,  1.38it/s]
em: 0.2725, f1: 0.3090, loss: 3.2913 ||:  84%|########4 | 3539/4204 [41:39<08:07,  1.37it/s]
em: 0.2724, f1: 0.3090, loss: 3.2918 ||:  85%|########4 | 3553/4204 [41:49<07:55,  1.37it/s]
em: 0.2729, f1: 0.3094, loss: 3.2908 ||:  85%|########4 | 3568/4204 [41:59<07:36,  1.39it/s]
em: 0.2728, f1: 0.3094, loss: 3.2887 ||:  85%|########5 | 3583/4204 [42:10<07:17,  1.42it/s]
em: 0.2731, f1: 0.3097, loss: 3.2872 ||:  86%|########5 | 3598/4204 [42:20<07:08,  1.42it/s]
em: 0.2734, f1: 0.3100, loss: 3.2873 ||:  86%|########5 | 3613/4204 [42:31<06:58,  1.41it/s]
em: 0.2734, f1: 0.3100, loss: 3.2866 ||:  86%|########6 | 3628/4204 [42:41<06:45,  1.42it/s]
em: 0.2735, f1: 0.3101, loss: 3.2873 ||:  87%|########6 | 3643/4204 [42:52<06:34,  1.42it/s]
em: 0.2735, f1: 0.3102, loss: 3.2884 ||:  87%|########7 | 3658/4204 [43:03<06:30,  1.40it/s]
em: 0.2736, f1: 0.3103, loss: 3.2895 ||:  87%|########7 | 3674/4204 [43:14<06:11,  1.43it/s]
em: 0.2736, f1: 0.3102, loss: 3.2910 ||:  88%|########7 | 3689/4204 [43:25<06:05,  1.41it/s]
em: 0.2734, f1: 0.3101, loss: 3.2898 ||:  88%|########8 | 3704/4204 [43:35<05:55,  1.41it/s]
em: 0.2735, f1: 0.3101, loss: 3.2889 ||:  88%|########8 | 3719/4204 [43:46<05:44,  1.41it/s]
em: 0.2740, f1: 0.3106, loss: 3.2874 ||:  89%|########8 | 3734/4204 [43:56<05:30,  1.42it/s]
em: 0.2742, f1: 0.3109, loss: 3.2855 ||:  89%|########9 | 3749/4204 [44:07<05:20,  1.42it/s]
em: 0.2739, f1: 0.3107, loss: 3.2850 ||:  90%|########9 | 3764/4204 [44:17<05:09,  1.42it/s]
em: 0.2738, f1: 0.3106, loss: 3.2839 ||:  90%|########9 | 3779/4204 [44:28<05:01,  1.41it/s]
em: 0.2736, f1: 0.3104, loss: 3.2849 ||:  90%|######### | 3793/4204 [44:38<04:54,  1.40it/s]
em: 0.2738, f1: 0.3106, loss: 3.2857 ||:  91%|######### | 3808/4204 [44:49<04:40,  1.41it/s]
em: 0.2739, f1: 0.3107, loss: 3.2862 ||:  91%|######### | 3823/4204 [45:00<04:33,  1.39it/s]
em: 0.2743, f1: 0.3110, loss: 3.2848 ||:  91%|#########1| 3838/4204 [45:10<04:19,  1.41it/s]
em: 0.2744, f1: 0.3111, loss: 3.2849 ||:  92%|#########1| 3853/4204 [45:21<04:11,  1.40it/s]
em: 0.2743, f1: 0.3111, loss: 3.2856 ||:  92%|#########1| 3867/4204 [45:32<04:04,  1.38it/s]
em: 0.2742, f1: 0.3110, loss: 3.2855 ||:  92%|#########2| 3881/4204 [45:42<03:55,  1.37it/s]
em: 0.2741, f1: 0.3108, loss: 3.2857 ||:  93%|#########2| 3896/4204 [45:52<03:40,  1.39it/s]
em: 0.2742, f1: 0.3109, loss: 3.2863 ||:  93%|#########3| 3911/4204 [46:03<03:29,  1.40it/s]
em: 0.2738, f1: 0.3107, loss: 3.2891 ||:  93%|#########3| 3926/4204 [46:14<03:20,  1.38it/s]
em: 0.2734, f1: 0.3103, loss: 3.2909 ||:  94%|#########3| 3940/4204 [46:24<03:12,  1.37it/s]
em: 0.2731, f1: 0.3100, loss: 3.2930 ||:  94%|#########4| 3955/4204 [46:35<02:59,  1.39it/s]
em: 0.2729, f1: 0.3098, loss: 3.2958 ||:  94%|#########4| 3970/4204 [46:45<02:44,  1.42it/s]
em: 0.2729, f1: 0.3099, loss: 3.2977 ||:  95%|#########4| 3985/4204 [46:56<02:35,  1.41it/s]
em: 0.2725, f1: 0.3094, loss: 3.3017 ||:  95%|#########5| 4000/4204 [47:06<02:22,  1.44it/s]
em: 0.2720, f1: 0.3090, loss: 3.3056 ||:  96%|#########5| 4015/4204 [47:17<02:13,  1.42it/s]
em: 0.2716, f1: 0.3085, loss: 3.3092 ||:  96%|#########5| 4029/4204 [47:27<02:06,  1.38it/s]
em: 0.2711, f1: 0.3081, loss: 3.3124 ||:  96%|#########6| 4043/4204 [47:37<01:56,  1.39it/s]
em: 0.2708, f1: 0.3079, loss: 3.3157 ||:  97%|#########6| 4059/4204 [47:48<01:42,  1.42it/s]
em: 0.2704, f1: 0.3074, loss: 3.3189 ||:  97%|#########6| 4075/4204 [48:00<01:33,  1.38it/s]
em: 0.2700, f1: 0.3070, loss: 3.3217 ||:  97%|#########7| 4090/4204 [48:11<01:22,  1.39it/s]
em: 0.2695, f1: 0.3066, loss: 3.3244 ||:  98%|#########7| 4105/4204 [48:21<01:10,  1.40it/s]
em: 0.2691, f1: 0.3063, loss: 3.3276 ||:  98%|#########8| 4120/4204 [48:32<00:59,  1.41it/s]
em: 0.2685, f1: 0.3058, loss: 3.3317 ||:  98%|#########8| 4135/4204 [48:42<00:48,  1.42it/s]
em: 0.2682, f1: 0.3055, loss: 3.3338 ||:  99%|#########8| 4151/4204 [48:53<00:36,  1.44it/s]
em: 0.2678, f1: 0.3051, loss: 3.3377 ||:  99%|#########9| 4166/4204 [49:03<00:26,  1.44it/s]
em: 0.2673, f1: 0.3046, loss: 3.3412 ||:  99%|#########9| 4181/4204 [49:15<00:16,  1.40it/s]
em: 0.2670, f1: 0.3043, loss: 3.3448 ||: 100%|#########9| 4196/4204 [49:26<00:05,  1.40it/s]
em: 0.2665, f1: 0.3038, loss: 3.3474 ||: : 4210it [49:36,  1.40it/s]                        
em: 0.2662, f1: 0.3036, loss: 3.3492 ||: : 4224it [49:46,  1.38it/s]
em: 0.2660, f1: 0.3034, loss: 3.3509 ||: : 4239it [49:56,  1.41it/s]
em: 0.2657, f1: 0.3032, loss: 3.3545 ||: : 4254it [50:07,  1.40it/s]
em: 0.2656, f1: 0.3031, loss: 3.3555 ||: : 4260it [50:11,  1.41it/s]

2019-05-09 22:40:37,059 - INFO - allennlp.training.trainer - Validating
  0%|          | 0/596 [00:00<?, ?it/s]
em: 0.4050, f1: 0.4241, loss: 1225002.0554 ||:   4%|4         | 25/596 [00:10<03:59,  2.39it/s]
em: 0.3660, f1: 0.3878, loss: 1211736.8812 ||:   8%|8         | 49/596 [00:26<04:26,  2.05it/s]
em: 0.3477, f1: 0.3743, loss: 1408584.2673 ||:  11%|#1        | 67/596 [00:36<04:35,  1.92it/s]
em: 0.3328, f1: 0.3612, loss: 1636906.9238 ||:  14%|#4        | 84/596 [00:47<04:41,  1.82it/s]
em: 0.3431, f1: 0.3697, loss: 1644419.5475 ||:  17%|#7        | 103/596 [00:57<04:29,  1.83it/s]
em: 0.3428, f1: 0.3725, loss: 1690575.9188 ||:  20%|##        | 122/596 [01:08<04:25,  1.78it/s]
em: 0.3350, f1: 0.3649, loss: 1694544.4080 ||:  24%|##3       | 142/596 [01:19<04:10,  1.81it/s]
em: 0.3251, f1: 0.3537, loss: 1711958.7309 ||:  27%|##7       | 161/596 [01:32<04:15,  1.70it/s]
em: 0.3224, f1: 0.3516, loss: 1717879.2676 ||:  30%|###       | 179/596 [01:42<04:01,  1.73it/s]
em: 0.3201, f1: 0.3482, loss: 1703682.4345 ||:  33%|###3      | 197/596 [01:53<03:53,  1.71it/s]
em: 0.3263, f1: 0.3555, loss: 1664748.7441 ||:  36%|###6      | 217/596 [02:03<03:33,  1.78it/s]
em: 0.3224, f1: 0.3521, loss: 1658757.5104 ||:  40%|###9      | 237/596 [02:14<03:22,  1.78it/s]
em: 0.3251, f1: 0.3546, loss: 1598041.4880 ||:  43%|####2     | 255/596 [02:24<03:12,  1.77it/s]
em: 0.3233, f1: 0.3525, loss: 1636907.0551 ||:  46%|####5     | 273/596 [02:36<03:08,  1.71it/s]
em: 0.3286, f1: 0.3576, loss: 1564642.7094 ||:  49%|####8     | 292/596 [02:46<02:54,  1.74it/s]
em: 0.3280, f1: 0.3559, loss: 1585613.2347 ||:  52%|#####2    | 311/596 [02:57<02:45,  1.72it/s]
em: 0.3257, f1: 0.3535, loss: 1592805.3445 ||:  55%|#####5    | 330/596 [03:07<02:30,  1.77it/s]
em: 0.3235, f1: 0.3512, loss: 1613541.0081 ||:  59%|#####8    | 349/596 [03:20<02:28,  1.67it/s]
em: 0.3278, f1: 0.3555, loss: 1558267.9030 ||:  62%|######1   | 369/596 [03:31<02:11,  1.73it/s]
em: 0.3273, f1: 0.3555, loss: 1586664.7059 ||:  65%|######5   | 388/596 [03:43<02:04,  1.67it/s]
em: 0.3260, f1: 0.3534, loss: 1571693.5118 ||:  68%|######8   | 408/596 [03:54<01:48,  1.74it/s]
em: 0.3254, f1: 0.3532, loss: 1553740.6791 ||:  72%|#######1  | 428/596 [04:06<01:38,  1.71it/s]
em: 0.3252, f1: 0.3532, loss: 1512053.9585 ||:  75%|#######4  | 446/596 [04:16<01:27,  1.72it/s]
em: 0.3189, f1: 0.3464, loss: 1561155.4094 ||:  78%|#######7  | 464/596 [04:27<01:18,  1.68it/s]
em: 0.3134, f1: 0.3409, loss: 1684673.1038 ||:  82%|########1 | 486/596 [04:38<01:01,  1.79it/s]
em: 0.3090, f1: 0.3368, loss: 1804062.9310 ||:  86%|########5 | 511/596 [04:48<00:43,  1.94it/s]
em: 0.3072, f1: 0.3347, loss: 1852847.3914 ||:  90%|########9 | 536/596 [05:01<00:30,  1.96it/s]
em: 0.3022, f1: 0.3297, loss: 1910716.5249 ||:  94%|#########3| 560/596 [05:11<00:17,  2.05it/s]
em: 0.2944, f1: 0.3217, loss: 1980952.5629 ||:  98%|#########7| 584/596 [05:25<00:06,  1.94it/s]
em: 0.2902, f1: 0.3179, loss: 1975708.2176 ||: : 602it [05:38,  1.72it/s]                       
em: 0.2896, f1: 0.3174, loss: 1969166.1524 ||: : 604it [05:40,  1.77it/s]

2019-05-09 22:46:17,625 - INFO - allennlp.training.tensorboard_writer -                     Training |  Validation
2019-05-09 22:46:17,626 - INFO - allennlp.training.tensorboard_writer - cpu_memory_MB   |  8328.524  |       N/A
2019-05-09 22:46:17,626 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB |  4339.000  |       N/A
2019-05-09 22:46:17,627 - INFO - allennlp.training.tensorboard_writer - loss            |     3.355  |  1969166.152
2019-05-09 22:46:17,627 - INFO - allennlp.training.tensorboard_writer - em              |     0.266  |     0.290
2019-05-09 22:46:17,628 - INFO - allennlp.training.tensorboard_writer - f1              |     0.303  |     0.317
2019-05-09 22:46:22,328 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to 'out/rc_bert_simple/best.th'.
2019-05-09 22:46:27,220 - INFO - allennlp.training.trainer - Epoch duration: 00:56:01
2019-05-09 22:46:27,221 - INFO - allennlp.training.trainer - Estimated training time remaining: 6:40:31
2019-05-09 22:46:27,221 - INFO - allennlp.training.trainer - Epoch 3/9
2019-05-09 22:46:27,221 - INFO - allennlp.training.trainer - Peak CPU memory usage MB: 8328.74
2019-05-09 22:46:27,308 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 4319
2019-05-09 22:46:27,315 - INFO - allennlp.training.trainer - Training
  0%|          | 0/4204 [00:00<?, ?it/s]
em: 0.2545, f1: 0.2847, loss: 3.1875 ||:   0%|          | 14/4204 [00:10<50:58,  1.37it/s]
em: 0.2741, f1: 0.3028, loss: 3.0303 ||:   1%|          | 29/4204 [00:20<50:26,  1.38it/s]
em: 0.2965, f1: 0.3273, loss: 2.9784 ||:   1%|1         | 44/4204 [00:31<49:19,  1.41it/s]
em: 0.2934, f1: 0.3228, loss: 2.9940 ||:   1%|1         | 60/4204 [00:41<48:09,  1.43it/s]
em: 0.2883, f1: 0.3172, loss: 3.0567 ||:   2%|1         | 76/4204 [00:52<47:06,  1.46it/s]
em: 0.2838, f1: 0.3128, loss: 3.1383 ||:   2%|2         | 92/4204 [01:03<46:48,  1.46it/s]
em: 0.2832, f1: 0.3118, loss: 3.1747 ||:   3%|2         | 107/4204 [01:14<47:54,  1.43it/s]
em: 0.2772, f1: 0.3078, loss: 3.1956 ||:   3%|2         | 121/4204 [01:24<48:40,  1.40it/s]
em: 0.2767, f1: 0.3099, loss: 3.1753 ||:   3%|3         | 135/4204 [01:34<48:29,  1.40it/s]
em: 0.2829, f1: 0.3159, loss: 3.1721 ||:   4%|3         | 150/4204 [01:44<47:39,  1.42it/s]
em: 0.2918, f1: 0.3256, loss: 3.1442 ||:   4%|3         | 165/4204 [01:56<48:06,  1.40it/s]
em: 0.2886, f1: 0.3230, loss: 3.1499 ||:   4%|4         | 180/4204 [02:06<47:47,  1.40it/s]
em: 0.2955, f1: 0.3299, loss: 3.1235 ||:   5%|4         | 195/4204 [02:17<47:27,  1.41it/s]
em: 0.2971, f1: 0.3307, loss: 3.1238 ||:   5%|5         | 211/4204 [02:27<46:15,  1.44it/s]
em: 0.2958, f1: 0.3303, loss: 3.1247 ||:   5%|5         | 227/4204 [02:38<46:08,  1.44it/s]
em: 0.2958, f1: 0.3315, loss: 3.1235 ||:   6%|5         | 242/4204 [02:49<46:37,  1.42it/s]
em: 0.2978, f1: 0.3339, loss: 3.1256 ||:   6%|6         | 257/4204 [03:00<45:53,  1.43it/s]
em: 0.2985, f1: 0.3351, loss: 3.1269 ||:   6%|6         | 273/4204 [03:10<44:56,  1.46it/s]
em: 0.3004, f1: 0.3366, loss: 3.1261 ||:   7%|6         | 289/4204 [03:21<44:08,  1.48it/s]
em: 0.2990, f1: 0.3346, loss: 3.1349 ||:   7%|7         | 305/4204 [03:32<44:30,  1.46it/s]
em: 0.2957, f1: 0.3316, loss: 3.1491 ||:   8%|7         | 320/4204 [03:43<45:41,  1.42it/s]
em: 0.2972, f1: 0.3338, loss: 3.1544 ||:   8%|7         | 334/4204 [03:54<46:29,  1.39it/s]
em: 0.2966, f1: 0.3325, loss: 3.1566 ||:   8%|8         | 348/4204 [04:04<46:25,  1.38it/s]
em: 0.2960, f1: 0.3318, loss: 3.1659 ||:   9%|8         | 362/4204 [04:14<46:48,  1.37it/s]
em: 0.2943, f1: 0.3305, loss: 3.1710 ||:   9%|8         | 376/4204 [04:24<46:20,  1.38it/s]
em: 0.2924, f1: 0.3282, loss: 3.1761 ||:   9%|9         | 390/4204 [04:35<46:13,  1.37it/s]
em: 0.2898, f1: 0.3264, loss: 3.1859 ||:  10%|9         | 405/4204 [04:45<45:09,  1.40it/s]
em: 0.2886, f1: 0.3251, loss: 3.1910 ||:  10%|9         | 420/4204 [04:56<44:58,  1.40it/s]
em: 0.2889, f1: 0.3251, loss: 3.1939 ||:  10%|#         | 435/4204 [05:06<44:41,  1.41it/s]
em: 0.2896, f1: 0.3259, loss: 3.1870 ||:  11%|#         | 450/4204 [05:17<44:17,  1.41it/s]
em: 0.2883, f1: 0.3251, loss: 3.1912 ||:  11%|#1        | 465/4204 [05:28<44:32,  1.40it/s]
em: 0.2877, f1: 0.3248, loss: 3.1867 ||:  11%|#1        | 481/4204 [05:38<43:20,  1.43it/s]
em: 0.2898, f1: 0.3266, loss: 3.1830 ||:  12%|#1        | 497/4204 [05:50<43:20,  1.43it/s]
em: 0.2891, f1: 0.3258, loss: 3.1778 ||:  12%|#2        | 512/4204 [06:00<43:01,  1.43it/s]
em: 0.2895, f1: 0.3261, loss: 3.1730 ||:  13%|#2        | 527/4204 [06:11<43:22,  1.41it/s]
em: 0.2907, f1: 0.3272, loss: 3.1659 ||:  13%|#2        | 541/4204 [06:21<43:31,  1.40it/s]
em: 0.2919, f1: 0.3282, loss: 3.1618 ||:  13%|#3        | 556/4204 [06:31<42:38,  1.43it/s]
em: 0.2931, f1: 0.3294, loss: 3.1535 ||:  14%|#3        | 571/4204 [06:42<42:40,  1.42it/s]
em: 0.2925, f1: 0.3289, loss: 3.1618 ||:  14%|#3        | 586/4204 [06:52<42:38,  1.41it/s]
em: 0.2920, f1: 0.3287, loss: 3.1582 ||:  14%|#4        | 601/4204 [07:03<42:52,  1.40it/s]
em: 0.2924, f1: 0.3291, loss: 3.1593 ||:  15%|#4        | 615/4204 [07:14<42:57,  1.39it/s]
em: 0.2928, f1: 0.3300, loss: 3.1508 ||:  15%|#4        | 630/4204 [07:24<42:01,  1.42it/s]
em: 0.2926, f1: 0.3300, loss: 3.1526 ||:  15%|#5        | 645/4204 [07:35<42:11,  1.41it/s]
em: 0.2922, f1: 0.3293, loss: 3.1514 ||:  16%|#5        | 659/4204 [07:45<42:24,  1.39it/s]
em: 0.2928, f1: 0.3296, loss: 3.1476 ||:  16%|#6        | 674/4204 [07:55<41:54,  1.40it/s]
em: 0.2932, f1: 0.3305, loss: 3.1434 ||:  16%|#6        | 689/4204 [08:06<41:18,  1.42it/s]
em: 0.2943, f1: 0.3312, loss: 3.1397 ||:  17%|#6        | 705/4204 [08:16<40:09,  1.45it/s]
em: 0.2935, f1: 0.3306, loss: 3.1411 ||:  17%|#7        | 721/4204 [08:27<39:28,  1.47it/s]
em: 0.2936, f1: 0.3310, loss: 3.1394 ||:  18%|#7        | 737/4204 [08:38<39:22,  1.47it/s]
em: 0.2941, f1: 0.3312, loss: 3.1394 ||:  18%|#7        | 752/4204 [08:48<39:38,  1.45it/s]
em: 0.2947, f1: 0.3319, loss: 3.1373 ||:  18%|#8        | 767/4204 [08:59<39:44,  1.44it/s]
em: 0.2956, f1: 0.3326, loss: 3.1360 ||:  19%|#8        | 782/4204 [09:09<39:51,  1.43it/s]
em: 0.2951, f1: 0.3323, loss: 3.1315 ||:  19%|#8        | 797/4204 [09:20<39:30,  1.44it/s]
em: 0.2952, f1: 0.3323, loss: 3.1338 ||:  19%|#9        | 812/4204 [09:31<39:53,  1.42it/s]
em: 0.2947, f1: 0.3319, loss: 3.1376 ||:  20%|#9        | 827/4204 [09:41<39:21,  1.43it/s]
em: 0.2945, f1: 0.3317, loss: 3.1439 ||:  20%|##        | 842/4204 [09:51<39:04,  1.43it/s]
em: 0.2940, f1: 0.3315, loss: 3.1504 ||:  20%|##        | 857/4204 [10:01<38:28,  1.45it/s]
em: 0.2935, f1: 0.3312, loss: 3.1571 ||:  21%|##        | 873/4204 [10:12<37:49,  1.47it/s]
em: 0.2935, f1: 0.3311, loss: 3.1637 ||:  21%|##1       | 889/4204 [10:24<38:36,  1.43it/s]
em: 0.2933, f1: 0.3310, loss: 3.1638 ||:  21%|##1       | 903/4204 [10:34<38:59,  1.41it/s]
em: 0.2930, f1: 0.3310, loss: 3.1650 ||:  22%|##1       | 917/4204 [10:44<38:59,  1.41it/s]
em: 0.2942, f1: 0.3321, loss: 3.1596 ||:  22%|##2       | 933/4204 [10:55<38:06,  1.43it/s]
em: 0.2942, f1: 0.3322, loss: 3.1585 ||:  23%|##2       | 948/4204 [11:05<37:49,  1.43it/s]
em: 0.2940, f1: 0.3320, loss: 3.1624 ||:  23%|##2       | 963/4204 [11:16<37:24,  1.44it/s]
em: 0.2936, f1: 0.3314, loss: 3.1599 ||:  23%|##3       | 978/4204 [11:26<37:27,  1.44it/s]
em: 0.2944, f1: 0.3323, loss: 3.1592 ||:  24%|##3       | 993/4204 [11:37<37:24,  1.43it/s]
em: 0.2951, f1: 0.3328, loss: 3.1586 ||:  24%|##3       | 1008/4204 [11:47<37:01,  1.44it/s]
em: 0.2962, f1: 0.3338, loss: 3.1551 ||:  24%|##4       | 1023/4204 [11:58<37:30,  1.41it/s]
em: 0.2969, f1: 0.3343, loss: 3.1580 ||:  25%|##4       | 1038/4204 [12:08<36:57,  1.43it/s]
em: 0.2976, f1: 0.3350, loss: 3.1584 ||:  25%|##5       | 1053/4204 [12:19<36:34,  1.44it/s]
em: 0.2981, f1: 0.3355, loss: 3.1610 ||:  25%|##5       | 1068/4204 [12:30<36:56,  1.41it/s]
em: 0.2976, f1: 0.3349, loss: 3.1621 ||:  26%|##5       | 1082/4204 [12:40<36:57,  1.41it/s]
em: 0.2973, f1: 0.3347, loss: 3.1635 ||:  26%|##6       | 1097/4204 [12:50<36:34,  1.42it/s]
em: 0.2985, f1: 0.3358, loss: 3.1619 ||:  26%|##6       | 1112/4204 [13:01<36:59,  1.39it/s]
em: 0.2982, f1: 0.3355, loss: 3.1674 ||:  27%|##6       | 1126/4204 [13:11<36:48,  1.39it/s]
em: 0.2981, f1: 0.3355, loss: 3.1676 ||:  27%|##7       | 1141/4204 [13:22<36:35,  1.39it/s]
em: 0.2979, f1: 0.3353, loss: 3.1681 ||:  27%|##7       | 1156/4204 [13:33<36:22,  1.40it/s]
em: 0.2978, f1: 0.3352, loss: 3.1705 ||:  28%|##7       | 1171/4204 [13:43<35:52,  1.41it/s]
em: 0.2976, f1: 0.3348, loss: 3.1701 ||:  28%|##8       | 1186/4204 [13:54<35:45,  1.41it/s]
em: 0.2982, f1: 0.3353, loss: 3.1649 ||:  29%|##8       | 1201/4204 [14:04<35:12,  1.42it/s]
em: 0.2991, f1: 0.3360, loss: 3.1635 ||:  29%|##8       | 1216/4204 [14:15<35:14,  1.41it/s]
em: 0.2987, f1: 0.3356, loss: 3.1685 ||:  29%|##9       | 1230/4204 [14:25<35:47,  1.38it/s]
em: 0.2991, f1: 0.3360, loss: 3.1659 ||:  30%|##9       | 1245/4204 [14:36<35:19,  1.40it/s]
em: 0.2991, f1: 0.3360, loss: 3.1636 ||:  30%|##9       | 1260/4204 [14:47<35:20,  1.39it/s]
em: 0.2995, f1: 0.3363, loss: 3.1629 ||:  30%|###       | 1275/4204 [14:58<35:01,  1.39it/s]
em: 0.2998, f1: 0.3365, loss: 3.1610 ||:  31%|###       | 1290/4204 [15:08<34:28,  1.41it/s]
em: 0.2995, f1: 0.3361, loss: 3.1636 ||:  31%|###1      | 1306/4204 [15:19<33:41,  1.43it/s]
em: 0.2987, f1: 0.3353, loss: 3.1687 ||:  31%|###1      | 1321/4204 [15:30<34:24,  1.40it/s]
em: 0.2986, f1: 0.3353, loss: 3.1684 ||:  32%|###1      | 1335/4204 [15:40<34:13,  1.40it/s]
em: 0.2986, f1: 0.3355, loss: 3.1697 ||:  32%|###2      | 1351/4204 [15:50<33:05,  1.44it/s]
em: 0.2995, f1: 0.3364, loss: 3.1646 ||:  33%|###2      | 1367/4204 [16:01<32:44,  1.44it/s]
em: 0.2996, f1: 0.3367, loss: 3.1636 ||:  33%|###2      | 1382/4204 [16:12<32:52,  1.43it/s]
em: 0.2993, f1: 0.3365, loss: 3.1631 ||:  33%|###3      | 1396/4204 [16:23<33:27,  1.40it/s]
em: 0.2992, f1: 0.3364, loss: 3.1639 ||:  34%|###3      | 1410/4204 [16:33<33:25,  1.39it/s]
em: 0.2995, f1: 0.3365, loss: 3.1622 ||:  34%|###3      | 1424/4204 [16:44<33:56,  1.36it/s]
em: 0.2990, f1: 0.3360, loss: 3.1622 ||:  34%|###4      | 1438/4204 [16:54<33:35,  1.37it/s]
em: 0.2996, f1: 0.3365, loss: 3.1594 ||:  35%|###4      | 1452/4204 [17:04<33:25,  1.37it/s]
em: 0.2998, f1: 0.3368, loss: 3.1579 ||:  35%|###4      | 1466/4204 [17:14<33:20,  1.37it/s]
em: 0.2996, f1: 0.3368, loss: 3.1574 ||:  35%|###5      | 1480/4204 [17:24<33:07,  1.37it/s]
em: 0.2993, f1: 0.3366, loss: 3.1579 ||:  36%|###5      | 1495/4204 [17:35<32:39,  1.38it/s]
em: 0.2994, f1: 0.3368, loss: 3.1583 ||:  36%|###5      | 1510/4204 [17:46<32:28,  1.38it/s]
em: 0.2994, f1: 0.3368, loss: 3.1592 ||:  36%|###6      | 1524/4204 [17:56<32:38,  1.37it/s]
em: 0.2991, f1: 0.3366, loss: 3.1637 ||:  37%|###6      | 1539/4204 [18:07<31:52,  1.39it/s]
em: 0.2994, f1: 0.3370, loss: 3.1603 ||:  37%|###6      | 1554/4204 [18:17<31:29,  1.40it/s]
em: 0.2993, f1: 0.3369, loss: 3.1614 ||:  37%|###7      | 1569/4204 [18:28<31:14,  1.41it/s]
em: 0.2997, f1: 0.3372, loss: 3.1579 ||:  38%|###7      | 1584/4204 [18:38<30:49,  1.42it/s]
em: 0.3000, f1: 0.3374, loss: 3.1540 ||:  38%|###8      | 1599/4204 [18:49<30:55,  1.40it/s]
em: 0.2995, f1: 0.3369, loss: 3.1543 ||:  38%|###8      | 1614/4204 [18:59<30:16,  1.43it/s]
em: 0.2995, f1: 0.3369, loss: 3.1540 ||:  39%|###8      | 1629/4204 [19:10<30:24,  1.41it/s]
em: 0.2990, f1: 0.3365, loss: 3.1563 ||:  39%|###9      | 1644/4204 [19:20<29:51,  1.43it/s]
em: 0.2983, f1: 0.3359, loss: 3.1585 ||:  39%|###9      | 1659/4204 [19:31<29:58,  1.41it/s]
em: 0.2986, f1: 0.3361, loss: 3.1589 ||:  40%|###9      | 1674/4204 [19:42<29:40,  1.42it/s]
em: 0.2983, f1: 0.3360, loss: 3.1561 ||:  40%|####      | 1689/4204 [19:52<29:10,  1.44it/s]
em: 0.2983, f1: 0.3359, loss: 3.1558 ||:  41%|####      | 1704/4204 [20:02<29:15,  1.42it/s]
em: 0.2983, f1: 0.3361, loss: 3.1553 ||:  41%|####      | 1719/4204 [20:13<29:02,  1.43it/s]
em: 0.2980, f1: 0.3356, loss: 3.1577 ||:  41%|####1     | 1735/4204 [20:23<28:11,  1.46it/s]
em: 0.2983, f1: 0.3359, loss: 3.1584 ||:  42%|####1     | 1751/4204 [20:35<28:43,  1.42it/s]
em: 0.2976, f1: 0.3351, loss: 3.1587 ||:  42%|####1     | 1765/4204 [20:46<29:00,  1.40it/s]
em: 0.2978, f1: 0.3352, loss: 3.1565 ||:  42%|####2     | 1780/4204 [20:56<28:41,  1.41it/s]
em: 0.2976, f1: 0.3351, loss: 3.1571 ||:  43%|####2     | 1795/4204 [21:06<28:14,  1.42it/s]
em: 0.2973, f1: 0.3349, loss: 3.1570 ||:  43%|####3     | 1810/4204 [21:17<27:47,  1.44it/s]
em: 0.2974, f1: 0.3350, loss: 3.1575 ||:  43%|####3     | 1825/4204 [21:28<28:09,  1.41it/s]
em: 0.2969, f1: 0.3346, loss: 3.1605 ||:  44%|####3     | 1840/4204 [21:38<28:01,  1.41it/s]
em: 0.2974, f1: 0.3350, loss: 3.1598 ||:  44%|####4     | 1855/4204 [21:49<27:38,  1.42it/s]
em: 0.2972, f1: 0.3348, loss: 3.1588 ||:  44%|####4     | 1870/4204 [22:00<27:47,  1.40it/s]
em: 0.2972, f1: 0.3349, loss: 3.1608 ||:  45%|####4     | 1886/4204 [22:10<26:56,  1.43it/s]
em: 0.2972, f1: 0.3350, loss: 3.1595 ||:  45%|####5     | 1902/4204 [22:22<27:16,  1.41it/s]
em: 0.2969, f1: 0.3347, loss: 3.1577 ||:  46%|####5     | 1918/4204 [22:33<26:29,  1.44it/s]
em: 0.2968, f1: 0.3345, loss: 3.1577 ||:  46%|####6     | 1934/4204 [22:44<26:37,  1.42it/s]
em: 0.2962, f1: 0.3342, loss: 3.1588 ||:  46%|####6     | 1949/4204 [22:55<26:24,  1.42it/s]
em: 0.2966, f1: 0.3344, loss: 3.1566 ||:  47%|####6     | 1964/4204 [23:06<26:25,  1.41it/s]
em: 0.2969, f1: 0.3347, loss: 3.1551 ||:  47%|####7     | 1978/4204 [23:16<26:41,  1.39it/s]
em: 0.2968, f1: 0.3346, loss: 3.1555 ||:  47%|####7     | 1993/4204 [23:26<26:05,  1.41it/s]
em: 0.2968, f1: 0.3347, loss: 3.1566 ||:  48%|####7     | 2008/4204 [23:37<25:58,  1.41it/s]
em: 0.2965, f1: 0.3345, loss: 3.1576 ||:  48%|####8     | 2023/4204 [23:49<26:30,  1.37it/s]
em: 0.2966, f1: 0.3346, loss: 3.1564 ||:  48%|####8     | 2038/4204 [23:59<25:53,  1.39it/s]
em: 0.2964, f1: 0.3346, loss: 3.1574 ||:  49%|####8     | 2053/4204 [24:10<25:55,  1.38it/s]
em: 0.2963, f1: 0.3346, loss: 3.1571 ||:  49%|####9     | 2067/4204 [24:20<25:54,  1.37it/s]
em: 0.2963, f1: 0.3346, loss: 3.1562 ||:  50%|####9     | 2082/4204 [24:31<25:29,  1.39it/s]
em: 0.2961, f1: 0.3344, loss: 3.1561 ||:  50%|####9     | 2097/4204 [24:42<25:28,  1.38it/s]
em: 0.2961, f1: 0.3345, loss: 3.1567 ||:  50%|#####     | 2112/4204 [24:52<24:50,  1.40it/s]
em: 0.2961, f1: 0.3344, loss: 3.1565 ||:  51%|#####     | 2127/4204 [25:03<24:37,  1.41it/s]
em: 0.2960, f1: 0.3343, loss: 3.1586 ||:  51%|#####     | 2142/4204 [25:14<24:34,  1.40it/s]
em: 0.2962, f1: 0.3344, loss: 3.1594 ||:  51%|#####1    | 2157/4204 [25:24<24:11,  1.41it/s]
em: 0.2961, f1: 0.3344, loss: 3.1591 ||:  52%|#####1    | 2172/4204 [25:35<24:28,  1.38it/s]
em: 0.2962, f1: 0.3344, loss: 3.1584 ||:  52%|#####1    | 2186/4204 [25:46<24:16,  1.39it/s]
em: 0.2962, f1: 0.3345, loss: 3.1553 ||:  52%|#####2    | 2201/4204 [25:56<23:48,  1.40it/s]
em: 0.2965, f1: 0.3347, loss: 3.1546 ||:  53%|#####2    | 2216/4204 [26:07<23:43,  1.40it/s]
em: 0.2962, f1: 0.3346, loss: 3.1537 ||:  53%|#####3    | 2231/4204 [26:17<23:21,  1.41it/s]
em: 0.2963, f1: 0.3346, loss: 3.1512 ||:  53%|#####3    | 2246/4204 [26:27<22:49,  1.43it/s]
em: 0.2967, f1: 0.3350, loss: 3.1495 ||:  54%|#####3    | 2261/4204 [26:38<22:44,  1.42it/s]
em: 0.2967, f1: 0.3349, loss: 3.1498 ||:  54%|#####4    | 2277/4204 [26:49<22:14,  1.44it/s]
em: 0.2969, f1: 0.3350, loss: 3.1523 ||:  55%|#####4    | 2292/4204 [26:59<21:51,  1.46it/s]
em: 0.2968, f1: 0.3348, loss: 3.1495 ||:  55%|#####4    | 2308/4204 [27:09<21:27,  1.47it/s]
em: 0.2965, f1: 0.3346, loss: 3.1496 ||:  55%|#####5    | 2324/4204 [27:21<21:37,  1.45it/s]
em: 0.2964, f1: 0.3344, loss: 3.1499 ||:  56%|#####5    | 2339/4204 [27:31<21:33,  1.44it/s]
em: 0.2965, f1: 0.3346, loss: 3.1509 ||:  56%|#####5    | 2354/4204 [27:42<21:22,  1.44it/s]
em: 0.2967, f1: 0.3349, loss: 3.1492 ||:  56%|#####6    | 2369/4204 [27:52<21:22,  1.43it/s]
em: 0.2971, f1: 0.3353, loss: 3.1467 ||:  57%|#####6    | 2384/4204 [28:03<21:01,  1.44it/s]
em: 0.2970, f1: 0.3352, loss: 3.1471 ||:  57%|#####7    | 2399/4204 [28:13<20:59,  1.43it/s]
em: 0.2970, f1: 0.3352, loss: 3.1472 ||:  57%|#####7    | 2414/4204 [28:24<20:59,  1.42it/s]
em: 0.2970, f1: 0.3352, loss: 3.1481 ||:  58%|#####7    | 2429/4204 [28:35<20:55,  1.41it/s]
em: 0.2970, f1: 0.3353, loss: 3.1484 ||:  58%|#####8    | 2443/4204 [28:45<20:57,  1.40it/s]
em: 0.2972, f1: 0.3354, loss: 3.1501 ||:  58%|#####8    | 2457/4204 [28:55<20:57,  1.39it/s]
em: 0.2969, f1: 0.3351, loss: 3.1519 ||:  59%|#####8    | 2471/4204 [29:06<21:05,  1.37it/s]
em: 0.2968, f1: 0.3351, loss: 3.1519 ||:  59%|#####9    | 2487/4204 [29:16<20:18,  1.41it/s]
em: 0.2964, f1: 0.3347, loss: 3.1544 ||:  60%|#####9    | 2503/4204 [29:27<19:29,  1.45it/s]
em: 0.2968, f1: 0.3350, loss: 3.1527 ||:  60%|#####9    | 2519/4204 [29:37<19:04,  1.47it/s]
em: 0.2968, f1: 0.3349, loss: 3.1527 ||:  60%|######    | 2535/4204 [29:48<19:05,  1.46it/s]
em: 0.2966, f1: 0.3348, loss: 3.1523 ||:  61%|######    | 2550/4204 [29:59<19:02,  1.45it/s]
em: 0.2969, f1: 0.3351, loss: 3.1525 ||:  61%|######1   | 2566/4204 [30:09<18:29,  1.48it/s]
em: 0.2969, f1: 0.3351, loss: 3.1520 ||:  61%|######1   | 2582/4204 [30:20<18:15,  1.48it/s]
em: 0.2972, f1: 0.3354, loss: 3.1515 ||:  62%|######1   | 2597/4204 [30:31<18:25,  1.45it/s]
em: 0.2977, f1: 0.3359, loss: 3.1487 ||:  62%|######2   | 2612/4204 [30:41<18:08,  1.46it/s]
em: 0.2978, f1: 0.3359, loss: 3.1491 ||:  62%|######2   | 2627/4204 [30:52<18:29,  1.42it/s]
em: 0.2975, f1: 0.3357, loss: 3.1493 ||:  63%|######2   | 2641/4204 [31:02<18:28,  1.41it/s]
em: 0.2977, f1: 0.3359, loss: 3.1483 ||:  63%|######3   | 2656/4204 [31:13<18:15,  1.41it/s]
em: 0.2979, f1: 0.3360, loss: 3.1466 ||:  64%|######3   | 2671/4204 [31:23<17:52,  1.43it/s]
em: 0.2981, f1: 0.3363, loss: 3.1437 ||:  64%|######3   | 2686/4204 [31:33<17:29,  1.45it/s]
em: 0.2985, f1: 0.3367, loss: 3.1426 ||:  64%|######4   | 2701/4204 [31:43<17:20,  1.44it/s]
em: 0.2985, f1: 0.3367, loss: 3.1415 ||:  65%|######4   | 2716/4204 [31:54<17:03,  1.45it/s]
em: 0.2983, f1: 0.3365, loss: 3.1402 ||:  65%|######4   | 2731/4204 [32:04<16:49,  1.46it/s]
em: 0.2983, f1: 0.3365, loss: 3.1425 ||:  65%|######5   | 2746/4204 [32:15<16:53,  1.44it/s]
em: 0.2982, f1: 0.3363, loss: 3.1432 ||:  66%|######5   | 2760/4204 [32:25<16:57,  1.42it/s]
em: 0.2983, f1: 0.3364, loss: 3.1440 ||:  66%|######5   | 2774/4204 [32:35<17:04,  1.40it/s]
em: 0.2981, f1: 0.3362, loss: 3.1457 ||:  66%|######6   | 2788/4204 [32:45<16:59,  1.39it/s]
em: 0.2980, f1: 0.3362, loss: 3.1453 ||:  67%|######6   | 2802/4204 [32:56<16:55,  1.38it/s]
em: 0.2981, f1: 0.3363, loss: 3.1450 ||:  67%|######7   | 2817/4204 [33:06<16:38,  1.39it/s]
